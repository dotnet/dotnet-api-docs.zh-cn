<Type Name="RecognizedAudio" FullName="System.Speech.Recognition.RecognizedAudio">
  <Metadata><Meta Name="ms.openlocfilehash" Value="1589ccbeb73e54c0a5b41be74b6365a765fa1106" /><Meta Name="ms.sourcegitcommit" Value="c982ad9e10a242d90f1e631b5981570f9e5d9f34" /><Meta Name="ms.translationtype" Value="MT" /><Meta Name="ms.contentlocale" Value="zh-CN" /><Meta Name="ms.lasthandoff" Value="09/10/2020" /><Meta Name="ms.locfileid" Value="89900749" /></Metadata><TypeSignature Language="C#" Value="public class RecognizedAudio" />
  <TypeSignature Language="ILAsm" Value=".class public auto ansi serializable beforefieldinit RecognizedAudio extends System.Object" />
  <TypeSignature Language="DocId" Value="T:System.Speech.Recognition.RecognizedAudio" />
  <TypeSignature Language="VB.NET" Value="Public Class RecognizedAudio" />
  <TypeSignature Language="C++ CLI" Value="public ref class RecognizedAudio" />
  <TypeSignature Language="F#" Value="type RecognizedAudio = class" />
  <AssemblyInfo>
    <AssemblyName>System.Speech</AssemblyName>
    <AssemblyVersion>3.0.0.0</AssemblyVersion>
    <AssemblyVersion>4.0.0.0</AssemblyVersion>
  </AssemblyInfo>
  <Base>
    <BaseTypeName>System.Object</BaseTypeName>
  </Base>
  <Interfaces />
  <Attributes>
    <Attribute>
      <AttributeName Language="C#">[System.Serializable]</AttributeName>
      <AttributeName Language="F#">[&lt;System.Serializable&gt;]</AttributeName>
    </Attribute>
  </Attributes>
  <Docs>
    <summary>表示与 <see cref="T:System.Speech.Recognition.RecognitionResult" /> 关联的音频输入。</summary>
    <remarks>
      <format type="text/markdown"><![CDATA[  
  
## Remarks  
 语音识别器在识别操作过程中生成有关音频输入的信息。 若要访问识别的音频，请使用的 <xref:System.Speech.Recognition.RecognitionResult.Audio%2A> 属性或 <xref:System.Speech.Recognition.RecognitionResult.GetAudioForWordRange%2A> 方法 <xref:System.Speech.Recognition.RecognitionResult> 。  
  
 可以通过以下事件和和类的方法生成识别结果 <xref:System.Speech.Recognition.SpeechRecognizer> <xref:System.Speech.Recognition.SpeechRecognitionEngine> ：  
  
-   事件：  
  
    -   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized?displayProperty=nameWithType> 和 <xref:System.Speech.Recognition.SpeechRecognizer.SpeechHypothesized?displayProperty=nameWithType>  
  
    -   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected?displayProperty=nameWithType> 和 <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognitionRejected?displayProperty=nameWithType>  
  
    -   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized?displayProperty=nameWithType> 和 <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized?displayProperty=nameWithType>  
  
    -   <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted?displayProperty=nameWithType> 和 <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeCompleted?displayProperty=nameWithType>  
  
    -   <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted?displayProperty=nameWithType>  
  
-   方法：  
  
    -   <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize%2A?displayProperty=nameWithType> 和 <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognize%2A?displayProperty=nameWithType>  
  
    -   <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A?displayProperty=nameWithType> 和 <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync%2A?displayProperty=nameWithType>  
  
    -   <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A?displayProperty=nameWithType>  
  
    -   <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A?displayProperty=nameWithType>  
  
> [!IMPORTANT]
>  模拟语音识别生成的识别结果不包含可识别的音频。 对于这种识别结果，其 <xref:System.Speech.Recognition.RecognitionResult.Audio%2A> 属性返回， `null` 其 <xref:System.Speech.Recognition.RecognitionResult.GetAudioForWordRange%2A> 方法会引发异常。 有关模拟语音识别的详细信息，请参阅 `EmulateRecognize` `EmulateRecognizeAsync` 和类的和 <xref:System.Speech.Recognition.SpeechRecognizer> 方法 <xref:System.Speech.Recognition.SpeechRecognitionEngine> 。  
  
   
  
## Examples  
 下面的示例处理 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized?displayProperty=nameWithType> 、 <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized?displayProperty=nameWithType> 或事件， <xref:System.Speech.Recognition.Grammar.SpeechRecognized?displayProperty=nameWithType> 并输出到与识别结果关联的识别音频的控制台信息。  
  
```csharp  
  
// Handle the SpeechRecognized event.   
void SpeechRecognizedHandler(object sender, SpeechRecognizedEventArgs e)  
{  
  if (e.Result == null) return;  
  
  RecognitionResult result = e.Result;  
  
  Console.WriteLine("Grammar({0}): {1}",  
    result.Grammar.Name, result.Text);  
  
  if (e.Result.Audio != null)  
  {  
    RecognizedAudio audio = e.Result.Audio;  
  
    Console.WriteLine("   start time: {0}", audio.StartTime);  
    Console.WriteLine("   encoding format: {0}", audio.Format.EncodingFormat);  
    Console.WriteLine("   position: {0}, duration: {1}",  
      audio.AudioPosition, audio.Duration);  
  }  
  
  // Add event handler code here.  
}  
```  
  
 ]]></format>
    </remarks>
    <altmember cref="T:System.Speech.Recognition.RecognitionEventArgs" />
    <altmember cref="T:System.Speech.Recognition.RecognitionResult" />
    <altmember cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />
    <altmember cref="T:System.Speech.Recognition.SpeechRecognizer" />
  </Docs>
  <Members>
    <Member MemberName="AudioPosition">
      <MemberSignature Language="C#" Value="public TimeSpan AudioPosition { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.TimeSpan AudioPosition" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.RecognizedAudio.AudioPosition" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property AudioPosition As TimeSpan" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property TimeSpan AudioPosition { TimeSpan get(); };" />
      <MemberSignature Language="F#" Value="member this.AudioPosition : TimeSpan" Usage="System.Speech.Recognition.RecognizedAudio.AudioPosition" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Attributes>
        <Attribute FrameworkAlternate="netframework-4.0">
          <AttributeName Language="C#">[get: System.Runtime.TargetedPatchingOptOut("Performance critical to inline this type of method across NGen image boundaries")]</AttributeName>
          <AttributeName Language="F#">[&lt;get: System.Runtime.TargetedPatchingOptOut("Performance critical to inline this type of method across NGen image boundaries")&gt;]</AttributeName>
        </Attribute>
      </Attributes>
      <ReturnValue>
        <ReturnType>System.TimeSpan</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>获取可识别音频的开头在输入音频流中的位置。</summary>
        <value>可识别的音频开始的输入音频流的位置。</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 此属性在输入设备的生成音频流中引用已识别的短语开头的位置。 相反， `RecognizerAudioPosition` <xref:System.Speech.Recognition.SpeechRecognitionEngine> 和类的属性 <xref:System.Speech.Recognition.SpeechRecognizer> 引用识别器在其音频输入中的位置。 这些位置可能不同。 有关详细信息，请参阅 [使用语音识别事件](https://msdn.microsoft.com/library/01c598ca-2e0e-4e89-b303-cd1cef9e8482)。  
  
 <xref:System.Speech.Recognition.RecognizedAudio.StartTime%2A>属性获取识别操作开始时的系统时间。  
  
   
  
## Examples  
 下面的示例处理 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized?displayProperty=nameWithType> 或 <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized?displayProperty=nameWithType> 事件，并输出到与识别结果关联的识别音频的控制台信息。  
  
```csharp  
  
// Handle the SpeechRecognized event.   
void SpeechRecognizedHandler(object sender, SpeechRecognizedEventArgs e)  
{  
  if (e.Result == null) return;  
  
  RecognitionResult result = e.Result;  
  
  Console.WriteLine("Grammar({0}): {1}",  
    result.Grammar.Name, result.Text);  
  
  if (e.Result.Audio != null)  
  {  
    RecognizedAudio audio = e.Result.Audio;  
  
    Console.WriteLine("   start time: {0}", audio.StartTime);  
    Console.WriteLine("   encoding format: {0}", audio.Format.EncodingFormat);  
    Console.WriteLine("   position: {0}, duration: {1}",  
      audio.AudioPosition, audio.Duration);  
  }  
  
  // Add event handler code here.  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.Enabled" />
        <altmember cref="P:System.Speech.Recognition.RecognizedAudio.Duration" />
        <altmember cref="P:System.Speech.Recognition.RecognizedAudio.StartTime" />
      </Docs>
    </Member>
    <Member MemberName="Duration">
      <MemberSignature Language="C#" Value="public TimeSpan Duration { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.TimeSpan Duration" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.RecognizedAudio.Duration" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property Duration As TimeSpan" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property TimeSpan Duration { TimeSpan get(); };" />
      <MemberSignature Language="F#" Value="member this.Duration : TimeSpan" Usage="System.Speech.Recognition.RecognizedAudio.Duration" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Attributes>
        <Attribute FrameworkAlternate="netframework-4.0">
          <AttributeName Language="C#">[get: System.Runtime.TargetedPatchingOptOut("Performance critical to inline this type of method across NGen image boundaries")]</AttributeName>
          <AttributeName Language="F#">[&lt;get: System.Runtime.TargetedPatchingOptOut("Performance critical to inline this type of method across NGen image boundaries")&gt;]</AttributeName>
        </Attribute>
      </Attributes>
      <ReturnValue>
        <ReturnType>System.TimeSpan</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>获取已识别音频的输入音频流的持续时间。</summary>
        <value>可识别的音频开始的输入音频流中的持续时间。</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Examples  
 下面的示例处理 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized?displayProperty=nameWithType> 或 <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized?displayProperty=nameWithType> 事件，并输出到与识别结果关联的识别音频的控制台信息。  
  
```csharp  
  
// Handle the SpeechRecognized event.   
void SpeechRecognizedHandler(object sender, SpeechRecognizedEventArgs e)  
{  
  if (e.Result == null) return;  
  
  RecognitionResult result = e.Result;  
  
  Console.WriteLine("Grammar({0}): {1}",  
    result.Grammar.Name, result.Text);  
  
  if (e.Result.Audio != null)  
  {  
    RecognizedAudio audio = e.Result.Audio;  
  
    Console.WriteLine("   start time: {0}", audio.StartTime);  
    Console.WriteLine("   encoding format: {0}", audio.Format.EncodingFormat);  
    Console.WriteLine("   position: {0}, duration: {1}",  
      audio.AudioPosition, audio.Duration);  
  }  
  
  // Add event handler code here.  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="P:System.Speech.Recognition.RecognizedAudio.AudioPosition" />
        <altmember cref="P:System.Speech.Recognition.RecognizedAudio.StartTime" />
      </Docs>
    </Member>
    <Member MemberName="Format">
      <MemberSignature Language="C#" Value="public System.Speech.AudioFormat.SpeechAudioFormatInfo Format { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance class System.Speech.AudioFormat.SpeechAudioFormatInfo Format" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.RecognizedAudio.Format" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property Format As SpeechAudioFormatInfo" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property System::Speech::AudioFormat::SpeechAudioFormatInfo ^ Format { System::Speech::AudioFormat::SpeechAudioFormatInfo ^ get(); };" />
      <MemberSignature Language="F#" Value="member this.Format : System.Speech.AudioFormat.SpeechAudioFormatInfo" Usage="System.Speech.Recognition.RecognizedAudio.Format" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Attributes>
        <Attribute FrameworkAlternate="netframework-4.0">
          <AttributeName Language="C#">[get: System.Runtime.TargetedPatchingOptOut("Performance critical to inline this type of method across NGen image boundaries")]</AttributeName>
          <AttributeName Language="F#">[&lt;get: System.Runtime.TargetedPatchingOptOut("Performance critical to inline this type of method across NGen image boundaries")&gt;]</AttributeName>
        </Attribute>
      </Attributes>
      <ReturnValue>
        <ReturnType>System.Speech.AudioFormat.SpeechAudioFormatInfo</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>获取识别引擎处理的音频格式。</summary>
        <value>语音识别器处理的音频的格式。</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Examples  
 下面的示例处理 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized?displayProperty=nameWithType> 或 <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized?displayProperty=nameWithType> 事件，并输出到与识别结果关联的识别音频的控制台信息。  
  
```csharp  
  
// Handle the SpeechRecognized event.   
void SpeechRecognizedHandler(object sender, SpeechRecognizedEventArgs e)  
{  
  if (e.Result == null) return;  
  
  RecognitionResult result = e.Result;  
  
  Console.WriteLine("Grammar({0}): {1}",  
    result.Grammar.Name, result.Text);  
  
  if (e.Result.Audio != null)  
  {  
    RecognizedAudio audio = e.Result.Audio;  
  
    Console.WriteLine("   start time: {0}", audio.StartTime);  
    Console.WriteLine("   encoding format: {0}", audio.Format.EncodingFormat);  
    Console.WriteLine("   position: {0}, duration: {1}",  
      audio.AudioPosition, audio.Duration);  
  }  
  
  // Add event handler code here.  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="N:System.Speech.AudioFormat" />
      </Docs>
    </Member>
    <Member MemberName="GetRange">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.RecognizedAudio GetRange (TimeSpan audioPosition, TimeSpan duration);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance class System.Speech.Recognition.RecognizedAudio GetRange(valuetype System.TimeSpan audioPosition, valuetype System.TimeSpan duration) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.RecognizedAudio.GetRange(System.TimeSpan,System.TimeSpan)" />
      <MemberSignature Language="VB.NET" Value="Public Function GetRange (audioPosition As TimeSpan, duration As TimeSpan) As RecognizedAudio" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; System::Speech::Recognition::RecognizedAudio ^ GetRange(TimeSpan audioPosition, TimeSpan duration);" />
      <MemberSignature Language="F#" Value="member this.GetRange : TimeSpan * TimeSpan -&gt; System.Speech.Recognition.RecognizedAudio" Usage="recognizedAudio.GetRange (audioPosition, duration)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.RecognizedAudio</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="audioPosition" Type="System.TimeSpan" />
        <Parameter Name="duration" Type="System.TimeSpan" />
      </Parameters>
      <Docs>
        <param name="audioPosition">将返回的起点音频数据。</param>
        <param name="duration">要返回的段的长度。</param>
        <summary>选择并返回当前已识别音频为二进制数据的部分。</summary>
        <returns>返回可识别的音频的小节，如 <paramref name="audioPosition" /> 和 <paramref name="duration" /> 所定义。</returns>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Examples  
 下面的示例为名称输入创建语音识别语法，为事件添加处理程序， <xref:System.Speech.Recognition.Grammar.SpeechRecognized> 并将语法加载到进程内语音识别器中。 然后，它将输入的名称部分的音频信息写入音频文件。 音频文件用作对象的输入 <xref:System.Speech.Synthesis.SpeechSynthesizer> ，该对象讲述了包含录制的音频的短语。  
  
```  
private static void AddNameGrammar(SpeechRecognitionEngine recognizer)  
{  
  GrammarBuilder builder = new GrammarBuilder();  
  builder.Append("My name is");  
  builder.AppendWildcard();  
  
  Grammar nameGrammar = new Grammar(builder);  
  nameGrammar.Name = "Name Grammar";  
  nameGrammar.SpeechRecognized +=  
    new EventHandler<SpeechRecognizedEventArgs>(  
      NameSpeechRecognized);  
  
  recognizer.LoadGrammar(nameGrammar);  
}  
  
// Handle the SpeechRecognized event of the name grammar.  
private static void NameSpeechRecognized(  
  object sender, SpeechRecognizedEventArgs e)  
{  
  Console.WriteLine("Grammar ({0}) recognized speech: {1}",  
    e.Result.Grammar.Name, e.Result.Text);  
  
  try  
  {  
  
    // The name phrase starts after the first three words.  
    if (e.Result.Words.Count < 4)  
    {  
  
      // Add code to check for an alternate that contains the wildcard.  
      return;  
    }  
  
    RecognizedAudio audio = e.Result.Audio;  
    TimeSpan start = e.Result.Words[3].AudioPosition;  
    TimeSpan duration = audio.Duration - start;  
  
    // Add code to verify and persist the audio.  
    string path = @"C:\temp\nameAudio.wav";  
    using (Stream outputStream = new FileStream(path, FileMode.Create))  
    {  
      RecognizedAudio nameAudio = audio.GetRange(start, duration);  
      nameAudio.WriteToWaveStream(outputStream);  
      outputStream.Close();  
    }  
  
    Thread testThread =  
      new Thread(new ParameterizedThreadStart(TestAudio));  
    testThread.Start(path);  
  }  
  catch (Exception ex)  
  {  
    Console.WriteLine("Exception thrown while processing audio:");  
    Console.WriteLine(ex.ToString());  
  }  
}  
  
// Use the speech synthesizer to play back the .wav file  
// that was created in the SpeechRecognized event handler.  
  
private static void TestAudio(object item)  
{  
  string path = item as string;  
  if (path != null && File.Exists(path))  
  {  
    SpeechSynthesizer synthesizer = new SpeechSynthesizer();  
    PromptBuilder builder = new PromptBuilder();  
    builder.AppendText("Hello");  
    builder.AppendAudio(path);  
    synthesizer.Speak(builder);  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentOutOfRangeException"><paramref name="audioPosition" /> 和 <paramref name="duration" /> 在当前段范围之外定义音频段。</exception>
        <exception cref="T:System.InvalidOperationException">识别出当前的音频不包含任何数据。</exception>
        <altmember cref="M:System.Speech.Recognition.RecognizedAudio.WriteToWaveStream(System.IO.Stream)" />
        <altmember cref="M:System.Speech.Recognition.RecognizedAudio.WriteToAudioStream(System.IO.Stream)" />
      </Docs>
    </Member>
    <Member MemberName="StartTime">
      <MemberSignature Language="C#" Value="public DateTime StartTime { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.DateTime StartTime" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.RecognizedAudio.StartTime" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property StartTime As DateTime" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property DateTime StartTime { DateTime get(); };" />
      <MemberSignature Language="F#" Value="member this.StartTime : DateTime" Usage="System.Speech.Recognition.RecognizedAudio.StartTime" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Attributes>
        <Attribute FrameworkAlternate="netframework-4.0">
          <AttributeName Language="C#">[get: System.Runtime.TargetedPatchingOptOut("Performance critical to inline this type of method across NGen image boundaries")]</AttributeName>
          <AttributeName Language="F#">[&lt;get: System.Runtime.TargetedPatchingOptOut("Performance critical to inline this type of method across NGen image boundaries")&gt;]</AttributeName>
        </Attribute>
      </Attributes>
      <ReturnValue>
        <ReturnType>System.DateTime</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>获取在标识操作开始时的系统时间。</summary>
        <value>在识别操作开始时的系统时间。</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 <xref:System.Speech.Recognition.RecognizedAudio.StartTime%2A>属性可获取识别操作开始时的系统时间，这对于延迟和性能计算非常有用。  
  
 <xref:System.Speech.Recognition.RecognizedAudio.AudioPosition%2A>属性获取输入设备生成的音频流中的位置。  
  
   
  
## Examples  
 下面的示例处理 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized?displayProperty=nameWithType> 或 <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized?displayProperty=nameWithType> 事件，并输出到与识别结果关联的识别音频的控制台信息。  
  
```csharp  
  
// Handle the SpeechRecognized event.   
void SpeechRecognizedHandler(object sender, SpeechRecognizedEventArgs e)  
{  
  if (e.Result == null) return;  
  
  RecognitionResult result = e.Result;  
  
  Console.WriteLine("Grammar({0}): {1}",  
    result.Grammar.Name, result.Text);  
  
  if (e.Result.Audio != null)  
  {  
    RecognizedAudio audio = e.Result.Audio;  
  
    Console.WriteLine("   start time: {0}", audio.StartTime);  
    Console.WriteLine("   encoding format: {0}", audio.Format.EncodingFormat);  
    Console.WriteLine("   position: {0}, duration: {1}",  
      audio.AudioPosition, audio.Duration);  
  }  
  
  // Add event handler code here.  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="P:System.Speech.Recognition.RecognizedAudio.AudioPosition" />
      </Docs>
    </Member>
    <Member MemberName="WriteToAudioStream">
      <MemberSignature Language="C#" Value="public void WriteToAudioStream (System.IO.Stream outputStream);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void WriteToAudioStream(class System.IO.Stream outputStream) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.RecognizedAudio.WriteToAudioStream(System.IO.Stream)" />
      <MemberSignature Language="VB.NET" Value="Public Sub WriteToAudioStream (outputStream As Stream)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void WriteToAudioStream(System::IO::Stream ^ outputStream);" />
      <MemberSignature Language="F#" Value="member this.WriteToAudioStream : System.IO.Stream -&gt; unit" Usage="recognizedAudio.WriteToAudioStream outputStream" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="outputStream" Type="System.IO.Stream" />
      </Parameters>
      <Docs>
        <param name="outputStream">将接收经过音频数据的流。</param>
        <summary>向一个流写入作为原始数据的整个音频。</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 音频数据以 `outputStream` 二进制形式写入。 不包含任何标头信息。  
  
 <xref:System.Speech.Recognition.RecognizedAudio.WriteToAudioStream%2A>方法使用波形格式，但不包括波形标头。 若要包含波形标头，请使用 <xref:System.Speech.Recognition.RecognizedAudio.WriteToWaveStream%2A> 方法。  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.RecognizedAudio.GetRange(System.TimeSpan,System.TimeSpan)" />
        <altmember cref="M:System.Speech.Recognition.RecognizedAudio.WriteToWaveStream(System.IO.Stream)" />
      </Docs>
    </Member>
    <Member MemberName="WriteToWaveStream">
      <MemberSignature Language="C#" Value="public void WriteToWaveStream (System.IO.Stream outputStream);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void WriteToWaveStream(class System.IO.Stream outputStream) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.RecognizedAudio.WriteToWaveStream(System.IO.Stream)" />
      <MemberSignature Language="VB.NET" Value="Public Sub WriteToWaveStream (outputStream As Stream)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void WriteToWaveStream(System::IO::Stream ^ outputStream);" />
      <MemberSignature Language="F#" Value="member this.WriteToWaveStream : System.IO.Stream -&gt; unit" Usage="recognizedAudio.WriteToWaveStream outputStream" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="outputStream" Type="System.IO.Stream" />
      </Parameters>
      <Docs>
        <param name="outputStream">将接收经过音频数据的流。</param>
        <summary>编写音频定向到波形格式的流。</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 音频数据是以 `outputStream` 波形格式写入的，其中包括资源交换文件格式 (RIFF) 标头。  
  
 <xref:System.Speech.Recognition.RecognizedAudio.WriteToAudioStream%2A>方法使用相同的二进制格式，但不包括波形标头。  
  
   
  
## Examples  
 下面的示例为名称输入创建语音识别语法，为事件添加处理程序， <xref:System.Speech.Recognition.Grammar.SpeechRecognized> 并将语法加载到进程内语音识别器中。 然后，它将输入的名称部分的音频信息写入音频文件。 音频文件用作对象的输入 <xref:System.Speech.Synthesis.SpeechSynthesizer> ，该对象讲述了包含录制的音频的短语。  
  
```  
private static void AddNameGrammar(SpeechRecognitionEngine recognizer)  
{  
  GrammarBuilder builder = new GrammarBuilder();  
  builder.Append("My name is");  
  builder.AppendWildcard();  
  
  Grammar nameGrammar = new Grammar(builder);  
  nameGrammar.Name = "Name Grammar";  
  nameGrammar.SpeechRecognized +=  
    new EventHandler<SpeechRecognizedEventArgs>(  
      NameSpeechRecognized);  
  
  recognizer.LoadGrammar(nameGrammar);  
}  
  
// Handle the SpeechRecognized event of the name grammar.  
private static void NameSpeechRecognized(  
  object sender, SpeechRecognizedEventArgs e)  
{  
  Console.WriteLine("Grammar ({0}) recognized speech: {1}",  
    e.Result.Grammar.Name, e.Result.Text);  
  
  try  
  {  
    // The name phrase starts after the first three words.  
    if (e.Result.Words.Count < 4)  
    {  
  
      // Add code to check for an alternate that contains the   
wildcard.  
      return;  
    }  
  
    RecognizedAudio audio = e.Result.Audio;  
    TimeSpan start = e.Result.Words[3].AudioPosition;  
    TimeSpan duration = audio.Duration - start;  
  
    // Add code to verify and persist the audio.  
    string path = @"C:\temp\nameAudio.wav";  
    using (Stream outputStream = new FileStream(path, FileMode.Create))  
    {  
      RecognizedAudio nameAudio = audio.GetRange(start, duration);  
      nameAudio.WriteToWaveStream(outputStream);  
      outputStream.Close();  
    }  
  
    Thread testThread =  
      new Thread(new ParameterizedThreadStart(TestAudio));  
    testThread.Start(path);  
  }  
  catch (Exception ex)  
  {  
    Console.WriteLine("Exception thrown while processing audio:");  
    Console.WriteLine(ex.ToString());  
  }  
}  
  
// Use the speech synthesizer to play back the .wav file  
// that was created in the SpeechRecognized event handler.  
  
private static void TestAudio(object item)  
{  
  string path = item as string;  
  if (path != null && File.Exists(path))  
  {  
    SpeechSynthesizer synthesizer = new SpeechSynthesizer();  
    PromptBuilder builder = new PromptBuilder();  
    builder.AppendText("Hello");  
    builder.AppendAudio(path);  
    synthesizer.Speak(builder);  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.RecognizedAudio.GetRange(System.TimeSpan,System.TimeSpan)" />
        <altmember cref="M:System.Speech.Recognition.RecognizedAudio.WriteToAudioStream(System.IO.Stream)" />
      </Docs>
    </Member>
  </Members>
</Type>
