<Type Name="SpeechRecognitionEngine" FullName="System.Speech.Recognition.SpeechRecognitionEngine">
  <Metadata><Meta Name="ms.openlocfilehash" Value="6765b110d58a92b3c61d8eb8802293962496b25a" /><Meta Name="ms.sourcegitcommit" Value="c982ad9e10a242d90f1e631b5981570f9e5d9f34" /><Meta Name="ms.translationtype" Value="MT" /><Meta Name="ms.contentlocale" Value="zh-CN" /><Meta Name="ms.lasthandoff" Value="09/10/2020" /><Meta Name="ms.locfileid" Value="89942455" /></Metadata><TypeSignature Language="C#" Value="public class SpeechRecognitionEngine : IDisposable" />
  <TypeSignature Language="ILAsm" Value=".class public auto ansi beforefieldinit SpeechRecognitionEngine extends System.Object implements class System.IDisposable" />
  <TypeSignature Language="DocId" Value="T:System.Speech.Recognition.SpeechRecognitionEngine" />
  <TypeSignature Language="VB.NET" Value="Public Class SpeechRecognitionEngine&#xA;Implements IDisposable" />
  <TypeSignature Language="C++ CLI" Value="public ref class SpeechRecognitionEngine : IDisposable" />
  <TypeSignature Language="F#" Value="type SpeechRecognitionEngine = class&#xA;    interface IDisposable" />
  <AssemblyInfo>
    <AssemblyName>System.Speech</AssemblyName>
    <AssemblyVersion>3.0.0.0</AssemblyVersion>
    <AssemblyVersion>4.0.0.0</AssemblyVersion>
  </AssemblyInfo>
  <Base>
    <BaseTypeName>System.Object</BaseTypeName>
  </Base>
  <Interfaces>
    <Interface>
      <InterfaceName>System.IDisposable</InterfaceName>
    </Interface>
  </Interfaces>
  <Docs>
    <summary>提供访问和管理进程内语音识别引擎。</summary>
    <remarks>
      <format type="text/markdown"><![CDATA[  
  
## Remarks  
 可以为任何已安装的语音识别器创建此类的实例。 若要获取有关已安装的识别器的信息，请使用静态 <xref:System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers%2A> 方法。  
  
 此类用于在进程内运行语音识别引擎，并提供对语音识别各个方面的控制，如下所示：  
  
-   若要创建进程内语音识别器，请使用其中一个 <xref:System.Speech.Recognition.SpeechRecognitionEngine.%23ctor%2A> 构造函数。  
  
-   若要管理语音识别语法，请使用 <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A> 、 <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> 、 <xref:System.Speech.Recognition.SpeechRecognitionEngine.UnloadGrammar%2A> 和 <xref:System.Speech.Recognition.SpeechRecognitionEngine.UnloadAllGrammars%2A> 方法以及 <xref:System.Speech.Recognition.SpeechRecognitionEngine.Grammars%2A> 属性。  
  
-   若要配置到识别器的输入，请使用 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream%2A> 、 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice%2A> 、 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A> 、 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile%2A> 或 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream%2A> 方法。  
  
-   若要执行语音识别，请使用 <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A> 或 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> 方法。  
  
-   若要修改识别如何处理无声或意外输入，请使用 <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A> 、 <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> 、 <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A> 和 <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> 属性。  
  
-   若要更改识别器返回的备用项的数目，请使用 <xref:System.Speech.Recognition.SpeechRecognitionEngine.MaxAlternates%2A> 属性。 识别器将识别结果返回 <xref:System.Speech.Recognition.RecognitionResult> 对象。  
  
-   若要同步对识别器所做的更改，请使用 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> 方法。 识别器使用多个线程来执行任务。  
  
-   若要模拟到识别器的输入，请使用 <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize%2A> 和 <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> 方法。  
  
 <xref:System.Speech.Recognition.SpeechRecognitionEngine>对象用于唯一使用实例化对象的进程。 与此相反， <xref:System.Speech.Recognition.SpeechRecognizer> 与要使用它的任何应用程序共享单个识别器。  
  
> [!NOTE]
>  <xref:System.Speech.Recognition.SpeechRecognitionEngine.Dispose%2A>在释放对语音识别器的最后一个引用之前，始终调用。 否则，在垃圾回收器调用识别器对象的方法之前，不会释放它所使用的资源 `Finalize` 。  
  
   
  
## Examples  
 下面的示例演示了演示基本语音识别的控制台应用程序的一部分。 由于本示例使用 `Multiple` 方法的模式 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> ，它会执行识别，直到关闭控制台窗口或停止调试。  
  
```csharp  
using System;  
using System.Speech.Recognition;  
  
namespace SpeechRecognitionApp  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Create an in-process speech recognizer for the en-US locale.  
      using (  
      SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(  
          new System.Globalization.CultureInfo("en-US")))  
      {  
  
        // Create and load a dictation grammar.  
        recognizer.LoadGrammar(new DictationGrammar());  
  
        // Add a handler for the speech recognized event.  
        recognizer.SpeechRecognized +=   
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start asynchronous, continuous speech recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Keep the console window open.  
        while (true)  
        {  
          Console.ReadLine();  
        }  
      }  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Recognized text: " + e.Result.Text);  
    }  
  }  
}  
```  
  
 ]]></format>
    </remarks>
    <altmember cref="T:System.Speech.Recognition.Grammar" />
    <altmember cref="T:System.Speech.Recognition.SpeechRecognizer" />
  </Docs>
  <Members>
    <MemberGroup MemberName=".ctor">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>初始化 <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> 类的新实例。</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 你可以 <xref:System.Speech.Recognition.SpeechRecognitionEngine> 从以下任意项构造实例：  
  
-   系统的默认语音识别引擎  
  
-   按名称指定的特定语音识别引擎  
  
-   指定的区域设置的默认语音识别引擎  
  
-   满足您在对象中指定的条件的特定识别引擎 <xref:System.Speech.Recognition.RecognizerInfo> 。  
  
 在语音识别器开始识别之前，必须至少加载一个语音识别语法，并为识别器配置输入。  
  
 若要加载语法，请调用 <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A> 或 <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> 方法。  
  
 若要配置音频输入，请使用以下方法之一：  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream%2A>  
  
 ]]></format>
        </remarks>
      </Docs>
    </MemberGroup>
    <Member MemberName=".ctor">
      <MemberSignature Language="C#" Value="public SpeechRecognitionEngine ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig specialname rtspecialname instance void .ctor() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.#ctor" />
      <MemberSignature Language="VB.NET" Value="Public Sub New ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; SpeechRecognitionEngine();" />
      <MemberType>Constructor</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Parameters />
      <Docs>
        <summary>使用系统的默认语音识别器来初始化 <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> 类的新实例。</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 在语音识别器开始语音识别之前，必须至少加载一个识别语法，并为识别器配置输入。  
  
 若要加载语法，请调用 <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A> 或 <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> 方法。  
  
 若要配置音频输入，请使用以下方法之一：  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream%2A>  
  
 ]]></format>
        </remarks>
      </Docs>
    </Member>
    <Member MemberName=".ctor">
      <MemberSignature Language="C#" Value="public SpeechRecognitionEngine (System.Globalization.CultureInfo culture);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig specialname rtspecialname instance void .ctor(class System.Globalization.CultureInfo culture) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.#ctor(System.Globalization.CultureInfo)" />
      <MemberSignature Language="VB.NET" Value="Public Sub New (culture As CultureInfo)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; SpeechRecognitionEngine(System::Globalization::CultureInfo ^ culture);" />
      <MemberSignature Language="F#" Value="new System.Speech.Recognition.SpeechRecognitionEngine : System.Globalization.CultureInfo -&gt; System.Speech.Recognition.SpeechRecognitionEngine" Usage="new System.Speech.Recognition.SpeechRecognitionEngine culture" />
      <MemberType>Constructor</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Parameters>
        <Parameter Name="culture" Type="System.Globalization.CultureInfo" />
      </Parameters>
      <Docs>
        <param name="culture">语音识别器必须支持的区域设置。</param>
        <summary>使用指定的区域设置的默认语音识别器来初始化 <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> 类的新实例。</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Microsoft Windows 和 Speech API 接受所有有效的语言-国家/地区代码。 若要使用参数中指定的语言执行语音识别 `CultureInfo` ，则必须安装支持该语言-国家/地区代码的语音识别引擎。 Microsoft Windows 7 随附的语音识别引擎使用以下语言-国家/地区代码。  
  
-   半 GB。 英语（英国）  
  
-   en-us。 英语（美国）  
  
-   取消。 德语（德国）  
  
-   es。 西班牙语(西班牙)  
  
-   fr-fr。 法语（法国）  
  
-   ja-jp。 日语（日本）  
  
-   zh-chs-CN。 中文（中国）  
  
-   zh-chs-幼圆。 中文（台湾）  
  
 还允许使用两个字母的语言代码，例如 "en"、"fr" 或 "es"。  
  
 在语音识别器开始识别之前，必须至少加载一个语音识别语法，并为识别器配置输入。  
  
 若要加载语法，请调用 <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A> 或 <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> 方法。  
  
 若要配置音频输入，请使用以下方法之一：  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream%2A>  
  
   
  
## Examples  
 下面的示例演示了演示基本语音识别的控制台应用程序的一部分，并为 en-us 区域设置初始化了语音识别器。  
  
```csharp  
using System;  
using System.Speech.Recognition;  
  
namespace SpeechRecognitionApp  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Create an in-process speech recognizer for the en-US locale.  
      using (  
      SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(  
          new System.Globalization.CultureInfo("en-US")))  
      {  
  
        // Create and load a dictation grammar.  
        recognizer.LoadGrammar(new DictationGrammar());  
  
        // Add a handler for the speech recognized event.  
        recognizer.SpeechRecognized +=   
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start asynchronous, continuous speech recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Keep the console window open.  
        while (true)  
        {  
          Console.ReadLine();  
        }  
      }  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Recognized text: " + e.Result.Text);  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentException">未安装的语音识别器都不支持区域设置，或者 <paramref name="culture" /> 为固定区域性。</exception>
        <exception cref="T:System.ArgumentNullException"><paramref name="Culture" /> 为 <see langword="null" />。</exception>
      </Docs>
    </Member>
    <Member MemberName=".ctor">
      <MemberSignature Language="C#" Value="public SpeechRecognitionEngine (System.Speech.Recognition.RecognizerInfo recognizerInfo);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig specialname rtspecialname instance void .ctor(class System.Speech.Recognition.RecognizerInfo recognizerInfo) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.#ctor(System.Speech.Recognition.RecognizerInfo)" />
      <MemberSignature Language="VB.NET" Value="Public Sub New (recognizerInfo As RecognizerInfo)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; SpeechRecognitionEngine(System::Speech::Recognition::RecognizerInfo ^ recognizerInfo);" />
      <MemberSignature Language="F#" Value="new System.Speech.Recognition.SpeechRecognitionEngine : System.Speech.Recognition.RecognizerInfo -&gt; System.Speech.Recognition.SpeechRecognitionEngine" Usage="new System.Speech.Recognition.SpeechRecognitionEngine recognizerInfo" />
      <MemberType>Constructor</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Parameters>
        <Parameter Name="recognizerInfo" Type="System.Speech.Recognition.RecognizerInfo" />
      </Parameters>
      <Docs>
        <param name="recognizerInfo">特定语音识别器的信息。</param>
        <summary>用 <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> 对象中的信息初始化  <see cref="T:System.Speech.Recognition.RecognizerInfo" /> 来指定要使用的识别器。</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 可以为任何已安装的语音识别器创建此类的实例。 若要获取有关已安装的识别器的信息，请使用 <xref:System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers%2A> 方法。  
  
 在语音识别器开始识别之前，必须至少加载一个语音识别语法，并为识别器配置输入。  
  
 若要加载语法，请调用 <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A> 或 <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> 方法。  
  
 若要配置音频输入，请使用以下方法之一：  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream%2A>  
  
   
  
## Examples  
 下面的示例演示了演示基本语音识别的控制台应用程序的一部分，并初始化了支持英语的语音识别器。  
  
```csharp  
 using System;  
using System.Speech.Recognition;  
  
namespace SpeechRecognitionApp  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Select a speech recognizer that supports English.  
      RecognizerInfo info = null;  
      foreach (RecognizerInfo ri in SpeechRecognitionEngine.InstalledRecognizers())  
      {  
        if (ri.Culture.TwoLetterISOLanguageName.Equals("en"))  
        {  
          info = ri;  
          break;  
        }  
      }  
      if (info == null) return;  
  
      // Create the selected recognizer.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(info))  
      {  
  
        // Create and load a dictation grammar.  
        recognizer.LoadGrammar(new DictationGrammar());  
  
        // Add a handler for the speech recognized event.  
        recognizer.SpeechRecognized +=   
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start asynchronous, continuous speech recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Keep the console window open.  
        while (true)  
        {  
          Console.ReadLine();  
        }  
      }  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Recognized text: " + e.Result.Text);  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
      </Docs>
    </Member>
    <Member MemberName=".ctor">
      <MemberSignature Language="C#" Value="public SpeechRecognitionEngine (string recognizerId);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig specialname rtspecialname instance void .ctor(string recognizerId) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.#ctor(System.String)" />
      <MemberSignature Language="VB.NET" Value="Public Sub New (recognizerId As String)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; SpeechRecognitionEngine(System::String ^ recognizerId);" />
      <MemberSignature Language="F#" Value="new System.Speech.Recognition.SpeechRecognitionEngine : string -&gt; System.Speech.Recognition.SpeechRecognitionEngine" Usage="new System.Speech.Recognition.SpeechRecognitionEngine recognizerId" />
      <MemberType>Constructor</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Parameters>
        <Parameter Name="recognizerId" Type="System.String" />
      </Parameters>
      <Docs>
        <param name="recognizerId">使用一个识别器的标记名称。</param>
        <summary>使用字符串参数初始化 <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> 类的新实例，该参数指定要使用的识别器的名称。</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 识别器的标记名称是 <xref:System.Speech.Recognition.RecognizerInfo.Id%2A> <xref:System.Speech.Recognition.RecognizerInfo> 由识别器的属性返回的对象的属性值 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerInfo%2A> 。 若要获取所有已安装的识别器的集合，请使用静态 <xref:System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers%2A> 方法。  
  
 在语音识别器开始识别之前，必须至少加载一个语音识别语法，并为识别器配置输入。  
  
 若要加载语法，请调用 <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A> 或 <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> 方法。  
  
 若要配置音频输入，请使用以下方法之一：  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream%2A>  
  
   
  
## Examples  
 下面的示例演示了演示基本语音识别的控制台应用程序的一部分，并创建了语音识别 8.0 for Windows (英语-US) 的实例。  
  
```csharp  
using System;  
using System.Speech.Recognition;  
  
namespace SpeechRecognitionApp  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Create an instance of the Microsoft Speech Recognizer 8.0 for  
      // Windows (English - US).  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine("MS-1033-80-DESK"))  
      {  
  
        // Create and load a dictation grammar.  
        recognizer.LoadGrammar(new DictationGrammar());  
  
        // Add a handler for the speech recognized event.  
        recognizer.SpeechRecognized += new EventHandler(recognizer_SpeechRecognized);  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start asynchronous, continuous speech recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Keep the console window open.  
        while (true)  
        {  
          Console.ReadLine();  
        }  
      }  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Recognized text: " + e.Result.Text);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentException">没有带有此标记名称的语音识别器未安装，或者 <paramref name="recognizerId" /> 是空字符串 ("")。</exception>
        <exception cref="T:System.ArgumentNullException"><paramref name="recognizerId" /> 为 <see langword="null" />。</exception>
      </Docs>
    </Member>
    <Member MemberName="AudioFormat">
      <MemberSignature Language="C#" Value="public System.Speech.AudioFormat.SpeechAudioFormatInfo AudioFormat { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance class System.Speech.AudioFormat.SpeechAudioFormatInfo AudioFormat" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioFormat" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property AudioFormat As SpeechAudioFormatInfo" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property System::Speech::AudioFormat::SpeechAudioFormatInfo ^ AudioFormat { System::Speech::AudioFormat::SpeechAudioFormatInfo ^ get(); };" />
      <MemberSignature Language="F#" Value="member this.AudioFormat : System.Speech.AudioFormat.SpeechAudioFormatInfo" Usage="System.Speech.Recognition.SpeechRecognitionEngine.AudioFormat" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.AudioFormat.SpeechAudioFormatInfo</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>获取 <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> 接收的音频格式。</summary>
        <value>音频格式于实例 <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> 输入或 <see langword="null" /> ，如果输入不配置或设置为无效输入。</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 若要配置音频输入，请使用以下方法之一：  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream%2A>  
  
   
  
## Examples  
 下面的示例使用 <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioFormat%2A> 获取并显示音频格式数据。  
  
```  
static void DisplayAudioDeviceFormat(Label label, SpeechRecognitionEngine recognitionEngine)   
{  
  
  if (recognitionEngine != null && label != null)   
  {  
    label.Text = String.Format("Encoding Format:         {0}\n" +  
          "AverageBytesPerSecond    {1}\n" +  
          "BitsPerSample            {2}\n" +  
          "BlockAlign               {3}\n" +  
          "ChannelCount             {4}\n" +  
          "SamplesPerSecond         {5}",  
          recognitionEngine.AudioFormat.EncodingFormat.ToString(),  
          recognitionEngine.AudioFormat.AverageBytesPerSecond,  
          recognitionEngine.AudioFormat.BitsPerSample,  
          recognitionEngine.AudioFormat.BlockAlign,  
          recognitionEngine.AudioFormat.ChannelCount,  
          recognitionEngine.AudioFormat.SamplesPerSecond);  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.AudioFormat.SpeechAudioFormatInfo" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.AudioFormat" />
      </Docs>
    </Member>
    <Member MemberName="AudioLevel">
      <MemberSignature Language="C#" Value="public int AudioLevel { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance int32 AudioLevel" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevel" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property AudioLevel As Integer" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property int AudioLevel { int get(); };" />
      <MemberSignature Language="F#" Value="member this.AudioLevel : int" Usage="System.Speech.Recognition.SpeechRecognitionEngine.AudioLevel" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Int32</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>获取 <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> 接收的音频级别.</summary>
        <value>输入的音频级别到语音识别器，从 0 到 100。</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 值0表示无声，100表示最大输入量。  
  
 ]]></format>
        </remarks>
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevelUpdated" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.AudioLevel" />
      </Docs>
    </Member>
    <Member MemberName="AudioLevelUpdated">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.AudioLevelUpdatedEventArgs&gt; AudioLevelUpdated;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.AudioLevelUpdatedEventArgs&gt; AudioLevelUpdated" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevelUpdated" />
      <MemberSignature Language="VB.NET" Value="Public Custom Event AudioLevelUpdated As EventHandler(Of AudioLevelUpdatedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::AudioLevelUpdatedEventArgs ^&gt; ^ AudioLevelUpdated;" />
      <MemberSignature Language="F#" Value="member this.AudioLevelUpdated : EventHandler&lt;System.Speech.Recognition.AudioLevelUpdatedEventArgs&gt; " Usage="member this.AudioLevelUpdated : System.EventHandler&lt;System.Speech.Recognition.AudioLevelUpdatedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.AudioLevelUpdatedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>当 <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> 报告其音频输入的级别的时候引发。</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 <xref:System.Speech.Recognition.SpeechRecognitionEngine>每秒引发此事件多次。 引发事件的频率取决于运行应用程序的计算机。  
  
 若要在事件发生时获取音频级别，请使用关联的的 <xref:System.Speech.Recognition.AudioLevelUpdatedEventArgs.AudioLevel%2A> 属性 <xref:System.Speech.Recognition.AudioLevelUpdatedEventArgs> 。 若要获取识别器的当前音频级别，请使用识别器的 <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevel%2A> 属性。  
  
 创建 <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevelUpdated> 委托时，需要标识将要处理该事件的方法。 若要将事件与事件处理程序关联，请将该委托的一个实例添加到事件中。 除非移除了该委托，否则每当发生该事件时就会调用事件处理程序。 有关事件处理程序委托的详细信息，请参阅 [事件和委托](https://go.microsoft.com/fwlink/?LinkId=162418)。  
  
   
  
## Examples  
 下面的示例将事件的处理程序添加 <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevelUpdated> 到 <xref:System.Speech.Recognition.SpeechRecognitionEngine> 对象。 处理程序将新的音频级别输出到控制台。  
  
```  
private SpeechRecognitionEngine recognizer;  
  
// Initialize the SpeechRecognitionEngine object.   
private void Initialize()  
{  
  recognizer = new SpeechRecognitionEngine();  
  
  // Add an event handler for the AudioLevelUpdated event.  
  recognizer.AudioLevelUpdated +=   
   new EventHandler<AudioLevelUpdatedEventArgs>(recognizer_AudioLevelUpdated);  
  
  // Add other initialization code here.  
  
}  
  
// Write the audio level to the console when the AudioLevelUpdated event is raised.  
void recognizer_AudioLevelUpdated(object sender, AudioLevelUpdatedEventArgs e)  
{  
  Console.WriteLine("The audio level is now: {0}.", e.AudioLevel);  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.AudioLevelUpdatedEventArgs" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevel" />
      </Docs>
    </Member>
    <Member MemberName="AudioPosition">
      <MemberSignature Language="C#" Value="public TimeSpan AudioPosition { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.TimeSpan AudioPosition" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property AudioPosition As TimeSpan" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property TimeSpan AudioPosition { TimeSpan get(); };" />
      <MemberSignature Language="F#" Value="member this.AudioPosition : TimeSpan" Usage="System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.TimeSpan</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>获取音频流中的当前位置，此音频流由向 <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> 提供输入的设备产生。</summary>
        <value>输入设备生成的音频流中的当前位置。</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A>属性引用输入设备在其生成的音频流中的位置。 与此相反，该 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A> 属性引用识别器在其音频输入中的位置。 这些位置可能不同。 例如，如果识别器收到了尚未生成识别结果的输入，则该属性的值 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A> 小于属性的值 <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A> 。  
  
   
  
## Examples  
 在以下示例中，进程内语音识别器使用听写语法来匹配语音输入。 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected> <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A> <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A> <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevel%2A> 当语音识别器在其输入时检测到语音时，事件的处理程序会将、和写入控制台。  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    private static SpeechRecognitionEngine recognizer;  
    public static void Main(string[] args)  
    {  
  
      // Initialize an in-process speech recognition engine for US English.  
      using (recognizer = new SpeechRecognitionEngine(  
        new System.Globalization.CultureInfo("en-US")))  
      {  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Create a grammar for finding services in different cities.  
        Choices services = new Choices(new string[] { "restaurants", "hotels", "gas stations" });  
        Choices cities = new Choices(new string[] { "Seattle", "Boston", "Dallas" });  
  
        GrammarBuilder findServices = new GrammarBuilder("Find");  
        findServices.Append(services);  
        findServices.Append("near");  
        findServices.Append(cities);  
  
        // Create a Grammar object from the GrammarBuilder and load it to the recognizer.  
        Grammar servicesGrammar = new Grammar(findServices);  
        recognizer.LoadGrammarAsync(servicesGrammar);  
  
        // Add handlers for events.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
        recognizer.SpeechDetected +=  
          new EventHandler<SpeechDetectedEventArgs>(recognizer_SpeechDetected);  
  
        // Start asynchronous recognition.  
        recognizer.RecognizeAsync();  
        Console.WriteLine("Starting asynchronous recognition...");  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
    }  
  
    // Gather information about detected speech and write it to the console.  
    static void recognizer_SpeechDetected(object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine();  
      Console.WriteLine("Speech detected:");  
      Console.WriteLine("  Audio level: " + recognizer.AudioLevel);  
      Console.WriteLine("  Audio position at the event: " + e.AudioPosition);  
      Console.WriteLine("  Current audio position: " + recognizer.AudioPosition);  
      Console.WriteLine("  Current recognizer audio position: " +   
        recognizer.RecognizerAudioPosition);  
    }  
  
    // Write the text of the recognition result to the console.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("\nSpeech recognized: " + e.Result.Text);  
  
      // Add event handler code here.  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.AudioPosition" />
      </Docs>
    </Member>
    <Member MemberName="AudioSignalProblemOccurred">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.AudioSignalProblemOccurredEventArgs&gt; AudioSignalProblemOccurred;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.AudioSignalProblemOccurredEventArgs&gt; AudioSignalProblemOccurred" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.AudioSignalProblemOccurred" />
      <MemberSignature Language="VB.NET" Value="Public Custom Event AudioSignalProblemOccurred As EventHandler(Of AudioSignalProblemOccurredEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::AudioSignalProblemOccurredEventArgs ^&gt; ^ AudioSignalProblemOccurred;" />
      <MemberSignature Language="F#" Value="member this.AudioSignalProblemOccurred : EventHandler&lt;System.Speech.Recognition.AudioSignalProblemOccurredEventArgs&gt; " Usage="member this.AudioSignalProblemOccurred : System.EventHandler&lt;System.Speech.Recognition.AudioSignalProblemOccurredEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.AudioSignalProblemOccurredEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>当 <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> 检测在音频信号的问题的时候引发。</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 若要获取发生的问题，请使用关联的的 <xref:System.Speech.Recognition.AudioSignalProblemOccurredEventArgs.AudioSignalProblem%2A> 属性 <xref:System.Speech.Recognition.AudioSignalProblemOccurredEventArgs> 。  
  
 创建 <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioSignalProblemOccurred> 委托时，需要标识将要处理该事件的方法。 若要将事件与事件处理程序关联，请将该委托的一个实例添加到事件中。 除非移除了该委托，否则每当发生该事件时就会调用事件处理程序。 有关事件处理程序委托的详细信息，请参阅 [事件和委托](https://go.microsoft.com/fwlink/?LinkId=162418)。  
  
   
  
## Examples  
 下面的示例定义了一个事件处理程序，用于收集有关事件的信息 <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioSignalProblemOccurred> 。  
  
```  
private SpeechRecognitionEngine recognizer;  
  
// Initialize the speech recognition engine.  
private void Initialize()  
{  
  recognizer = new SpeechRecognitionEngine();  
  
  // Add a handler for the AudioSignalProblemOccurred event.  
  recognizer.AudioSignalProblemOccurred +=   
    new EventHandler<AudioSignalProblemOccurredEventArgs>(  
      recognizer_AudioSignalProblemOccurred);  
}  
  
// Gather information when the AudioSignalProblemOccurred event is raised.  
void recognizer_AudioSignalProblemOccurred(object sender, AudioSignalProblemOccurredEventArgs e)  
{  
  StringBuilder details = new StringBuilder();  
  
  details.AppendLine("Audio signal problem information:");  
  details.AppendFormat(  
    " Audio level:               {0}" + Environment.NewLine +  
    " Audio position:            {1}" + Environment.NewLine +  
    " Audio signal problem:      {2}" + Environment.NewLine +  
    " Recognition engine audio position: {3}" + Environment.NewLine,  
    e.AudioLevel, e.AudioPosition,  e.AudioSignalProblem,  
    e.recoEngineAudioPosition);  
  
  // Insert additional event handler code here.  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.AudioSignalProblem" />
        <altmember cref="T:System.Speech.Recognition.AudioSignalProblemOccurredEventArgs" />
      </Docs>
    </Member>
    <Member MemberName="AudioState">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.AudioState AudioState { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.Speech.Recognition.AudioState AudioState" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioState" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property AudioState As AudioState" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property System::Speech::Recognition::AudioState AudioState { System::Speech::Recognition::AudioState get(); };" />
      <MemberSignature Language="F#" Value="member this.AudioState : System.Speech.Recognition.AudioState" Usage="System.Speech.Recognition.SpeechRecognitionEngine.AudioState" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.AudioState</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>获取 <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />接收的状态音频.</summary>
        <value>语音识别器处理的音频输入的状态。</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioState%2A>属性表示具有枚举成员的音频状态 <xref:System.Speech.Recognition.AudioState> 。  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.AudioState" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.AudioStateChanged" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.AudioState" />
      </Docs>
    </Member>
    <Member MemberName="AudioStateChanged">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.AudioStateChangedEventArgs&gt; AudioStateChanged;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.AudioStateChangedEventArgs&gt; AudioStateChanged" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.AudioStateChanged" />
      <MemberSignature Language="VB.NET" Value="Public Custom Event AudioStateChanged As EventHandler(Of AudioStateChangedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::AudioStateChangedEventArgs ^&gt; ^ AudioStateChanged;" />
      <MemberSignature Language="F#" Value="member this.AudioStateChanged : EventHandler&lt;System.Speech.Recognition.AudioStateChangedEventArgs&gt; " Usage="member this.AudioStateChanged : System.EventHandler&lt;System.Speech.Recognition.AudioStateChangedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.AudioStateChangedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>当状态在 <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> 接收的音频更改的时候引发。</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 若要获取事件时的音频状态，请使用关联的的 <xref:System.Speech.Recognition.AudioStateChangedEventArgs.AudioState%2A> 属性 <xref:System.Speech.Recognition.AudioStateChangedEventArgs> 。 若要获取识别器的当前音频状态，请使用识别器的 <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioState%2A> 属性。 有关音频状态的详细信息，请参阅 <xref:System.Speech.Recognition.AudioState> 枚举。  
  
 创建 <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioStateChanged> 委托时，需要标识将要处理该事件的方法。 若要将事件与事件处理程序关联，请将该委托的一个实例添加到事件中。 除非移除了该委托，否则每当发生该事件时就会调用事件处理程序。 有关事件处理程序委托的详细信息，请参阅 [事件和委托](https://go.microsoft.com/fwlink/?LinkId=162418)。  
  
   
  
## Examples  
 下面的示例使用事件的处理程序 <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioStateChanged> ，在每次更改时将识别器的新写入 <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioState%2A> 控制台，并使用枚举的成员 <xref:System.Speech.Recognition.AudioState> 。  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
  
    // Initialize an in-process speech recognition engine.  
    {  
      using (SpeechRecognitionEngine recognizer =  
         new SpeechRecognitionEngine(new System.Globalization.CultureInfo("en-US")))  
      {  
  
        // Create and load a grammar.  
        Choices animals = new Choices(new string[] { "cow", "pig", "goat" });  
        GrammarBuilder farm = new GrammarBuilder("On this farm he had a");  
        farm.Append(animals);  
        Grammar farmAnimals = new Grammar(farm);  
        farmAnimals.Name = "Farm";  
        recognizer.LoadGrammar(farmAnimals);  
  
        // Attach event handlers.  
        recognizer.AudioStateChanged +=  
          new EventHandler<AudioStateChangedEventArgs>(recognizer_AudioStateChanged);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
        recognizer.LoadGrammarCompleted +=  
          new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
  
        // Set the input to the recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start recognition.  
        recognizer.RecognizeAsync();  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
    }  
  
    // Handle the LoadGrammarCompleted event.  
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      Console.WriteLine("Grammar loaded: " + e.Grammar.Name);  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      if (e.Result != null && e.Result.Text != null)  
      {  
        Console.WriteLine();  
        Console.WriteLine("  Recognized text =  {0}", e.Result.Text);  
        Console.WriteLine();  
      }  
      else  
      {  
        Console.WriteLine("  Recognized text not available.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Done.");  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the AudioStateChanged event.  
    static void recognizer_AudioStateChanged(object sender, AudioStateChangedEventArgs e)  
    {  
      Console.WriteLine("The new audio state is: " + e.AudioState);  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.AudioState" />
        <altmember cref="T:System.Speech.Recognition.AudioStateChangedEventArgs" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioState" />
      </Docs>
    </Member>
    <Member MemberName="BabbleTimeout">
      <MemberSignature Language="C#" Value="public TimeSpan BabbleTimeout { get; set; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.TimeSpan BabbleTimeout" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout" />
      <MemberSignature Language="VB.NET" Value="Public Property BabbleTimeout As TimeSpan" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property TimeSpan BabbleTimeout { TimeSpan get(); void set(TimeSpan value); };" />
      <MemberSignature Language="F#" Value="member this.BabbleTimeout : TimeSpan with get, set" Usage="System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Attributes>
        <Attribute>
          <AttributeName Language="C#">[System.ComponentModel.EditorBrowsable(System.ComponentModel.EditorBrowsableState.Advanced)]</AttributeName>
          <AttributeName Language="F#">[&lt;System.ComponentModel.EditorBrowsable(System.ComponentModel.EditorBrowsableState.Advanced)&gt;]</AttributeName>
        </Attribute>
      </Attributes>
      <ReturnValue>
        <ReturnType>System.TimeSpan</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>获取或设置时间间隔，在此时间间隔内，<see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> 将在终结识别之前接受仅包含背景噪音的输入。</summary>
        <value>计时器间隔的持续时间。</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 每个语音识别器都有一种算法来区分无声和语音。 识别器会将与任何识别器已加载和启用的语音识别语法的初始规则不匹配的任何非无声输入分类为背景噪音。 如果识别器仅在干扰超时间隔内接收到后台噪音和静默，则识别器将终结识别操作。  
  
-   对于异步识别操作，识别器引发 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> 事件，其中 <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.BabbleTimeout%2A?displayProperty=nameWithType> 属性为 `true` ， <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.Result%2A?displayProperty=nameWithType> 属性为 `null` 。  
  
-   对于同步识别操作和仿真，识别器将返回 `null` ，而不是有效的 <xref:System.Speech.Recognition.RecognitionResult> 。  
  
 如果干扰超时期限设置为0，则识别器不执行干扰超时检查。 超时间隔可以是任何非负值。 默认值为0秒。  
  
   
  
## Examples  
 下面的示例演示了控制台应用程序的一部分，该应用程序演示了在 <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A> <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> 启动语音识别之前设置的和属性的基本语音识别 <xref:System.Speech.Recognition.SpeechRecognitionEngine> 。 语音识别器的和事件的处理程序将 <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioStateChanged> <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> 事件信息输出到控制台，以演示 <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> 的属性如何 <xref:System.Speech.Recognition.SpeechRecognitionEngine> 影响识别操作。  
  
```csharp  
  
using System;  
using System.Speech.Recognition;  
  
namespace SpeechRecognitionApp  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Initialize an in-process speech recognizer.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(  
          new System.Globalization.CultureInfo("en-US")))  
      {  
        // Load a Grammar object.  
        recognizer.LoadGrammar(CreateServicesGrammar("FindServices"));  
  
        // Add event handlers.  
        recognizer.AudioStateChanged +=  
          new EventHandler<AudioStateChangedEventArgs>(  
            AudioStateChangedHandler);  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(  
            RecognizeCompletedHandler);  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        recognizer.InitialSilenceTimeout = TimeSpan.FromSeconds(3);  
        recognizer.BabbleTimeout = TimeSpan.FromSeconds(2);  
        recognizer.EndSilenceTimeout = TimeSpan.FromSeconds(1);  
        recognizer.EndSilenceTimeoutAmbiguous = TimeSpan.FromSeconds(1.5);  
  
        Console.WriteLine("BabbleTimeout: {0}", recognizer.BabbleTimeout);  
        Console.WriteLine("InitialSilenceTimeout: {0}", recognizer.InitialSilenceTimeout);  
        Console.WriteLine("EndSilenceTimeout: {0}", recognizer.EndSilenceTimeout);  
        Console.WriteLine("EndSilenceTimeoutAmbiguous: {0}", recognizer.EndSilenceTimeoutAmbiguous);  
        Console.WriteLine();  
  
        // Start asynchronous speech recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Single);  
  
        // Keep the console window open.  
        while (true)  
        {  
          Console.ReadLine();  
        }  
      }  
    }  
  
    // Create a grammar and build it into a Grammar object.   
    static Grammar CreateServicesGrammar(string grammarName)  
    {  
  
      // Create a grammar for finding services in different cities.  
      Choices services = new Choices(new string[] { "restaurants", "hotels", "gas stations" });  
      Choices cities = new Choices(new string[] { "Seattle", "Boston", "Dallas" });  
  
      GrammarBuilder findServices = new GrammarBuilder("Find");  
      findServices.Append(services);  
      findServices.Append("near");  
      findServices.Append(cities);  
  
      // Create a Grammar object from the GrammarBuilder. 
      Grammar servicesGrammar = new Grammar(findServices);  
      servicesGrammar.Name = ("FindServices");  
      return servicesGrammar;  
    }  
  
    // Handle the AudioStateChanged event.  
    static void AudioStateChangedHandler(  
      object sender, AudioStateChangedEventArgs e)  
    {  
      Console.WriteLine("AudioStateChanged ({0}): {1}",  
        DateTime.Now.ToString("mm:ss.f"), e.AudioState);  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void RecognizeCompletedHandler(  
      object sender, RecognizeCompletedEventArgs e)  
    {  
      Console.WriteLine("RecognizeCompleted ({0}):",  
        DateTime.Now.ToString("mm:ss.f"));  
  
      string resultText;  
      if (e.Result != null) { resultText = e.Result.Text; }  
      else { resultText = "<null>"; }  
  
      Console.WriteLine(  
        " BabbleTimeout: {0}; InitialSilenceTimeout: {1}; Result text: {2}",  
        e.BabbleTimeout, e.InitialSilenceTimeout, resultText);  
      if (e.Error != null)  
      {  
        Console.WriteLine(" Exception message: ", e.Error.Message);  
      }  
  
      // Start the next asynchronous recognition operation.  
      ((SpeechRecognitionEngine)sender).RecognizeAsync(RecognizeMode.Single);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentOutOfRangeException">该属性已设置为小于 0 秒。</exception>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
      </Docs>
    </Member>
    <MemberGroup MemberName="Dispose">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>处置此 <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> 对象。</summary>
      </Docs>
    </MemberGroup>
    <Member MemberName="Dispose">
      <MemberSignature Language="C#" Value="public void Dispose ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig newslot virtual instance void Dispose() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.Dispose" />
      <MemberSignature Language="VB.NET" Value="Public Sub Dispose ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; virtual void Dispose();" />
      <MemberSignature Language="F#" Value="abstract member Dispose : unit -&gt; unit&#xA;override this.Dispose : unit -&gt; unit" Usage="speechRecognitionEngine.Dispose " />
      <MemberType>Method</MemberType>
      <Implements>
        <InterfaceMember>M:System.IDisposable.Dispose</InterfaceMember>
      </Implements>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>处置此 <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> 对象。</summary>
        <remarks>To be added.</remarks>
      </Docs>
    </Member>
    <Member MemberName="Dispose">
      <MemberSignature Language="C#" Value="protected virtual void Dispose (bool disposing);" />
      <MemberSignature Language="ILAsm" Value=".method familyhidebysig newslot virtual instance void Dispose(bool disposing) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.Dispose(System.Boolean)" />
      <MemberSignature Language="VB.NET" Value="Protected Overridable Sub Dispose (disposing As Boolean)" />
      <MemberSignature Language="C++ CLI" Value="protected:&#xA; virtual void Dispose(bool disposing);" />
      <MemberSignature Language="F#" Value="abstract member Dispose : bool -&gt; unit&#xA;override this.Dispose : bool -&gt; unit" Usage="speechRecognitionEngine.Dispose disposing" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="disposing" Type="System.Boolean" />
      </Parameters>
      <Docs>
        <param name="disposing">若要释放托管资源和非托管资源，则为 <see langword="true" />；若仅释放非托管资源，则为 <see langword="false" />。</param>
        <summary>处置 <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> 对象并释放会话期间使用的资源。</summary>
        <remarks>To be added.</remarks>
      </Docs>
    </Member>
    <MemberGroup MemberName="EmulateRecognize">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>使用文本代替同步语音识别的音频来模拟输入到语音识别器。</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 这些方法绕过系统音频输入，并以 <xref:System.String> 对象或对象数组的形式向识别器提供文本 <xref:System.Speech.Recognition.RecognizedWordUnit> 。 这在测试或调试应用程序或语法时非常有用。 例如，您可以使用模拟来确定单词是否在语法中，以及在识别字词时返回的语义。 使用 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A> 方法在仿真操作期间禁用语音识别引擎的音频输入。  
  
 语音识别器会引发 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected> 、 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized> 、 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> 和 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> 事件，就好像未模拟识别操作一样。 识别器忽略新行和额外的空格，并将标点符号视为文本输入。  
  
> [!NOTE]
>  <xref:System.Speech.Recognition.RecognitionResult>语音识别器为响应仿真输入而生成的对象的 `null` 属性值为 <xref:System.Speech.Recognition.RecognitionResult.Audio%2A> 。  
  
 若要模拟异步识别，请使用 <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> 方法。  
  
 ]]></format>
        </remarks>
      </Docs>
    </MemberGroup>
    <Member MemberName="EmulateRecognize">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.RecognitionResult EmulateRecognize (string inputText);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance class System.Speech.Recognition.RecognitionResult EmulateRecognize(string inputText) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String)" />
      <MemberSignature Language="VB.NET" Value="Public Function EmulateRecognize (inputText As String) As RecognitionResult" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; System::Speech::Recognition::RecognitionResult ^ EmulateRecognize(System::String ^ inputText);" />
      <MemberSignature Language="F#" Value="member this.EmulateRecognize : string -&gt; System.Speech.Recognition.RecognitionResult" Usage="speechRecognitionEngine.EmulateRecognize inputText" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Attributes>
        <Attribute FrameworkAlternate="netframework-4.0">
          <AttributeName Language="C#">[System.Runtime.TargetedPatchingOptOut("Performance critical to inline this type of method across NGen image boundaries")]</AttributeName>
          <AttributeName Language="F#">[&lt;System.Runtime.TargetedPatchingOptOut("Performance critical to inline this type of method across NGen image boundaries")&gt;]</AttributeName>
        </Attribute>
      </Attributes>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.RecognitionResult</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="inputText" Type="System.String" />
      </Parameters>
      <Docs>
        <param name="inputText">标识操作的输入。</param>
        <summary>使用文本代替同步语音识别的音频来模拟输入短语到语音识别器。</summary>
        <returns>输入的标识结果或 <see langword="null" /> ，如果操作不成功或识别器未启用。</returns>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 语音识别器会引发 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected> 、 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized> 、 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> 和 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> 事件，就好像未模拟识别操作一样。  
  
 向输入短语应用语法规则时，Vista 和 Windows 7 附带的识别器忽略大小写和字符宽度。 有关此类比较的详细信息，请参阅 <xref:System.Globalization.CompareOptions> 枚举值 <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> 和 <xref:System.Globalization.CompareOptions.IgnoreWidth> 。 识别器还会忽略新行和额外的空格，并将标点符号视为文本输入。  
  
   
  
## Examples  
 下面的代码示例是一个控制台应用程序的一部分，该应用程序演示了仿真输入、关联的识别结果和语音识别器引发的相关事件。 该示例生成以下输出。  
  
```  
TestRecognize("Smith")...  
 SpeechDetected event raised.  
 SpeechRecognized event raised.  
  Grammar = Smith; Text = Smith  
...Recognition result text = Smith  
  
TestRecognize("Jones")...  
 SpeechDetected event raised.  
 SpeechRecognized event raised.  
  Grammar = Jones; Text = Jones  
...Recognition result text = Jones  
  
TestRecognize("Mister")...  
 SpeechDetected event raised.  
 SpeechHypothesized event raised.  
  Grammar = Smith; Text = mister  
 SpeechRecognitionRejected event raised.  
  Grammar = <not available>; Text =  
...No recognition result.  
  
TestRecognize("Mister Smith")...  
 SpeechDetected event raised.  
 SpeechRecognized event raised.  
  Grammar = Smith; Text = mister Smith  
...Recognition result text = mister Smith  
  
press any key to exit...  
```  
  
```csharp  
  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
  
namespace Sre_EmulateRecognize  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Create an in-process speech recognizer for the en-US locale.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
  
        // Load grammars.  
        recognizer.LoadGrammar(CreateNameGrammar("Smith"));  
        recognizer.LoadGrammar(CreateNameGrammar("Jones"));  
  
        // Disable audio input to the recognizer.  
        recognizer.SetInputToNull();  
  
        // Add handlers for events raised by the EmulateRecognize method.  
        recognizer.SpeechDetected +=  
          new EventHandler<SpeechDetectedEventArgs>(  
            SpeechDetectedHandler);  
        recognizer.SpeechHypothesized +=  
          new EventHandler<SpeechHypothesizedEventArgs>(  
            SpeechHypothesizedHandler);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(  
            SpeechRecognitionRejectedHandler);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
  
        // Start four synchronous emulated recognition operations.  
        TestRecognize(recognizer, "Smith");  
        TestRecognize(recognizer, "Jones");  
        TestRecognize(recognizer, "Mister");  
        TestRecognize(recognizer, "Mister Smith");  
      }  
  
      Console.WriteLine("press any key to exit...");  
      Console.ReadKey(true);  
    }  
  
    // Create a simple name grammar.  
    // Set the grammar name to the surname.  
    private static Grammar CreateNameGrammar(string surname)  
    {  
      GrammarBuilder builder = new GrammarBuilder("mister", 0, 1);  
      builder.Append(surname);  
  
      Grammar nameGrammar = new Grammar(builder);  
      nameGrammar.Name = surname;  
  
      return nameGrammar;  
    }  
  
    // Send emulated input to the recognizer for synchronous recognition.  
    private static void TestRecognize(  
      SpeechRecognitionEngine recognizer, string input)  
    {  
      Console.WriteLine("TestRecognize(\"{0}\")...", input);  
      RecognitionResult result =  
        recognizer.EmulateRecognize(input,CompareOptions.IgnoreCase);  
      if (result != null)  
      {  
        Console.WriteLine("...Recognition result text = {0}",  
          result.Text ?? "<null>");  
      }  
      else  
      {  
        Console.WriteLine("...No recognition result.");  
      }  
      Console.WriteLine();  
    }  
  
    static void SpeechDetectedHandler(  
      object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine(" SpeechDetected event raised.");  
    }  
  
    // Handle events.  
    static void SpeechHypothesizedHandler(  
      object sender, SpeechHypothesizedEventArgs e)  
    {  
      Console.WriteLine(" SpeechHypothesized event raised.");  
      if (e.Result != null)  
      {  
        Console.WriteLine("  Grammar = {0}; Text = {1}",  
          e.Result.Grammar.Name ?? "<none>", e.Result.Text);  
      }  
      else  
      {  
        Console.WriteLine("  No recognition result available.");  
      }  
    }  
  
    static void SpeechRecognitionRejectedHandler(  
      object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine(" SpeechRecognitionRejected event raised.");  
      if (e.Result != null)  
      {  
        string grammarName;  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name ?? "<none>";  
        }  
        else  
        {  
          grammarName = "<not available>";  
        }  
        Console.WriteLine("  Grammar = {0}; Text = {1}",  
          grammarName, e.Result.Text);  
      }  
      else  
      {  
        Console.WriteLine("  No recognition result available.");  
      }  
    }  
  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine(" SpeechRecognized event raised.");  
      if (e.Result != null)  
      {  
        Console.WriteLine("  Grammar = {0}; Text = {1}",  
          e.Result.Grammar.Name ?? "<none>", e.Result.Text);  
      }  
      else  
      {  
        Console.WriteLine("  No recognition result available.");  
      }  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.InvalidOperationException">识别器没有加载的语音识别语法。</exception>
        <exception cref="T:System.ArgumentNullException"><paramref name="inputText" /> 为 <see langword="null" />。</exception>
        <exception cref="T:System.ArgumentException"><paramref name="inputText" /> 为空字符串 ("")。</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
      </Docs>
    </Member>
    <Member MemberName="EmulateRecognize">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.RecognitionResult EmulateRecognize (System.Speech.Recognition.RecognizedWordUnit[] wordUnits, System.Globalization.CompareOptions compareOptions);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance class System.Speech.Recognition.RecognitionResult EmulateRecognize(class System.Speech.Recognition.RecognizedWordUnit[] wordUnits, valuetype System.Globalization.CompareOptions compareOptions) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
      <MemberSignature Language="VB.NET" Value="Public Function EmulateRecognize (wordUnits As RecognizedWordUnit(), compareOptions As CompareOptions) As RecognitionResult" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; System::Speech::Recognition::RecognitionResult ^ EmulateRecognize(cli::array &lt;System::Speech::Recognition::RecognizedWordUnit ^&gt; ^ wordUnits, System::Globalization::CompareOptions compareOptions);" />
      <MemberSignature Language="F#" Value="member this.EmulateRecognize : System.Speech.Recognition.RecognizedWordUnit[] * System.Globalization.CompareOptions -&gt; System.Speech.Recognition.RecognitionResult" Usage="speechRecognitionEngine.EmulateRecognize (wordUnits, compareOptions)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.RecognitionResult</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="wordUnits" Type="System.Speech.Recognition.RecognizedWordUnit[]" />
        <Parameter Name="compareOptions" Type="System.Globalization.CompareOptions" />
      </Parameters>
      <Docs>
        <param name="wordUnits">词单元数组，包含识别操作输入。</param>
        <param name="compareOptions">描述用于模拟识别操作的比较说明类型的枚举值的按位组合。</param>
        <summary>使用文本代替同步语音识别的音频来模拟特定字输入到语音识别器，并指定识别器如何处理单词和加载的语音识别语法间的 Unicode 比较。</summary>
        <returns>输入的标识结果或 <see langword="null" /> ，如果操作不成功或识别器未启用。</returns>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 语音识别器会引发 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected> 、 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized> 、 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> 和 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> 事件，就好像未模拟识别操作一样。  
  
 识别器在将 `compareOptions` 语法规则应用于输入短语时使用。 如果 <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> 或值存在，则 Vista 和 Windows 7 附带的识别器会忽略大小写 <xref:System.Globalization.CompareOptions.IgnoreCase> 。 识别器始终忽略字符宽度并从不忽略假名类型。 识别器还会忽略新行和额外的空格，并将标点符号视为文本输入。 有关字符宽度和假名类型的详细信息，请参阅 <xref:System.Globalization.CompareOptions> 枚举。  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.InvalidOperationException">识别器没有加载的语音识别语法。</exception>
        <exception cref="T:System.ArgumentNullException"><paramref name="wordUnits" /> 为 <see langword="null" />。</exception>
        <exception cref="T:System.ArgumentException"><paramref name="wordUnits" /> 包含一个或多个 <see langword="null" /> 元素。</exception>
        <exception cref="T:System.NotSupportedException"><paramref name="compareOptions" /> 包含值 <see cref="F:System.Globalization.CompareOptions.IgnoreNonSpace" />， <see cref="F:System.Globalization.CompareOptions.IgnoreSymbols" />，或 <see cref="F:System.Globalization.CompareOptions.StringSort" /> 标志。</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
      </Docs>
    </Member>
    <Member MemberName="EmulateRecognize">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.RecognitionResult EmulateRecognize (string inputText, System.Globalization.CompareOptions compareOptions);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance class System.Speech.Recognition.RecognitionResult EmulateRecognize(string inputText, valuetype System.Globalization.CompareOptions compareOptions) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String,System.Globalization.CompareOptions)" />
      <MemberSignature Language="VB.NET" Value="Public Function EmulateRecognize (inputText As String, compareOptions As CompareOptions) As RecognitionResult" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; System::Speech::Recognition::RecognitionResult ^ EmulateRecognize(System::String ^ inputText, System::Globalization::CompareOptions compareOptions);" />
      <MemberSignature Language="F#" Value="member this.EmulateRecognize : string * System.Globalization.CompareOptions -&gt; System.Speech.Recognition.RecognitionResult" Usage="speechRecognitionEngine.EmulateRecognize (inputText, compareOptions)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.RecognitionResult</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="inputText" Type="System.String" />
        <Parameter Name="compareOptions" Type="System.Globalization.CompareOptions" />
      </Parameters>
      <Docs>
        <param name="inputText">标识操作的输入相。</param>
        <param name="compareOptions">描述用于模拟识别操作的比较说明类型的枚举值的按位组合。</param>
        <summary>使用文本代替异步语音识别的音频来模拟短语输入到语音识别器，并指定识别器如何处理短语和加载的语音识别语法间的 Unicode 比较。</summary>
        <returns>输入的标识结果或 <see langword="null" /> ，如果操作不成功或识别器未启用。</returns>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 语音识别器会引发 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected> 、 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized> 、 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> 和 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> 事件，就好像未模拟识别操作一样。  
  
 识别器在将 `compareOptions` 语法规则应用于输入短语时使用。 如果 <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> 或值存在，则 Vista 和 Windows 7 附带的识别器会忽略大小写 <xref:System.Globalization.CompareOptions.IgnoreCase> 。 识别器始终忽略字符宽度并从不忽略假名类型。 识别器还会忽略新行和额外的空格，并将标点符号视为文本输入。 有关字符宽度和假名类型的详细信息，请参阅 <xref:System.Globalization.CompareOptions> 枚举。  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.InvalidOperationException">识别器没有加载的语音识别语法。</exception>
        <exception cref="T:System.ArgumentNullException"><paramref name="inputText" /> 为 <see langword="null" />。</exception>
        <exception cref="T:System.ArgumentException"><paramref name="inputText" /> 为空字符串 ("")。</exception>
        <exception cref="T:System.NotSupportedException"><paramref name="compareOptions" /> 包含值 <see cref="F:System.Globalization.CompareOptions.IgnoreNonSpace" />， <see cref="F:System.Globalization.CompareOptions.IgnoreSymbols" />，或 <see cref="F:System.Globalization.CompareOptions.StringSort" /> 标志。</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
      </Docs>
    </Member>
    <MemberGroup MemberName="EmulateRecognizeAsync">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>使用文本代替异步语音识别的音频来模拟输入到语音识别器。</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 这些方法绕过系统音频输入，并以 <xref:System.String> 对象或对象数组的形式向识别器提供文本 <xref:System.Speech.Recognition.RecognizedWordUnit> 。 这在测试或调试应用程序或语法时非常有用。 例如，您可以使用模拟来确定单词是否在语法中，以及在识别字词时返回的语义。 使用 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A> 方法在仿真操作期间禁用语音识别引擎的音频输入。  
  
 语音识别器会引发 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected> 、 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized> 、 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> 和 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> 事件，就好像未模拟识别操作一样。 当识别器完成异步识别操作时，它会引发 <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> 事件。 识别器忽略新行和额外的空格，并将标点符号视为文本输入。  
  
> [!NOTE]
>  <xref:System.Speech.Recognition.RecognitionResult>语音识别器为响应仿真输入而生成的对象的 `null` 属性值为 <xref:System.Speech.Recognition.RecognitionResult.Audio%2A> 。  
  
 若要模拟同步识别，请使用 <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize%2A> 方法。  
  
 ]]></format>
        </remarks>
      </Docs>
    </MemberGroup>
    <Member MemberName="EmulateRecognizeAsync">
      <MemberSignature Language="C#" Value="public void EmulateRecognizeAsync (string inputText);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void EmulateRecognizeAsync(string inputText) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
      <MemberSignature Language="VB.NET" Value="Public Sub EmulateRecognizeAsync (inputText As String)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void EmulateRecognizeAsync(System::String ^ inputText);" />
      <MemberSignature Language="F#" Value="member this.EmulateRecognizeAsync : string -&gt; unit" Usage="speechRecognitionEngine.EmulateRecognizeAsync inputText" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Attributes>
        <Attribute FrameworkAlternate="netframework-4.0">
          <AttributeName Language="C#">[System.Runtime.TargetedPatchingOptOut("Performance critical to inline this type of method across NGen image boundaries")]</AttributeName>
          <AttributeName Language="F#">[&lt;System.Runtime.TargetedPatchingOptOut("Performance critical to inline this type of method across NGen image boundaries")&gt;]</AttributeName>
        </Attribute>
      </Attributes>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="inputText" Type="System.String" />
      </Parameters>
      <Docs>
        <param name="inputText">标识操作的输入。</param>
        <summary>使用文本代替异步语音识别的音频来模拟输入到语音识别器。</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 语音识别器会引发 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected> 、 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized> 、 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> 和 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> 事件，就好像未模拟识别操作一样。 当识别器完成异步识别操作时，它会引发 <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> 事件。  
  
 向输入短语应用语法规则时，Vista 和 Windows 7 附带的识别器忽略大小写和字符宽度。 有关此类比较的详细信息，请参阅 <xref:System.Globalization.CompareOptions> 枚举值 <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> 和 <xref:System.Globalization.CompareOptions.IgnoreWidth> 。 识别器还会忽略新行和额外的空格，并将标点符号视为文本输入。  
  
   
  
## Examples  
 下面的代码示例是控制台应用程序的一部分，该应用程序演示了异步仿真输入、关联的识别结果和语音识别器引发的相关事件。 该示例生成以下输出。  
  
```  
  
TestRecognizeAsync("Smith")...  
 SpeechDetected event raised.  
 SpeechRecognized event raised.  
  Grammar = Smith; Text = Smith  
 EmulateRecognizeCompleted event raised.  
  Grammar = Smith; Text = Smith  
 Done.  
  
TestRecognizeAsync("Jones")...  
 SpeechDetected event raised.  
 SpeechRecognized event raised.  
  Grammar = Jones; Text = Jones  
 EmulateRecognizeCompleted event raised.  
  Grammar = Jones; Text = Jones  
 Done.  
  
TestRecognizeAsync("Mister")...  
 SpeechDetected event raised.  
 SpeechHypothesized event raised.  
  Grammar = Smith; Text = mister  
 SpeechRecognitionRejected event raised.  
  Grammar = <not available>; Text =  
 EmulateRecognizeCompleted event raised.  
  No recognition result available.  
 Done.  
  
TestRecognizeAsync("Mister Smith")...  
 SpeechDetected event raised.  
 SpeechRecognized event raised.  
  Grammar = Smith; Text = mister Smith  
 EmulateRecognizeCompleted event raised.  
  Grammar = Smith; Text = mister Smith  
 Done.  
  
press any key to exit...  
```  
  
```csharp  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace SreEmulateRecognizeAsync  
{  
  class Program  
  {  
    // Indicate when an asynchronous operation is finished.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
        // Load grammars.  
        recognizer.LoadGrammar(CreateNameGrammar("Smith"));  
        recognizer.LoadGrammar(CreateNameGrammar("Jones"));  
  
        // Configure the audio input.  
        recognizer.SetInputToNull();  
  
        // Add event handlers for the events raised by the  
        // EmulateRecognizeAsync method.  
        recognizer.SpeechDetected +=  
          new EventHandler<SpeechDetectedEventArgs>(  
            SpeechDetectedHandler);  
        recognizer.SpeechHypothesized +=  
          new EventHandler<SpeechHypothesizedEventArgs>(  
            SpeechHypothesizedHandler);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(  
            SpeechRecognitionRejectedHandler);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
        recognizer.EmulateRecognizeCompleted +=  
          new EventHandler<EmulateRecognizeCompletedEventArgs>(  
            EmulateRecognizeCompletedHandler);  
  
        // Start four asynchronous emulated recognition operations.  
        TestRecognizeAsync(recognizer, "Smith");  
        TestRecognizeAsync(recognizer, "Jones");  
        TestRecognizeAsync(recognizer, "Mister");  
        TestRecognizeAsync(recognizer, "Mister Smith");  
      }  
  
      Console.WriteLine("press any key to exit...");  
      Console.ReadKey(true);  
    }  
  
    // Create a simple name grammar.  
    // Set the grammar name to the surname.  
    private static Grammar CreateNameGrammar(string surname)  
    {  
      GrammarBuilder builder = new GrammarBuilder("mister", 0, 1);  
      builder.Append(surname);  
  
      Grammar nameGrammar = new Grammar(builder);  
      nameGrammar.Name = surname;  
  
      return nameGrammar;  
    }  
  
    // Send emulated input to the recognizer for asynchronous  
    // recognition.  
    private static void TestRecognizeAsync(  
      SpeechRecognitionEngine recognizer, string input)  
    {  
      completed = false;  
  
      Console.WriteLine("TestRecognizeAsync(\"{0}\")...", input);  
      recognizer.EmulateRecognizeAsync(input);  
  
      // Wait for the operation to complete.  
      while (!completed)  
      {  
        Thread.Sleep(333);  
      }  
  
      Console.WriteLine(" Done.");  
      Console.WriteLine();  
    }  
  
    static void SpeechDetectedHandler(  
      object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine(" SpeechDetected event raised.");  
    }  
  
    static void SpeechHypothesizedHandler(  
      object sender, SpeechHypothesizedEventArgs e)  
    {  
      Console.WriteLine(" SpeechHypothesized event raised.");  
      if (e.Result != null)  
      {  
        Console.WriteLine("  Grammar = {0}; Text = {1}",  
          e.Result.Grammar.Name ?? "<none>", e.Result.Text);  
      }  
      else  
      {  
        Console.WriteLine("  No recognition result available.");  
      }  
    }  
  
    // Handle events.  
    static void SpeechRecognitionRejectedHandler(  
      object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine(" SpeechRecognitionRejected event raised.");  
      if (e.Result != null)  
      {  
        string grammarName;  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name ?? "<none>";  
        }  
        else  
        {  
          grammarName = "<not available>";  
        }  
        Console.WriteLine("  Grammar = {0}; Text = {1}",  
          grammarName, e.Result.Text);  
      }  
      else  
      {  
        Console.WriteLine("  No recognition result available.");  
      }  
    }  
  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine(" SpeechRecognized event raised.");  
      if (e.Result != null)  
      {  
        Console.WriteLine("  Grammar = {0}; Text = {1}",  
          e.Result.Grammar.Name ?? "<none>", e.Result.Text );  
      }  
      else  
      {  
        Console.WriteLine("  No recognition result available.");  
      }  
    }  
  
    static void EmulateRecognizeCompletedHandler(  
      object sender, EmulateRecognizeCompletedEventArgs e)  
    {  
      Console.WriteLine(" EmulateRecognizeCompleted event raised.");  
  
      if (e.Error != null)  
      {  
        Console.WriteLine("  {0} exception encountered: {1}:",  
          e.Error.GetType().Name, e.Error.Message);  
      }  
      else if (e.Cancelled)  
      {  
        Console.WriteLine("  Operation cancelled.");  
      }  
      else if (e.Result != null)  
      {  
        Console.WriteLine("  Grammar = {0}; Text = {1}",  
          e.Result.Grammar.Name ?? "<none>", e.Result.Text);  
      }  
      else  
      {  
        Console.WriteLine("  No recognition result available.");  
      }  
  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.InvalidOperationException">识别器没有加载的语音识别语法，或者识别器不是完全标识的异步操作。</exception>
        <exception cref="T:System.ArgumentNullException"><paramref name="inputText" /> 为 <see langword="null" />。</exception>
        <exception cref="T:System.ArgumentException"><paramref name="inputText" /> 为空字符串 ("")。</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="EmulateRecognizeAsync">
      <MemberSignature Language="C#" Value="public void EmulateRecognizeAsync (System.Speech.Recognition.RecognizedWordUnit[] wordUnits, System.Globalization.CompareOptions compareOptions);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void EmulateRecognizeAsync(class System.Speech.Recognition.RecognizedWordUnit[] wordUnits, valuetype System.Globalization.CompareOptions compareOptions) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
      <MemberSignature Language="VB.NET" Value="Public Sub EmulateRecognizeAsync (wordUnits As RecognizedWordUnit(), compareOptions As CompareOptions)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void EmulateRecognizeAsync(cli::array &lt;System::Speech::Recognition::RecognizedWordUnit ^&gt; ^ wordUnits, System::Globalization::CompareOptions compareOptions);" />
      <MemberSignature Language="F#" Value="member this.EmulateRecognizeAsync : System.Speech.Recognition.RecognizedWordUnit[] * System.Globalization.CompareOptions -&gt; unit" Usage="speechRecognitionEngine.EmulateRecognizeAsync (wordUnits, compareOptions)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="wordUnits" Type="System.Speech.Recognition.RecognizedWordUnit[]" />
        <Parameter Name="compareOptions" Type="System.Globalization.CompareOptions" />
      </Parameters>
      <Docs>
        <param name="wordUnits">词单元数组，包含识别操作输入。</param>
        <param name="compareOptions">描述用于模拟识别操作的比较说明类型的枚举值的按位组合。</param>
        <summary>使用 <see cref="T:System.Speech.Recognition.RecognizedWordUnit" /> 对象的数组代替异步语音识别的音频来模拟特定字输入到语音识别器，并指定识别器如何处理单词和加载的语音识别语法间的 Unicode 比较。</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 语音识别器会引发 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected> 、 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized> 、 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> 和 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> 事件，就好像未模拟识别操作一样。 当识别器完成异步识别操作时，它会引发 <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> 事件。  
  
 识别器在将 `compareOptions` 语法规则应用于输入短语时使用。 如果 <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> 或值存在，则 Vista 和 Windows 7 附带的识别器会忽略大小写 <xref:System.Globalization.CompareOptions.IgnoreCase> 。 识别器始终忽略字符宽度并从不忽略假名类型。 识别器还会忽略新行和额外的空格，并将标点符号视为文本输入。 有关字符宽度和假名类型的详细信息，请参阅 <xref:System.Globalization.CompareOptions> 枚举。  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.InvalidOperationException">识别器没有加载的语音识别语法，或者识别器不是完全标识的异步操作。</exception>
        <exception cref="T:System.ArgumentNullException"><paramref name="wordUnits" /> 为 <see langword="null" />。</exception>
        <exception cref="T:System.ArgumentException"><paramref name="wordUnits" /> 包含一个或多个 <see langword="null" /> 元素。</exception>
        <exception cref="T:System.NotSupportedException"><paramref name="compareOptions" /> 包含值 <see cref="F:System.Globalization.CompareOptions.IgnoreNonSpace" />， <see cref="F:System.Globalization.CompareOptions.IgnoreSymbols" />，或 <see cref="F:System.Globalization.CompareOptions.StringSort" /> 标志。</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="EmulateRecognizeAsync">
      <MemberSignature Language="C#" Value="public void EmulateRecognizeAsync (string inputText, System.Globalization.CompareOptions compareOptions);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void EmulateRecognizeAsync(string inputText, valuetype System.Globalization.CompareOptions compareOptions) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String,System.Globalization.CompareOptions)" />
      <MemberSignature Language="VB.NET" Value="Public Sub EmulateRecognizeAsync (inputText As String, compareOptions As CompareOptions)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void EmulateRecognizeAsync(System::String ^ inputText, System::Globalization::CompareOptions compareOptions);" />
      <MemberSignature Language="F#" Value="member this.EmulateRecognizeAsync : string * System.Globalization.CompareOptions -&gt; unit" Usage="speechRecognitionEngine.EmulateRecognizeAsync (inputText, compareOptions)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="inputText" Type="System.String" />
        <Parameter Name="compareOptions" Type="System.Globalization.CompareOptions" />
      </Parameters>
      <Docs>
        <param name="inputText">标识操作的输入相。</param>
        <param name="compareOptions">描述用于模拟识别操作的比较说明类型的枚举值的按位组合。</param>
        <summary>使用文本代替异步语音识别的音频来模拟短语输入到语音识别器，并指定识别器如何处理短语和加载的语音识别语法间的 Unicode 比较。</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 语音识别器会引发 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected> 、 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized> 、 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> 和 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> 事件，就好像未模拟识别操作一样。 当识别器完成异步识别操作时，它会引发 <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> 事件。  
  
 识别器在将 `compareOptions` 语法规则应用于输入短语时使用。 如果 <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> 或值存在，则 Vista 和 Windows 7 附带的识别器会忽略大小写 <xref:System.Globalization.CompareOptions.IgnoreCase> 。 识别器始终忽略字符宽度并从不忽略假名类型。 识别器还会忽略新行和额外的空格，并将标点符号视为文本输入。 有关字符宽度和假名类型的详细信息，请参阅 <xref:System.Globalization.CompareOptions> 枚举。  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.InvalidOperationException">识别器没有加载的语音识别语法，或者识别器不是完全标识的异步操作。</exception>
        <exception cref="T:System.ArgumentNullException"><paramref name="inputText" /> 为 <see langword="null" />。</exception>
        <exception cref="T:System.ArgumentException"><paramref name="inputText" /> 为空字符串 ("")。</exception>
        <exception cref="T:System.NotSupportedException"><paramref name="compareOptions" /> 包含值 <see cref="F:System.Globalization.CompareOptions.IgnoreNonSpace" />， <see cref="F:System.Globalization.CompareOptions.IgnoreSymbols" />，或 <see cref="F:System.Globalization.CompareOptions.StringSort" /> 标志。</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="EmulateRecognizeCompleted">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.EmulateRecognizeCompletedEventArgs&gt; EmulateRecognizeCompleted;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.EmulateRecognizeCompletedEventArgs&gt; EmulateRecognizeCompleted" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted" />
      <MemberSignature Language="VB.NET" Value="Public Custom Event EmulateRecognizeCompleted As EventHandler(Of EmulateRecognizeCompletedEventArgs) " FrameworkAlternate="netframework-3.0;netframework-3.5;netframework-4.0;netframework-4.5;netframework-4.5.1;netframework-4.5.2" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::EmulateRecognizeCompletedEventArgs ^&gt; ^ EmulateRecognizeCompleted;" />
      <MemberSignature Language="F#" Value="member this.EmulateRecognizeCompleted : EventHandler&lt;System.Speech.Recognition.EmulateRecognizeCompletedEventArgs&gt; " Usage="member this.EmulateRecognizeCompleted : System.EventHandler&lt;System.Speech.Recognition.EmulateRecognizeCompletedEventArgs&gt; " />
      <MemberSignature Language="VB.NET" Value="Public Event EmulateRecognizeCompleted As EventHandler(Of EmulateRecognizeCompletedEventArgs) " FrameworkAlternate="netframework-4.6;netframework-4.6.1;netframework-4.6.2;netframework-4.7;netframework-4.7.1;netframework-4.7.2;netframework-4.8" />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.EmulateRecognizeCompletedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>当 <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> 完成模拟输入的异步标识操作的时候引发。</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 每个 <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> 方法开始一个异步标识操作。 <xref:System.Speech.Recognition.SpeechRecognitionEngine> <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> 当完成异步操作时，将引发事件。  
  
 <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A>操作可以引发 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected> 、 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized> 、 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> 和 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> 事件。 <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted>事件是识别器针对给定操作引发的最后一个此类事件。  
  
 如果模拟识别成功，你可以使用以下任一方法来访问识别结果：  
  
-   <xref:System.Speech.Recognition.EmulateRecognizeCompletedEventArgs.Result%2A> <xref:System.Speech.Recognition.EmulateRecognizeCompletedEventArgs> 事件的处理程序中的对象的属性 <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> 。  
  
-   <xref:System.Speech.Recognition.RecognitionEventArgs.Result%2A><xref:System.Speech.Recognition.SpeechRecognizedEventArgs>事件的处理程序中的对象的属性 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> 。  
  
 如果模拟识别不成功，则 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> 不会引发事件，并且 <xref:System.Speech.Recognition.EmulateRecognizeCompletedEventArgs.Result%2A> 将为 null。  
  
 <xref:System.Speech.Recognition.EmulateRecognizeCompletedEventArgs> 派生自 <xref:System.ComponentModel.AsyncCompletedEventArgs>。  
  
 <xref:System.Speech.Recognition.SpeechRecognizedEventArgs> 派生自 <xref:System.Speech.Recognition.RecognitionEventArgs>。  
  
 创建 <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> 委托时，需要标识将要处理该事件的方法。 若要将事件与事件处理程序关联，请将该委托的一个实例添加到事件中。 除非移除了该委托，否则每当发生该事件时就会调用事件处理程序。 有关事件处理程序委托的详细信息，请参阅 [事件和委托](https://go.microsoft.com/fwlink/?LinkId=162418)。  
  
   
  
## Examples  
 下面的示例是一个控制台应用程序的一部分，该应用程序加载语音识别语法并演示异步仿真输入、关联的识别结果和语音识别器引发的相关事件。  
  
```  
using System;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace InProcessRecognizer  
{  
  class Program  
  {  
    // Indicate whether the asynchronous emulate recognition  
    // operation has completed.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
  
      // Initialize an instance of an in-process recognizer.  
      using (SpeechRecognitionEngine recognizer =   
        new SpeechRecognitionEngine(new System.Globalization.CultureInfo("en-US")))  
      {  
        // Create and load a sample grammar.  
        Grammar testGrammar =  
          new Grammar(new GrammarBuilder("testing testing"));  
        testGrammar.Name = "Test Grammar";  
        recognizer.LoadGrammar(testGrammar);  
  
        // Attach event handlers for recognition events.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(SpeechRecognizedHandler);  
        recognizer.EmulateRecognizeCompleted +=  
          new EventHandler<EmulateRecognizeCompletedEventArgs>(  
            EmulateRecognizeCompletedHandler);  
  
        completed = false;  
  
        // This EmulateRecognizeAsync call matches the grammar  
        // and generates a SpeechRecognized event.  
        recognizer.EmulateRecognizeAsync("testing testing");  
  
        // Wait for the asynchronous operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
  
        completed = false;  
  
        // This EmulateRecognizeAsync call does not match the grammar  
        // or generate a SpeechRecognized event.  
        recognizer.EmulateRecognizeAsync("testing one two three");  
  
        // Wait for the asynchronous operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechRecognized event.  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      if (e.Result != null)  
      {  
        Console.WriteLine("Result of 1st call to EmulateRecognizeAsync = {0}",  
          e.Result.Text ?? "<no text>");  
        Console.WriteLine();  
      }  
      else  
      {  
        Console.WriteLine("No recognition result");  
      }  
    }  
  
    // Handle the EmulateRecognizeCompleted event.  
    static void EmulateRecognizeCompletedHandler(  
      object sender, EmulateRecognizeCompletedEventArgs e)  
    {  
      if (e.Result == null)  
      {  
        Console.WriteLine("Result of 2nd call to EmulateRecognizeAsync = No result generated.");  
      }  
  
      // Indicate the asynchronous operation is complete.  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.EmulateRecognizeCompletedEventArgs" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="EndSilenceTimeout">
      <MemberSignature Language="C#" Value="public TimeSpan EndSilenceTimeout { get; set; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.TimeSpan EndSilenceTimeout" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
      <MemberSignature Language="VB.NET" Value="Public Property EndSilenceTimeout As TimeSpan" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property TimeSpan EndSilenceTimeout { TimeSpan get(); void set(TimeSpan value); };" />
      <MemberSignature Language="F#" Value="member this.EndSilenceTimeout : TimeSpan with get, set" Usage="System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Attributes>
        <Attribute>
          <AttributeName Language="C#">[System.ComponentModel.EditorBrowsable(System.ComponentModel.EditorBrowsableState.Advanced)]</AttributeName>
          <AttributeName Language="F#">[&lt;System.ComponentModel.EditorBrowsable(System.ComponentModel.EditorBrowsableState.Advanced)&gt;]</AttributeName>
        </Attribute>
      </Attributes>
      <ReturnValue>
        <ReturnType>System.TimeSpan</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>获取或设置 <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> 将在终结识别操作之前，于明确输入末尾接受的无声段间隔。</summary>
        <value>安静间隔的持续时间。</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 当识别输入明确时，语音识别器将使用此超时时间间隔。 例如，对于支持识别 "新游戏" 或 "新游戏" 的语音识别语法，"新游戏" 是一种明确的输入，"新游戏" 是一种不明确的输入。  
  
 此属性确定在完成识别操作之前，语音识别引擎将等待额外输入的时间。 超时间隔可以是0秒到10秒（含）。 默认值为150毫秒。  
  
 若要为不明确输入设置超时间隔，请使用 <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> 属性。  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentOutOfRangeException">将属性设置为小于 0 秒或大于 10 秒。</exception>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
      </Docs>
    </Member>
    <Member MemberName="EndSilenceTimeoutAmbiguous">
      <MemberSignature Language="C#" Value="public TimeSpan EndSilenceTimeoutAmbiguous { get; set; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.TimeSpan EndSilenceTimeoutAmbiguous" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
      <MemberSignature Language="VB.NET" Value="Public Property EndSilenceTimeoutAmbiguous As TimeSpan" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property TimeSpan EndSilenceTimeoutAmbiguous { TimeSpan get(); void set(TimeSpan value); };" />
      <MemberSignature Language="F#" Value="member this.EndSilenceTimeoutAmbiguous : TimeSpan with get, set" Usage="System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Attributes>
        <Attribute>
          <AttributeName Language="C#">[System.ComponentModel.EditorBrowsable(System.ComponentModel.EditorBrowsableState.Advanced)]</AttributeName>
          <AttributeName Language="F#">[&lt;System.ComponentModel.EditorBrowsable(System.ComponentModel.EditorBrowsableState.Advanced)&gt;]</AttributeName>
        </Attribute>
      </Attributes>
      <ReturnValue>
        <ReturnType>System.TimeSpan</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>获取或设置 <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> 将在终结识别操作之前，于二义性输入末尾接受的无声段间隔。</summary>
        <value>安静间隔的持续时间。</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 如果识别输入不明确，语音识别器将使用此超时间隔。 例如，对于支持识别 "新游戏" 或 "新游戏" 的语音识别语法，"新游戏" 是一种明确的输入，"新游戏" 是一种不明确的输入。  
  
 此属性确定在完成识别操作之前，语音识别引擎将等待额外输入的时间。 超时间隔可以是0秒到10秒（含）。 默认值为500毫秒。  
  
 若要设置明确输入的超时间隔，请使用 <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A> 属性。  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentOutOfRangeException">将属性设置为小于 0 秒或大于 10 秒。</exception>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
      </Docs>
    </Member>
    <Member MemberName="Grammars">
      <MemberSignature Language="C#" Value="public System.Collections.ObjectModel.ReadOnlyCollection&lt;System.Speech.Recognition.Grammar&gt; Grammars { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance class System.Collections.ObjectModel.ReadOnlyCollection`1&lt;class System.Speech.Recognition.Grammar&gt; Grammars" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.Grammars" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property Grammars As ReadOnlyCollection(Of Grammar)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property System::Collections::ObjectModel::ReadOnlyCollection&lt;System::Speech::Recognition::Grammar ^&gt; ^ Grammars { System::Collections::ObjectModel::ReadOnlyCollection&lt;System::Speech::Recognition::Grammar ^&gt; ^ get(); };" />
      <MemberSignature Language="F#" Value="member this.Grammars : System.Collections.ObjectModel.ReadOnlyCollection&lt;System.Speech.Recognition.Grammar&gt;" Usage="System.Speech.Recognition.SpeechRecognitionEngine.Grammars" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Collections.ObjectModel.ReadOnlyCollection&lt;System.Speech.Recognition.Grammar&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>获取在此 <see cref="T:System.Speech.Recognition.Grammar" /> 实例中加载的 <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> 对象的集合。</summary>
        <value><see cref="T:System.Speech.Recognition.Grammar" /> 对象的集合。</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Examples  
 下面的示例将语音识别器当前加载的每个语音识别语法的信息输出到控制台。  
  
> [!IMPORTANT]
>  如果在此方法枚举集合中的元素时修改了集合，则复制语法集合以避免出现错误。  
  
```csharp  
  
private static void ListGrammars(SpeechRecognitionEngine recognizer)  
{  
  string qualifier;  
  List<Grammar> grammars = new List<Grammar>(recognizer.Grammars);  
  foreach (Grammar g in grammars)  
  {  
    qualifier = (g.Enabled) ? "enabled" : "disabled";  
  
    Console.WriteLine("Grammar {0} is loaded and is {1}.",  
      g.Name, qualifier);  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.Grammar" />
      </Docs>
    </Member>
    <Member MemberName="InitialSilenceTimeout">
      <MemberSignature Language="C#" Value="public TimeSpan InitialSilenceTimeout { get; set; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.TimeSpan InitialSilenceTimeout" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout" />
      <MemberSignature Language="VB.NET" Value="Public Property InitialSilenceTimeout As TimeSpan" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property TimeSpan InitialSilenceTimeout { TimeSpan get(); void set(TimeSpan value); };" />
      <MemberSignature Language="F#" Value="member this.InitialSilenceTimeout : TimeSpan with get, set" Usage="System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Attributes>
        <Attribute>
          <AttributeName Language="C#">[System.ComponentModel.EditorBrowsable(System.ComponentModel.EditorBrowsableState.Advanced)]</AttributeName>
          <AttributeName Language="F#">[&lt;System.ComponentModel.EditorBrowsable(System.ComponentModel.EditorBrowsableState.Advanced)&gt;]</AttributeName>
        </Attribute>
      </Attributes>
      <ReturnValue>
        <ReturnType>System.TimeSpan</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>获取或设置时间间隔，在此时间间隔内，<see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> 将在终结识别之前接受仅包含无声段的输入。</summary>
        <value>安静间隔的持续时间。</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 每个语音识别器都有一种算法来区分无声和语音。 如果在初始无声超时期限内识别器输入不能完成，识别器将终结识别操作。  
  
-   对于异步识别操作和仿真，识别器引发 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> 事件，其中 <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.InitialSilenceTimeout%2A?displayProperty=nameWithType> 属性为 `true` ， <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.Result%2A?displayProperty=nameWithType> 属性为 `null` 。  
  
-   对于同步识别操作和仿真，识别器将返回 `null` ，而不是有效的 <xref:System.Speech.Recognition.RecognitionResult> 。  
  
 如果初始无声超时间隔设置为0，则识别器不会执行初始无声超时检查。 超时间隔可以是任何非负值。 默认值为0秒。  
  
   
  
## Examples  
 下面的示例演示了演示基本语音识别的控制台应用程序的一部分。 该示例在 <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A> <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> <xref:System.Speech.Recognition.SpeechRecognitionEngine> 启动语音识别之前设置的和属性。 语音识别器的和事件的处理程序将 <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioStateChanged> <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> 事件信息输出到控制台，以演示 <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> 属性属性如何 <xref:System.Speech.Recognition.SpeechRecognitionEngine> 影响识别操作。  
  
```csharp  
  
using System;  
using System.Speech.Recognition;  
  
namespace SpeechRecognitionApp  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Initialize an in-process speech recognizer.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(  
          new System.Globalization.CultureInfo("en-US")))  
      {  
        // Load a Grammar object.  
        recognizer.LoadGrammar(CreateServicesGrammar("FindServices"));  
  
        // Add event handlers.  
        recognizer.AudioStateChanged +=  
          new EventHandler<AudioStateChangedEventArgs>(  
            AudioStateChangedHandler);  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(  
            RecognizeCompletedHandler);  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        recognizer.InitialSilenceTimeout = TimeSpan.FromSeconds(3);  
        recognizer.BabbleTimeout = TimeSpan.FromSeconds(2);  
        recognizer.EndSilenceTimeout = TimeSpan.FromSeconds(1);  
        recognizer.EndSilenceTimeoutAmbiguous = TimeSpan.FromSeconds(1.5);  
  
        Console.WriteLine("BabbleTimeout: {0}", recognizer.BabbleTimeout);  
        Console.WriteLine("InitialSilenceTimeout: {0}", recognizer.InitialSilenceTimeout);  
        Console.WriteLine("EndSilenceTimeout: {0}", recognizer.EndSilenceTimeout);  
        Console.WriteLine("EndSilenceTimeoutAmbiguous: {0}", recognizer.EndSilenceTimeoutAmbiguous);  
        Console.WriteLine();  
  
        // Start asynchronous speech recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Single);  
  
        // Keep the console window open.  
        while (true)  
        {  
          Console.ReadLine();  
        }  
      }  
    }  
  
    // Create a grammar and build it into a Grammar object.   
    static Grammar CreateServicesGrammar(string grammarName)  
    {  
  
      // Create a grammar for finding services in different cities.  
      Choices services = new Choices(new string[] { "restaurants", "hotels", "gas stations" });  
      Choices cities = new Choices(new string[] { "Seattle", "Boston", "Dallas" });  
  
      GrammarBuilder findServices = new GrammarBuilder("Find");  
      findServices.Append(services);  
      findServices.Append("near");  
      findServices.Append(cities);  
  
      // Create a Grammar object from the GrammarBuilder. 
      Grammar servicesGrammar = new Grammar(findServices);  
      servicesGrammar.Name = ("FindServices");  
      return servicesGrammar;  
    }  
  
    // Handle the AudioStateChanged event.  
    static void AudioStateChangedHandler(  
      object sender, AudioStateChangedEventArgs e)  
    {  
      Console.WriteLine("AudioStateChanged ({0}): {1}",  
        DateTime.Now.ToString("mm:ss.f"), e.AudioState);  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void RecognizeCompletedHandler(  
      object sender, RecognizeCompletedEventArgs e)  
    {  
      Console.WriteLine("RecognizeCompleted ({0}):",  
        DateTime.Now.ToString("mm:ss.f"));  
  
      string resultText;  
      if (e.Result != null) { resultText = e.Result.Text; }  
      else { resultText = "<null>"; }  
  
      Console.WriteLine(  
        " BabbleTimeout: {0}; InitialSilenceTimeout: {1}; Result text: {2}",  
        e.BabbleTimeout, e.InitialSilenceTimeout, resultText);  
      if (e.Error != null)  
      {  
        Console.WriteLine(" Exception message: ", e.Error.Message);  
      }  
  
      // Start the next asynchronous recognition operation.  
      ((SpeechRecognitionEngine)sender).RecognizeAsync(RecognizeMode.Single);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentOutOfRangeException">该属性已设置为小于 0 秒。</exception>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
      </Docs>
    </Member>
    <Member MemberName="InstalledRecognizers">
      <MemberSignature Language="C#" Value="public static System.Collections.ObjectModel.ReadOnlyCollection&lt;System.Speech.Recognition.RecognizerInfo&gt; InstalledRecognizers ();" />
      <MemberSignature Language="ILAsm" Value=".method public static hidebysig class System.Collections.ObjectModel.ReadOnlyCollection`1&lt;class System.Speech.Recognition.RecognizerInfo&gt; InstalledRecognizers() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers" />
      <MemberSignature Language="VB.NET" Value="Public Shared Function InstalledRecognizers () As ReadOnlyCollection(Of RecognizerInfo)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; static System::Collections::ObjectModel::ReadOnlyCollection&lt;System::Speech::Recognition::RecognizerInfo ^&gt; ^ InstalledRecognizers();" />
      <MemberSignature Language="F#" Value="static member InstalledRecognizers : unit -&gt; System.Collections.ObjectModel.ReadOnlyCollection&lt;System.Speech.Recognition.RecognizerInfo&gt;" Usage="System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers " />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Collections.ObjectModel.ReadOnlyCollection&lt;System.Speech.Recognition.RecognizerInfo&gt;</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>返回有关当前系统中安装的所有语音识别器的信息。</summary>
        <returns>描述安装的识别器的 <see cref="T:System.Speech.Recognition.RecognizerInfo" /> 对象的只读集合。</returns>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 若要获取有关当前识别器的信息，请使用 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerInfo%2A> 属性。  
  
   
  
## Examples  
 下面的示例演示了演示基本语音识别的控制台应用程序的一部分。 该示例使用方法返回的集合 <xref:System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers%2A> 来查找支持英语的语音识别器。  
  
```csharp  
using System;  
using System.Speech.Recognition;  
  
namespace SpeechRecognitionApp  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Select a speech recognizer that supports English.  
      RecognizerInfo info = null;  
      foreach (RecognizerInfo ri in SpeechRecognitionEngine.InstalledRecognizers())  
      {  
        if (ri.Culture.TwoLetterISOLanguageName.Equals("en"))  
        {  
          info = ri;  
          break;  
        }  
      }  
      if (info == null) return;  
  
      // Create the selected recognizer.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(info))  
      {  
  
        // Create and load a dictation grammar.  
        recognizer.LoadGrammar(new DictationGrammar());  
  
        // Add a handler for the speech recognized event.  
        recognizer.SpeechRecognized +=   
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start asynchronous, continuous speech recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Keep the console window open.  
        while (true)  
        {  
          Console.ReadLine();  
        }  
      }  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Recognized text: " + e.Result.Text);  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.#ctor(System.Speech.Recognition.RecognizerInfo)" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerInfo" />
      </Docs>
    </Member>
    <Member MemberName="LoadGrammar">
      <MemberSignature Language="C#" Value="public void LoadGrammar (System.Speech.Recognition.Grammar grammar);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void LoadGrammar(class System.Speech.Recognition.Grammar grammar) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar(System.Speech.Recognition.Grammar)" />
      <MemberSignature Language="VB.NET" Value="Public Sub LoadGrammar (grammar As Grammar)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void LoadGrammar(System::Speech::Recognition::Grammar ^ grammar);" />
      <MemberSignature Language="F#" Value="member this.LoadGrammar : System.Speech.Recognition.Grammar -&gt; unit" Usage="speechRecognitionEngine.LoadGrammar grammar" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="grammar" Type="System.Speech.Recognition.Grammar" />
      </Parameters>
      <Docs>
        <param name="grammar">要加载的语法对象。</param>
        <summary>同步加载 <see cref="T:System.Speech.Recognition.Grammar" /> 对象。</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 如果 <xref:System.Speech.Recognition.Grammar> 对象已加载、正在异步加载或者未能加载到任何识别器中，则识别器会引发异常。 不能将同一个 <xref:System.Speech.Recognition.Grammar> 对象加载到的多个实例中 <xref:System.Speech.Recognition.SpeechRecognitionEngine> 。 应 <xref:System.Speech.Recognition.Grammar> 为每个实例创建一个新的对象 <xref:System.Speech.Recognition.SpeechRecognitionEngine> 。  
  
 如果识别器正在运行，则应用程序必须使用在 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> 加载、卸载、启用或禁用语法之前暂停语音识别引擎。  
  
 加载语法时，它默认处于启用状态。 若要禁用已加载的语法，请使用 <xref:System.Speech.Recognition.Grammar.Enabled%2A> 属性。  
  
 若要以 <xref:System.Speech.Recognition.Grammar> 异步方式加载对象，请使用 <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> 方法。  
  
   
  
## Examples  
 下面的示例演示了演示基本语音识别的控制台应用程序的一部分。 该示例创建一个 <xref:System.Speech.Recognition.DictationGrammar> 并将其加载到语音识别器。  
  
```csharp  
using System;  
using System.Speech.Recognition;  
  
namespace SpeechRecognitionApp  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Create an in-process speech recognizer for the en-US locale.  
      using (  
      SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(  
          new System.Globalization.CultureInfo("en-US")))  
      {  
  
        // Create and load a dictation grammar.  
        recognizer.LoadGrammar(new DictationGrammar());  
  
        // Add a handler for the speech recognized event.  
        recognizer.SpeechRecognized +=   
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start asynchronous, continuous speech recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Keep the console window open.  
        while (true)  
        {  
          Console.ReadLine();  
        }  
      }  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Recognized text: " + e.Result.Text);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentNullException"><paramref name="Grammar" /> 为 <see langword="null" />。</exception>
        <exception cref="T:System.InvalidOperationException"><paramref name="Grammar" /> 不在有效状态中。</exception>
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync(System.Speech.Recognition.Grammar)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.UnloadAllGrammars" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.UnloadGrammar(System.Speech.Recognition.Grammar)" />
      </Docs>
    </Member>
    <Member MemberName="LoadGrammarAsync">
      <MemberSignature Language="C#" Value="public void LoadGrammarAsync (System.Speech.Recognition.Grammar grammar);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void LoadGrammarAsync(class System.Speech.Recognition.Grammar grammar) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync(System.Speech.Recognition.Grammar)" />
      <MemberSignature Language="VB.NET" Value="Public Sub LoadGrammarAsync (grammar As Grammar)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void LoadGrammarAsync(System::Speech::Recognition::Grammar ^ grammar);" />
      <MemberSignature Language="F#" Value="member this.LoadGrammarAsync : System.Speech.Recognition.Grammar -&gt; unit" Usage="speechRecognitionEngine.LoadGrammarAsync grammar" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="grammar" Type="System.Speech.Recognition.Grammar" />
      </Parameters>
      <Docs>
        <param name="grammar">加载语音识别语法。</param>
        <summary>异步加载语音识别语法。</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 当识别器完成加载 <xref:System.Speech.Recognition.Grammar> 对象时，会引发一个 <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarCompleted> 事件。 如果 <xref:System.Speech.Recognition.Grammar> 对象已加载、正在异步加载或者未能加载到任何识别器中，则识别器会引发异常。 不能将同一个 <xref:System.Speech.Recognition.Grammar> 对象加载到的多个实例中 <xref:System.Speech.Recognition.SpeechRecognitionEngine> 。 应 <xref:System.Speech.Recognition.Grammar> 为每个实例创建一个新的对象 <xref:System.Speech.Recognition.SpeechRecognitionEngine> 。  
  
 如果识别器正在运行，则应用程序必须使用在 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> 加载、卸载、启用或禁用语法之前暂停语音识别引擎。  
  
 加载语法时，它默认处于启用状态。 若要禁用已加载的语法，请使用 <xref:System.Speech.Recognition.Grammar.Enabled%2A> 属性。  
  
 若要同步加载语音识别语法，请使用 <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A> 方法。  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentNullException"><paramref name="Grammar" /> 为 <see langword="null" />。</exception>
        <exception cref="T:System.InvalidOperationException"><paramref name="Grammar" /> 不在有效状态中。</exception>
        <exception cref="T:System.OperationCanceledException">异步操作已取消。</exception>
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarCompleted" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar(System.Speech.Recognition.Grammar)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.UnloadAllGrammars" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.UnloadGrammar(System.Speech.Recognition.Grammar)" />
      </Docs>
    </Member>
    <Member MemberName="LoadGrammarCompleted">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.LoadGrammarCompletedEventArgs&gt; LoadGrammarCompleted;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.LoadGrammarCompletedEventArgs&gt; LoadGrammarCompleted" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarCompleted" />
      <MemberSignature Language="VB.NET" Value="Public Custom Event LoadGrammarCompleted As EventHandler(Of LoadGrammarCompletedEventArgs) " FrameworkAlternate="netframework-3.0;netframework-3.5;netframework-4.0;netframework-4.5;netframework-4.5.1;netframework-4.5.2" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::LoadGrammarCompletedEventArgs ^&gt; ^ LoadGrammarCompleted;" />
      <MemberSignature Language="F#" Value="member this.LoadGrammarCompleted : EventHandler&lt;System.Speech.Recognition.LoadGrammarCompletedEventArgs&gt; " Usage="member this.LoadGrammarCompleted : System.EventHandler&lt;System.Speech.Recognition.LoadGrammarCompletedEventArgs&gt; " />
      <MemberSignature Language="VB.NET" Value="Public Event LoadGrammarCompleted As EventHandler(Of LoadGrammarCompletedEventArgs) " FrameworkAlternate="netframework-4.6;netframework-4.6.1;netframework-4.6.2;netframework-4.7;netframework-4.7.1;netframework-4.7.2;netframework-4.8" />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.LoadGrammarCompletedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>当 <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> 完成 <see cref="T:System.Speech.Recognition.Grammar" /> 对象的异步加载时候引发。</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 识别器的 <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> 方法启动异步操作。 <xref:System.Speech.Recognition.SpeechRecognitionEngine>完成操作后，将引发此事件。 若要获取 <xref:System.Speech.Recognition.Grammar> 识别器加载的对象，请使用 <xref:System.Speech.Recognition.LoadGrammarCompletedEventArgs.Grammar%2A> 关联的的属性 <xref:System.Speech.Recognition.LoadGrammarCompletedEventArgs> 。 若要获取 <xref:System.Speech.Recognition.Grammar> 识别器已加载的当前对象，请使用识别器的 <xref:System.Speech.Recognition.SpeechRecognitionEngine.Grammars%2A> 属性。  
  
 如果识别器正在运行，则应用程序必须使用在 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> 加载、卸载、启用或禁用语法之前暂停语音识别引擎。  
  
 创建 <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarCompleted> 委托时，需要标识将处理该事件的方法。 若要将事件与事件处理程序关联，请将该委托的一个实例添加到事件中。 除非移除了该委托，否则每当发生该事件时就会调用事件处理程序。 有关事件处理程序委托的详细信息，请参阅 [事件和委托](https://go.microsoft.com/fwlink/?LinkId=162418)。  
  
   
  
## Examples  
 下面的示例创建一个进程内语音识别器，然后创建两种类型的语法来识别特定单词并接受免费听写。 该示例 <xref:System.Speech.Recognition.Grammar> 从每个已完成的语音识别语法构造对象，然后将对象异步加载 <xref:System.Speech.Recognition.Grammar> 到该 <xref:System.Speech.Recognition.SpeechRecognitionEngine> 实例。 识别器的和事件的处理程序 <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarCompleted> <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> 向控制台写入 <xref:System.Speech.Recognition.Grammar> 用于分别执行识别和识别结果文本的对象的名称。  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    private static SpeechRecognitionEngine recognizer;  
    public static void Main(string[] args)  
    {  
  
      // Initialize an in-process speech recognition engine and set its input.  
      recognizer = new SpeechRecognitionEngine();  
      recognizer.SetInputToDefaultAudioDevice();  
  
      // Add a handler for the LoadGrammarCompleted event.  
      recognizer.LoadGrammarCompleted +=  
        new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
  
      // Add a handler for the SpeechRecognized event.  
      recognizer.SpeechRecognized +=  
        new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
      // Create the "yesno" grammar.  
      Choices yesChoices = new Choices(new string[] { "yes", "yup", "yeah" });  
      SemanticResultValue yesValue =  
          new SemanticResultValue(yesChoices, (bool)true);  
      Choices noChoices = new Choices(new string[] { "no", "nope", "neah" });  
      SemanticResultValue noValue =  
          new SemanticResultValue(noChoices, (bool)false);  
      SemanticResultKey yesNoKey =  
          new SemanticResultKey("yesno", new Choices(new GrammarBuilder[] { yesValue, noValue }));  
      Grammar yesnoGrammar = new Grammar(yesNoKey);  
      yesnoGrammar.Name = "yesNo";  
  
      // Create the "done" grammar.  
      Grammar doneGrammar =  
        new Grammar(new Choices(new string[] { "done", "exit", "quit", "stop" }));  
      doneGrammar.Name = "Done";  
  
      // Create a dictation grammar.  
      Grammar dictation = new DictationGrammar();  
      dictation.Name = "Dictation";  
  
      // Load grammars to the recognizer.  
      recognizer.LoadGrammarAsync(yesnoGrammar);  
      recognizer.LoadGrammarAsync(doneGrammar);  
      recognizer.LoadGrammarAsync(dictation);  
  
      // Start asynchronous, continuous recognition.  
      recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
      // Keep the console window open.  
      Console.ReadLine();  
    }  
  
    // Handle the LoadGrammarCompleted event.   
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      string grammarName = e.Grammar.Name;  
      bool grammarLoaded = e.Grammar.Loaded;  
  
      if (e.Error != null)  
      {  
        Console.WriteLine("LoadGrammar for {0} failed with a {1}.",  
        grammarName, e.Error.GetType().Name);  
  
        // Add exception handling code here.  
      }  
  
      Console.WriteLine("Grammar {0} {1} loaded.",  
      grammarName, (grammarLoaded) ? "is" : "is not");  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Grammar({0}): {1}", e.Result.Grammar.Name, e.Result.Text);  
  
      // Add event handler code here.  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached" />
        <altmember cref="T:System.Speech.Recognition.Grammar" />
        <altmember cref="T:System.Speech.Recognition.LoadGrammarCompletedEventArgs" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync(System.Speech.Recognition.Grammar)" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.Grammars" />
      </Docs>
    </Member>
    <Member MemberName="MaxAlternates">
      <MemberSignature Language="C#" Value="public int MaxAlternates { get; set; }" />
      <MemberSignature Language="ILAsm" Value=".property instance int32 MaxAlternates" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.MaxAlternates" />
      <MemberSignature Language="VB.NET" Value="Public Property MaxAlternates As Integer" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property int MaxAlternates { int get(); void set(int value); };" />
      <MemberSignature Language="F#" Value="member this.MaxAlternates : int with get, set" Usage="System.Speech.Recognition.SpeechRecognitionEngine.MaxAlternates" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Int32</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>获取或设置 <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> 为每个识别操作返回的备用识别结果的最大数量。</summary>
        <value>要返回的替换结果的数。</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 <xref:System.Speech.Recognition.RecognitionResult.Alternates%2A>类的属性 <xref:System.Speech.Recognition.RecognitionResult> 包含 <xref:System.Speech.Recognition.RecognizedPhrase> 对象的集合，这些对象表示输入的可能的解释。  
  
 的默认值 <xref:System.Speech.Recognition.SpeechRecognitionEngine.MaxAlternates%2A> 为10。  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentOutOfRangeException"><see cref="P:System.Speech.Recognition.SpeechRecognitionEngine.MaxAlternates" /> 设置为小于 0 的值。</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
      </Docs>
    </Member>
    <Member MemberName="QueryRecognizerSetting">
      <MemberSignature Language="C#" Value="public object QueryRecognizerSetting (string settingName);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance object QueryRecognizerSetting(string settingName) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.QueryRecognizerSetting(System.String)" />
      <MemberSignature Language="VB.NET" Value="Public Function QueryRecognizerSetting (settingName As String) As Object" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; System::Object ^ QueryRecognizerSetting(System::String ^ settingName);" />
      <MemberSignature Language="F#" Value="member this.QueryRecognizerSetting : string -&gt; obj" Usage="speechRecognitionEngine.QueryRecognizerSetting settingName" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Object</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="settingName" Type="System.String" />
      </Parameters>
      <Docs>
        <param name="settingName">要返回的设置的名称。</param>
        <summary>为识别器返回设置的值。</summary>
        <returns>设置的值。</returns>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 识别器设置可以包含字符串、64位整数或内存地址数据。 下表描述了为 Microsoft Speech API (SAPI) 兼容的识别器定义的设置。 对于支持设置的每个识别器，以下设置必须具有相同的范围。 支持这些设置并可支持其他设置时，不需要使用 SAPI 兼容的识别器。  
  
|名称|说明|  
|----------|-----------------|  
|`ResourceUsage`|指定识别器的 CPU 消耗。 范围为0到100。 默认值为 50。|  
|`ResponseSpeed`|指示语音识别器完成识别操作之前明确输入结束时的静默时间长度。 范围为0到10000毫秒 (ms) 。 此设置对应于识别器的 <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A> 属性。  默认值为150ms。|  
|`ComplexResponseSpeed`|指示语音识别器完成识别操作之前，不明确输入结束时的静默时间长度。 范围为0到10、000ms。 此设置对应于识别器的 <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> 属性。 默认值为500毫秒。|  
|`AdaptationOn`|指示是否在 (值 = `1`) 或关闭 (值 =) 时声音模型的适应 `0` 。 )  (默认值 `1` 。|  
|`PersistedBackgroundAdaptation`|指示是否 (值 = `1`) 或关闭 (值 = `0`) ，并保留注册表中的设置。 )  (默认值 `1` 。|  
  
 若要更新识别器的设置，请使用其中一种 <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> 方法。  
  
   
  
## Examples  
 下面的示例是一个控制台应用程序的一部分，该控制台应用程序为支持 en-us 区域设置的识别器定义的多个设置的值。 该示例生成以下输出。  
  
```  
Settings for recognizer MS-1033-80-DESK:  
  
  ResourceUsage                  is not supported by this recognizer.  
  ResponseSpeed                  = 150  
  ComplexResponseSpeed           = 500  
  AdaptationOn                   = 1  
  PersistedBackgroundAdaptation  = 1  
  
Press any key to exit...  
```  
  
```csharp  
  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
  
namespace RecognizerSettings  
{  
  class Program  
  {  
    static readonly string[] settings = new string[] {  
      "ResourceUsage",  
      "ResponseSpeed",  
      "ComplexResponseSpeed",  
      "AdaptationOn",  
      "PersistedBackgroundAdaptation"  
    };  
  
    static void Main(string[] args)  
    {  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new System.Globalization.CultureInfo("en-US")))  
      {  
        Console.WriteLine("Settings for recognizer {0}:",  
          recognizer.RecognizerInfo.Name);  
        Console.WriteLine();  
  
        foreach (string setting in settings)  
        {  
          try  
          {  
            object value = recognizer.QueryRecognizerSetting(setting);  
            Console.WriteLine("  {0,-30} = {1}", setting, value);  
          }  
          catch  
          {  
            Console.WriteLine("  {0,-30} is not supported by this recognizer.",  
              setting);  
          }  
        }  
      }  
      Console.WriteLine();  
  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentNullException"><paramref name="settingName" /> 为 <see langword="null" />。</exception>
        <exception cref="T:System.ArgumentException"><paramref name="settingName" /> 为空字符串 ("")。</exception>
        <exception cref="T:System.Collections.Generic.KeyNotFoundException">不用该名称设置的识别器。</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting(System.String,System.Int32)" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
      </Docs>
    </Member>
    <MemberGroup MemberName="Recognize">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>启动同步语音识别操作。</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 这些方法执行单个同步标识操作。 识别器对其已加载和已启用的语音识别语法执行此操作。  
  
 在调用此方法时，识别器可以引发以下事件：  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>.  当识别器检测到它可以标识为语音的输入时引发。  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>.  当输入使用某个活动语法创建不明确的匹配项时引发。  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> 或 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>。 当识别器终结识别操作时引发。  
  
 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted>当使用其中一种方法时，识别器不会引发事件 <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A> 。  
  
 <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A> <xref:System.Speech.Recognition.RecognitionResult> `null` 如果操作不成功或识别器未启用，则方法将返回对象。  
  
 同步识别操作可能会由于以下原因而失败：  
  
-   在 <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A> 或属性或方法的参数的超时间隔到期之前，不会检测到语音 <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> `initialSilenceTimeout` <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A> 。  
  
-   识别引擎检测语音，但在任何已加载和启用的对象中找不到匹配项 <xref:System.Speech.Recognition.Grammar> 。  
  
 若要修改识别器如何处理语音或静默处理识别的时间，请使用、、 <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A> <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A> 和 <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> 属性。  
  
 在 <xref:System.Speech.Recognition.SpeechRecognitionEngine> <xref:System.Speech.Recognition.Grammar> 执行识别之前，必须至少加载一个对象。 若要加载语音识别语法，请使用 <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A> 或 <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> 方法。  
  
 若要执行异步识别，请使用其中一种 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> 方法。  
  
 ]]></format>
        </remarks>
      </Docs>
    </MemberGroup>
    <Member MemberName="Recognize">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.RecognitionResult Recognize ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance class System.Speech.Recognition.RecognitionResult Recognize() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
      <MemberSignature Language="VB.NET" Value="Public Function Recognize () As RecognitionResult" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; System::Speech::Recognition::RecognitionResult ^ Recognize();" />
      <MemberSignature Language="F#" Value="member this.Recognize : unit -&gt; System.Speech.Recognition.RecognitionResult" Usage="speechRecognitionEngine.Recognize " />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.RecognitionResult</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>执行同步语音识别操作。</summary>
        <returns>输入的标识结果或 <see langword="null" /> ，如果操作不成功或识别器未启用。</returns>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 此方法执行单个识别操作。 识别器对其已加载和已启用的语音识别语法执行此操作。  
  
 在调用此方法时，识别器可以引发以下事件：  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>.  当识别器检测到它可以标识为语音的输入时引发。  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>.  当输入使用某个活动语法创建不明确的匹配项时引发。  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> 或 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>。 当识别器终结识别操作时引发。  
  
 使用此方法时，识别器不会引发 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> 事件。  
  
 <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize> <xref:System.Speech.Recognition.RecognitionResult> 如果操作不成功，则此方法返回一个对象 `null` 。  
  
 同步识别操作可能会由于以下原因而失败：  
  
-   在或属性的超时间隔到期之前，不会检测到语音 <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A> <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> 。  
  
-   识别引擎检测语音，但在任何已加载和启用的对象中找不到匹配项 <xref:System.Speech.Recognition.Grammar> 。  
  
 若要执行异步识别，请使用其中一种 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> 方法。  
  
   
  
## Examples  
 下面的示例演示了演示基本语音识别的控制台应用程序的一部分。 该示例创建一个 <xref:System.Speech.Recognition.DictationGrammar> ，将其加载到进程内语音识别器，并执行一个识别操作。  
  
```  
  
using System;  
using System.Speech.Recognition;  
  
namespace SynchronousRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
      // Create an in-process speech recognizer for the en-US locale.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(  
          new System.Globalization.CultureInfo("en-US")))  
      {  
  
        // Create and load a dictation grammar.  
        recognizer.LoadGrammar(new DictationGrammar());  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Modify the initial silence time-out value.  
        recognizer.InitialSilenceTimeout = TimeSpan.FromSeconds(5);  
  
        // Start synchronous speech recognition.  
        RecognitionResult result = recognizer.Recognize();  
  
        if (result != null)  
        {  
          Console.WriteLine("Recognized text = {0}", result.Text);  
        }  
        else  
        {  
          Console.WriteLine("No recognition result available.");  
        }  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to continue...");  
      Console.ReadKey();  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String)" />
      </Docs>
    </Member>
    <Member MemberName="Recognize">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.RecognitionResult Recognize (TimeSpan initialSilenceTimeout);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance class System.Speech.Recognition.RecognitionResult Recognize(valuetype System.TimeSpan initialSilenceTimeout) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize(System.TimeSpan)" />
      <MemberSignature Language="VB.NET" Value="Public Function Recognize (initialSilenceTimeout As TimeSpan) As RecognitionResult" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; System::Speech::Recognition::RecognitionResult ^ Recognize(TimeSpan initialSilenceTimeout);" />
      <MemberSignature Language="F#" Value="member this.Recognize : TimeSpan -&gt; System.Speech.Recognition.RecognitionResult" Usage="speechRecognitionEngine.Recognize initialSilenceTimeout" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.RecognitionResult</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="initialSilenceTimeout" Type="System.TimeSpan" />
      </Parameters>
      <Docs>
        <param name="initialSilenceTimeout">时间间隔语音识别器接受仅包含在完成标识之前的无声输入。</param>
        <summary>使用指定初始静音超时执行同步语音识别操作。</summary>
        <returns>输入的标识结果或 <see langword="null" /> ，如果操作不成功或识别器未启用。</returns>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 如果语音识别引擎在参数所指定的时间间隔内检测到语音 `initialSilenceTimeout` ， <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%28System.TimeSpan%29> 则执行单个识别操作，然后终止。  `initialSilenceTimeout`参数取代了识别器的 <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> 属性。  
  
 在调用此方法时，识别器可以引发以下事件：  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>.  当识别器检测到它可以标识为语音的输入时引发。  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>.  当输入使用某个活动语法创建不明确的匹配项时引发。  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> 或 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>。 当识别器终结识别操作时引发。  
  
 使用此方法时，识别器不会引发 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> 事件。  
  
 <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize> <xref:System.Speech.Recognition.RecognitionResult> 如果操作不成功，则此方法返回一个对象 `null` 。  
  
 同步识别操作可能会由于以下原因而失败：  
  
-   在或参数的超时间隔到期之前，不会检测到语音 <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A> `initialSilenceTimeout` 。  
  
-   识别引擎检测语音，但在任何已加载和启用的对象中找不到匹配项 <xref:System.Speech.Recognition.Grammar> 。  
  
 若要执行异步识别，请使用其中一种 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> 方法。  
  
   
  
## Examples  
 下面的示例演示了演示基本语音识别的控制台应用程序的一部分。 该示例创建一个 <xref:System.Speech.Recognition.DictationGrammar> ，将其加载到进程内语音识别器，并执行一个识别操作。  
  
```csharp  
  
using System;  
using System.Speech.Recognition;  
  
namespace SynchronousRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
      // Create an in-process speech recognizer for the en-US locale.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(  
          new System.Globalization.CultureInfo("en-US")))  
      {  
        // Create and load a dictation grammar.  
        recognizer.LoadGrammar(new DictationGrammar());  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start synchronous speech recognition.  
        RecognitionResult result = recognizer.Recognize(TimeSpan.FromSeconds(5));  
  
        if (result != null)  
        {  
          Console.WriteLine("Recognized text = {0}", result.Text);  
        }  
        else  
        {  
          Console.WriteLine("No recognition result available.");  
        }  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to continue...");  
      Console.ReadKey();  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String)" />
      </Docs>
    </Member>
    <MemberGroup MemberName="RecognizeAsync">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>启动异步语音识别操作。</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 这些方法执行单个或多个异步识别操作。 识别器对其已加载和已启用的语音识别语法执行每个操作。  
  
 在调用此方法时，识别器可以引发以下事件：  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>.  当识别器检测到它可以标识为语音的输入时引发。  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>.  当输入使用某个活动语法创建不明确的匹配项时引发。  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> 或 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>。 当识别器终结识别操作时引发。  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted>. 在 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> 操作完成时引发。  
  
 若要检索异步标识操作的结果，请将事件处理程序附加到识别器的 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> 事件。 当识别器成功完成同步或异步识别操作时，将引发此事件。 如果识别不成功，则 <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.Result%2A> 对象上可以访问的属性的属性 <xref:System.Speech.Recognition.RecognizeCompletedEventArgs> <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> 将为 `null` 。  
  
 异步识别操作可能会由于以下原因而失败：  
  
-   在或属性的超时间隔到期之前，不会检测到语音 <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A> <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> 。  
  
-   识别引擎检测语音，但在任何已加载和启用的对象中找不到匹配项 <xref:System.Speech.Recognition.Grammar> 。  
  
-   在 <xref:System.Speech.Recognition.SpeechRecognitionEngine> <xref:System.Speech.Recognition.Grammar> 执行识别之前，必须至少加载一个对象。 若要加载语音识别语法，请使用 <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A> 或 <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> 方法。  
  
-   若要修改识别器如何处理语音或静默处理识别的时间，请使用、、 <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A> <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A> 和 <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> 属性。  
  
-   若要执行同步识别，请使用其中一种 <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A> 方法。  
  
 ]]></format>
        </remarks>
      </Docs>
    </MemberGroup>
    <Member MemberName="RecognizeAsync">
      <MemberSignature Language="C#" Value="public void RecognizeAsync ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void RecognizeAsync() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
      <MemberSignature Language="VB.NET" Value="Public Sub RecognizeAsync ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void RecognizeAsync();" />
      <MemberSignature Language="F#" Value="member this.RecognizeAsync : unit -&gt; unit" Usage="speechRecognitionEngine.RecognizeAsync " />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Attributes>
        <Attribute FrameworkAlternate="netframework-4.0">
          <AttributeName Language="C#">[System.Runtime.TargetedPatchingOptOut("Performance critical to inline this type of method across NGen image boundaries")]</AttributeName>
          <AttributeName Language="F#">[&lt;System.Runtime.TargetedPatchingOptOut("Performance critical to inline this type of method across NGen image boundaries")&gt;]</AttributeName>
        </Attribute>
      </Attributes>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>执行单个异步语音识别操作。</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 此方法执行单个异步标识操作。 识别器对其已加载和已启用的语音识别语法执行该操作。  
  
 在调用此方法时，识别器可以引发以下事件：  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>.  当识别器检测到它可以标识为语音的输入时引发。  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>.  当输入使用某个活动语法创建不明确的匹配项时引发。  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> 或 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>。 当识别器终结识别操作时引发。  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted>. 在 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> 操作完成时引发。  
  
 若要检索异步标识操作的结果，请将事件处理程序附加到识别器的 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> 事件。 当识别器成功完成同步或异步识别操作时，将引发此事件。 如果识别不成功，则 <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.Result%2A> 对象上可以访问的属性的属性 <xref:System.Speech.Recognition.RecognizeCompletedEventArgs> <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> 将为 `null` 。  
  
 若要执行同步识别，请使用其中一种 <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A> 方法。  
  
   
  
## Examples  
 下面的示例演示了演示基本异步语音识别的控制台应用程序的一部分。 该示例创建一个 <xref:System.Speech.Recognition.DictationGrammar> ，将其加载到进程内语音识别器，并执行一个异步识别操作。 包含事件处理程序是为了演示识别器在操作过程中引发的事件。  
  
```csharp  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace AsynchronousRecognition  
{  
  class Program  
  {  
    // Indicate whether asynchronous recognition is complete.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
      // Create an in-process speech recognizer.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
        // Create a grammar for choosing cities for a flight.  
        Choices cities = new Choices(new string[]   
        { "Los Angeles", "New York", "Chicago", "San Francisco", "Miami", "Dallas" });  
  
        GrammarBuilder gb = new GrammarBuilder();  
        gb.Append("I want to fly from");  
        gb.Append(cities);  
        gb.Append("to");  
        gb.Append(cities);  
  
        // Construct a Grammar object and load it to the recognizer.  
        Grammar cityChooser = new Grammar(gb);  
        cityChooser.Name = ("City Chooser");  
        recognizer.LoadGrammarAsync(cityChooser);  
  
        // Attach event handlers.  
        recognizer.SpeechDetected +=  
          new EventHandler<SpeechDetectedEventArgs>(  
            SpeechDetectedHandler);  
        recognizer.SpeechHypothesized +=  
          new EventHandler<SpeechHypothesizedEventArgs>(  
            SpeechHypothesizedHandler);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(  
            SpeechRecognitionRejectedHandler);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(  
            RecognizeCompletedHandler);  
  
        // Assign input to the recognizer and start an asynchronous  
        // recognition operation.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        completed = false;  
        Console.WriteLine("Starting asynchronous recognition...");  
        recognizer.RecognizeAsync();  
  
        // Wait for the operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
        Console.WriteLine("Done.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechDetected event.  
    static void SpeechDetectedHandler(object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechDetectedHandler:");  
      Console.WriteLine(" - AudioPosition = {0}", e.AudioPosition);  
    }  
  
    // Handle the SpeechHypothesized event.  
    static void SpeechHypothesizedHandler(  
      object sender, SpeechHypothesizedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechHypothesizedHandler:");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the SpeechRecognitionRejected event.  
    static void SpeechRecognitionRejectedHandler(  
      object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechRecognitionRejectedHandler:");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the SpeechRecognized event.  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechRecognizedHandler.");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void RecognizeCompletedHandler(  
      object sender, RecognizeCompletedEventArgs e)  
    {  
      Console.WriteLine(" In RecognizeCompletedHandler.");  
  
      if (e.Error != null)  
      {  
        Console.WriteLine(  
          " - Error occurred during recognition: {0}", e.Error);  
        return;  
      }  
      if (e.InitialSilenceTimeout || e.BabbleTimeout)  
      {  
        Console.WriteLine(  
          " - BabbleTimeout = {0}; InitialSilenceTimeout = {1}",  
          e.BabbleTimeout, e.InitialSilenceTimeout);  
        return;  
      }  
      if (e.InputStreamEnded)  
      {  
        Console.WriteLine(  
          " - AudioPosition = {0}; InputStreamEnded = {1}",  
          e.AudioPosition, e.InputStreamEnded);  
      }  
      if (e.Result != null)  
      {  
        Console.WriteLine(  
          " - Grammar = {0}; Text = {1}; Confidence = {2}",  
          e.Result.Grammar.Name, e.Result.Text, e.Result.Confidence);  
        Console.WriteLine(" - AudioPosition = {0}", e.AudioPosition);  
      }  
      else  
      {  
        Console.WriteLine(" - No result.");  
      }  
  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
      </Docs>
    </Member>
    <Member MemberName="RecognizeAsync">
      <MemberSignature Language="C#" Value="public void RecognizeAsync (System.Speech.Recognition.RecognizeMode mode);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void RecognizeAsync(valuetype System.Speech.Recognition.RecognizeMode mode) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync(System.Speech.Recognition.RecognizeMode)" />
      <MemberSignature Language="VB.NET" Value="Public Sub RecognizeAsync (mode As RecognizeMode)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void RecognizeAsync(System::Speech::Recognition::RecognizeMode mode);" />
      <MemberSignature Language="F#" Value="member this.RecognizeAsync : System.Speech.Recognition.RecognizeMode -&gt; unit" Usage="speechRecognitionEngine.RecognizeAsync mode" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="mode" Type="System.Speech.Recognition.RecognizeMode" />
      </Parameters>
      <Docs>
        <param name="mode">指示是否执行一个或多个识别操作。</param>
        <summary>执行一个或多个异步语音识别操作。</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 如果 `mode` 为 <xref:System.Speech.Recognition.RecognizeMode.Multiple> ，则在 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncCancel%2A> 调用或方法之前，识别器将继续执行异步标识操作 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncStop%2A> 。  
  
 在调用此方法时，识别器可以引发以下事件：  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>.  当识别器检测到它可以标识为语音的输入时引发。  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>.  当输入使用某个活动语法创建不明确的匹配项时引发。  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> 或 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>。 当识别器终结识别操作时引发。  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted>. 在 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> 操作完成时引发。  
  
 若要检索异步标识操作的结果，请将事件处理程序附加到识别器的 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> 事件。 当识别器成功完成同步或异步识别操作时，将引发此事件。 如果识别不成功，则 <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.Result%2A> 对象上可以访问的属性的属性 <xref:System.Speech.Recognition.RecognizeCompletedEventArgs> <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> 将为 `null` 。  
  
 异步识别操作可能会由于以下原因而失败：  
  
-   在或属性的超时间隔到期之前，不会检测到语音 <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A> <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> 。  
  
-   识别引擎检测语音，但在任何已加载和启用的对象中找不到匹配项 <xref:System.Speech.Recognition.Grammar> 。  
  
 若要执行同步识别，请使用其中一种 <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A> 方法。  
  
   
  
## Examples  
 下面的示例演示了演示基本异步语音识别的控制台应用程序的一部分。 该示例创建一个 <xref:System.Speech.Recognition.DictationGrammar> ，将其加载到进程内语音识别器中，并执行多个异步标识操作。 异步操作将在30秒后取消。 包含事件处理程序是为了演示识别器在操作过程中引发的事件。  
  
```csharp  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace AsynchronousRecognition  
{  
  class Program  
  {  
    // Indicate whether asynchronous recognition is complete.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
      // Create an in-process speech recognizer.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
        // Create a grammar for choosing cities for a flight.  
        Choices cities = new Choices(new string[] { "Los Angeles", "New York", "Chicago", "San Francisco", "Miami", "Dallas" });  
  
        GrammarBuilder gb = new GrammarBuilder();  
        gb.Append("I want to fly from");  
        gb.Append(cities);  
        gb.Append("to");  
        gb.Append(cities);  
  
        // Construct a Grammar object and load it to the recognizer.  
        Grammar cityChooser = new Grammar(gb);  
        cityChooser.Name = ("City Chooser");  
        recognizer.LoadGrammarAsync(cityChooser);  
  
        // Attach event handlers.  
        recognizer.SpeechDetected +=  
          new EventHandler<SpeechDetectedEventArgs>(  
            SpeechDetectedHandler);  
        recognizer.SpeechHypothesized +=  
          new EventHandler<SpeechHypothesizedEventArgs>(  
            SpeechHypothesizedHandler);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(  
            SpeechRecognitionRejectedHandler);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(  
            RecognizeCompletedHandler);  
  
        // Assign input to the recognizer and start asynchronous  
        // recognition.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        completed = false;  
        Console.WriteLine("Starting asynchronous recognition...");  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Wait 30 seconds, and then cancel asynchronous recognition.  
        Thread.Sleep(TimeSpan.FromSeconds(30));  
        recognizer.RecognizeAsyncCancel();  
  
        // Wait for the operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
        Console.WriteLine("Done.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechDetected event.  
    static void SpeechDetectedHandler(object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechDetectedHandler:");  
      Console.WriteLine(" - AudioPosition = {0}", e.AudioPosition);  
    }  
  
    // Handle the SpeechHypothesized event.  
    static void SpeechHypothesizedHandler(  
      object sender, SpeechHypothesizedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechHypothesizedHandler:");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the SpeechRecognitionRejected event.  
    static void SpeechRecognitionRejectedHandler(  
      object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechRecognitionRejectedHandler:");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the SpeechRecognized event.  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechRecognizedHandler.");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void RecognizeCompletedHandler(  
      object sender, RecognizeCompletedEventArgs e)  
    {  
      Console.WriteLine(" In RecognizeCompletedHandler.");  
  
      if (e.Error != null)  
      {  
        Console.WriteLine(  
          " - Error occurred during recognition: {0}", e.Error);  
        return;  
      }  
      if (e.InitialSilenceTimeout || e.BabbleTimeout)  
      {  
        Console.WriteLine(  
          " - BabbleTimeout = {0}; InitialSilenceTimeout = {1}",  
          e.BabbleTimeout, e.InitialSilenceTimeout);  
        return;  
      }  
      if (e.InputStreamEnded)  
      {  
        Console.WriteLine(  
          " - AudioPosition = {0}; InputStreamEnded = {1}",  
          e.AudioPosition, e.InputStreamEnded);  
      }  
      if (e.Result != null)  
      {  
        Console.WriteLine(  
          " - Grammar = {0}; Text = {1}; Confidence = {2}",  
          e.Result.Grammar.Name, e.Result.Text, e.Result.Confidence);  
        Console.WriteLine(" - AudioPosition = {0}", e.AudioPosition);  
      }  
      else  
      {  
        Console.WriteLine(" - No result.");  
      }  
  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
      </Docs>
    </Member>
    <Member MemberName="RecognizeAsyncCancel">
      <MemberSignature Language="C#" Value="public void RecognizeAsyncCancel ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void RecognizeAsyncCancel() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncCancel" />
      <MemberSignature Language="VB.NET" Value="Public Sub RecognizeAsyncCancel ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void RecognizeAsyncCancel();" />
      <MemberSignature Language="F#" Value="member this.RecognizeAsyncCancel : unit -&gt; unit" Usage="speechRecognitionEngine.RecognizeAsyncCancel " />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>停止异步标识，而不必等待当前标识该操作完成。</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 此方法立即终结异步识别。 如果当前的异步识别操作正在接收输入，则会截断输入，并以现有输入完成操作。 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> 当取消异步操作并将 <xref:System.ComponentModel.AsyncCompletedEventArgs.Cancelled%2A> 的属性设置为时， <xref:System.Speech.Recognition.RecognizeCompletedEventArgs> 识别器将引发或事件 `true` 。 此方法取消由和方法启动的异步操作 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> 。  
  
 若要停止异步识别而不截断输入，请使用 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncStop%2A> 方法。  
  
   
  
## Examples  
 下面的示例演示了控制台应用程序的一部分，该应用程序演示了方法的用法 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncCancel%2A> 。 该示例创建并加载语音识别语法，启动一个持续异步识别操作，然后在取消操作前暂停2秒。 识别器接收来自文件的输入，c:\temp\audioinput\sample.wav。 包含事件处理程序是为了演示识别器在操作过程中引发的事件。  
  
```csharp  
  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace AsynchronousRecognition  
{  
  class Program  
  {  
    // Indicate whether asynchronous recognition is complete.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
      // Create an in-process speech recognizer.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
        // Create and load a dictation grammar.  
        Grammar dictation = new DictationGrammar();  
        dictation.Name = "Dictation Grammar";  
  
        recognizer.LoadGrammar(dictation);  
  
        // Attach event handlers.  
        recognizer.SpeechDetected +=  
          new EventHandler<SpeechDetectedEventArgs>(  
            SpeechDetectedHandler);  
        recognizer.SpeechHypothesized +=  
          new EventHandler<SpeechHypothesizedEventArgs>(  
            SpeechHypothesizedHandler);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(  
            SpeechRecognitionRejectedHandler);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(  
            RecognizeCompletedHandler);  
  
        // Begin asynchronous recognition from pre-recorded input.  
        recognizer.SetInputToWaveFile(@"c:\temp\audioinput\sample.wav");  
  
        completed = false;  
        Console.WriteLine("Begin continuing asynchronous recognition...");  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Wait 2 seconds and then cancel the recognition operation.  
        Thread.Sleep(TimeSpan.FromSeconds(2));  
        recognizer.RecognizeAsyncCancel();  
  
        // Wait for the operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
  
        Console.WriteLine("Done.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechDetected event.  
    static void SpeechDetectedHandler(object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechDetectedHandler:");  
      Console.WriteLine(" - AudioPosition = {0}", e.AudioPosition);  
    }  
  
    // Handle the SpeechHypothesized event.  
    static void SpeechHypothesizedHandler(  
      object sender, SpeechHypothesizedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechHypothesizedHandler:");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the SpeechRecognitionRejected event.  
    static void SpeechRecognitionRejectedHandler(  
      object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechRecognitionRejectedHandler:");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the SpeechRecognized event.  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechRecognizedHandler.");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void RecognizeCompletedHandler(  
      object sender, RecognizeCompletedEventArgs e)  
    {  
      Console.WriteLine(" In RecognizeCompletedHandler.");  
  
      if (e.Error != null)  
      {  
        Console.WriteLine(  
          " - Error occurred during recognition: {0}", e.Error);  
        return;  
      }  
      if (e.Cancelled)  
      {  
        Console.WriteLine(" - asynchronous operation canceled.");  
      }  
      if (e.InitialSilenceTimeout || e.BabbleTimeout)  
      {  
        Console.WriteLine(  
          " - BabbleTimeout = {0}; InitialSilenceTimeout = {1}",  
          e.BabbleTimeout, e.InitialSilenceTimeout);  
        return;  
      }  
      if (e.InputStreamEnded)  
      {  
        Console.WriteLine(  
          " - AudioPosition = {0}; InputStreamEnded = {1}",  
          e.AudioPosition, e.InputStreamEnded);  
      }  
      if (e.Result != null)  
      {  
        Console.WriteLine(  
          " - Grammar = {0}; Text = {1}; Confidence = {2}",  
          e.Result.Grammar.Name, e.Result.Text, e.Result.Confidence);  
      }  
      else  
      {  
        Console.WriteLine(" - No result.");  
      }  
  
      completed = true;  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncStop" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="RecognizeAsyncStop">
      <MemberSignature Language="C#" Value="public void RecognizeAsyncStop ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void RecognizeAsyncStop() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncStop" />
      <MemberSignature Language="VB.NET" Value="Public Sub RecognizeAsyncStop ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void RecognizeAsyncStop();" />
      <MemberSignature Language="F#" Value="member this.RecognizeAsyncStop : unit -&gt; unit" Usage="speechRecognitionEngine.RecognizeAsyncStop " />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>当前标识该操作完成后，停止异步标识。</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 此方法在不截断输入的情况下完成异步识别。 如果当前异步识别操作正在接收输入，则识别器将继续接受输入，直到当前的识别操作完成。 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> 当异步操作停止时，识别器会引发或事件，并将 <xref:System.ComponentModel.AsyncCompletedEventArgs.Cancelled%2A> 的属性设置 <xref:System.Speech.Recognition.RecognizeCompletedEventArgs> 为 `true` 。 此方法停止由和方法启动的异步操作 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> 。  
  
 若要立即取消仅包含现有输入的异步识别，请使用 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncCancel%2A> 方法。  
  
   
  
## Examples  
 下面的示例演示了控制台应用程序的一部分，该应用程序演示了方法的用法 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncStop%2A> 。 该示例创建并加载语音识别语法，启动继续异步识别操作，然后在2秒后停止操作。 识别器接收来自文件的输入，c:\temp\audioinput\sample.wav。 包含事件处理程序是为了演示识别器在操作过程中引发的事件。  
  
```csharp  
  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace AsynchronousRecognition  
{  
  class Program  
  {  
    // Indicate whether asynchronous recognition is complete.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
      // Create an in-process speech recognizer.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
        // Create and load a dictation grammar.  
        Grammar dictation = new DictationGrammar();  
        dictation.Name = "Dictation Grammar";  
  
        recognizer.LoadGrammar(dictation);  
  
        // Attach event handlers.  
        recognizer.SpeechDetected +=  
          new EventHandler<SpeechDetectedEventArgs>(  
            SpeechDetectedHandler);  
        recognizer.SpeechHypothesized +=  
          new EventHandler<SpeechHypothesizedEventArgs>(  
            SpeechHypothesizedHandler);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(  
            SpeechRecognitionRejectedHandler);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(  
            RecognizeCompletedHandler);  
  
        // Begin asynchronous recognition from pre-recorded input.  
        recognizer.SetInputToWaveFile(@"c:\temp\audioinput\sample.wav");  
  
        completed = false;  
        Console.WriteLine("Begin continuing asynchronous recognition...");  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Wait 2 seconds and then stop the recognition operation.  
        Thread.Sleep(TimeSpan.FromSeconds(2));  
        recognizer.RecognizeAsyncStop();  
  
        // Wait for the operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
  
        Console.WriteLine("Done.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechDetected event.  
    static void SpeechDetectedHandler(object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechDetectedHandler:");  
      Console.WriteLine(" - AudioPosition = {0}", e.AudioPosition);  
    }  
  
    // Handle the SpeechHypothesized event.  
    static void SpeechHypothesizedHandler(  
      object sender, SpeechHypothesizedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechHypothesizedHandler:");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the SpeechRecognitionRejected event.  
    static void SpeechRecognitionRejectedHandler(  
      object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechRecognitionRejectedHandler:");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the SpeechRecognized event.  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechRecognizedHandler.");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void RecognizeCompletedHandler(  
      object sender, RecognizeCompletedEventArgs e)  
    {  
      Console.WriteLine(" In RecognizeCompletedHandler.");  
  
      if (e.Error != null)  
      {  
        Console.WriteLine(  
          " - Error occurred during recognition: {0}", e.Error);  
        return;  
      }  
      if (e.Cancelled)  
      {  
        Console.WriteLine(" - asynchronous operation canceled.");  
      }  
      if (e.InitialSilenceTimeout || e.BabbleTimeout)  
      {  
        Console.WriteLine(  
          " - BabbleTimeout = {0}; InitialSilenceTimeout = {1}",  
          e.BabbleTimeout, e.InitialSilenceTimeout);  
        return;  
      }  
      if (e.InputStreamEnded)  
      {  
        Console.WriteLine(  
          " - AudioPosition = {0}; InputStreamEnded = {1}",  
          e.AudioPosition, e.InputStreamEnded);  
      }  
      if (e.Result != null)  
      {  
        Console.WriteLine(  
          " - Grammar = {0}; Text = {1}; Confidence = {2}",  
          e.Result.Grammar.Name, e.Result.Text, e.Result.Confidence);  
      }  
      else  
      {  
        Console.WriteLine(" - No result.");  
      }  
  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncCancel" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="RecognizeCompleted">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.RecognizeCompletedEventArgs&gt; RecognizeCompleted;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.RecognizeCompletedEventArgs&gt; RecognizeCompleted" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
      <MemberSignature Language="VB.NET" Value="Public Custom Event RecognizeCompleted As EventHandler(Of RecognizeCompletedEventArgs) " FrameworkAlternate="netframework-3.0;netframework-3.5;netframework-4.0;netframework-4.5;netframework-4.5.1;netframework-4.5.2" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::RecognizeCompletedEventArgs ^&gt; ^ RecognizeCompleted;" />
      <MemberSignature Language="F#" Value="member this.RecognizeCompleted : EventHandler&lt;System.Speech.Recognition.RecognizeCompletedEventArgs&gt; " Usage="member this.RecognizeCompleted : System.EventHandler&lt;System.Speech.Recognition.RecognizeCompletedEventArgs&gt; " />
      <MemberSignature Language="VB.NET" Value="Public Event RecognizeCompleted As EventHandler(Of RecognizeCompletedEventArgs) " FrameworkAlternate="netframework-4.6;netframework-4.6.1;netframework-4.6.2;netframework-4.7;netframework-4.7.1;netframework-4.7.2;netframework-4.8" />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.RecognizeCompletedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>当 <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> 完成异步标识操作的时候引发。</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 <xref:System.Speech.Recognition.SpeechRecognitionEngine>对象的 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> 方法启动异步识别操作。 当识别器终结异步操作时，它将引发此事件。  
  
 使用事件的处理程序 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> ，你可以访问 <xref:System.Speech.Recognition.RecognitionResult> 对象中的 <xref:System.Speech.Recognition.RecognizeCompletedEventArgs> 。 如果识别未成功， <xref:System.Speech.Recognition.RecognitionResult> 则为 `null` 。 若要确定音频输入中的超时或中断是否导致了识别失败，你可以访问 <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.InitialSilenceTimeout%2A> 、或的属性 <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.BabbleTimeout%2A> <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.InputStreamEnded%2A> 。  
  
 有关更多信息，请参见 <xref:System.Speech.Recognition.RecognizeCompletedEventArgs> 类。  
  
 若要获取有关已拒绝的最佳识别的详细信息，请为事件附加处理程序 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> 。  
  
 创建 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> 委托时，需要标识将处理该事件的方法。 若要将事件与事件处理程序关联，请将该委托的一个实例添加到事件中。 除非移除了该委托，否则每当发生该事件时就会调用事件处理程序。 有关事件处理程序委托的详细信息，请参阅 [事件和委托](https://go.microsoft.com/fwlink/?LinkId=162418)。  
  
   
  
## Examples  
 下面的示例将识别一些短语，如 "在爵士乐类别中显示音乐家列表" 或 "显示唱片集 gospel"。 该示例使用事件的处理程序 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> 在控制台中显示有关识别结果的信息。  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
  
    // Initialize an in-process speech recognition engine.  
    {  
      using (SpeechRecognitionEngine recognizer =  
         new SpeechRecognitionEngine())  
      {  
  
        //  Create lists of alternative choices.  
        Choices listTypes = new Choices(new string[] { "albums", "artists" });  
        Choices genres = new Choices(new string[] {   
          "blues", "classical", "gospel", "jazz", "rock" });  
  
        //  Create a GrammarBuilder object and assemble the grammar components.  
        GrammarBuilder mediaMenu = new GrammarBuilder("Display");  
        mediaMenu.Append("the list of", 0, 1);  
        mediaMenu.Append(listTypes);  
        mediaMenu.Append("in the", 0, 1);  
        mediaMenu.Append(genres);  
        mediaMenu.Append("category.", 0, 1);  
  
        //  Build a Grammar object from the GrammarBuilder.  
        Grammar mediaMenuGrammar = new Grammar(mediaMenu);  
        mediaMenuGrammar.Name = "Media Chooser";  
  
        // Attach event handlers.  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(recognizer_RecognizeCompleted);  
        recognizer.LoadGrammarCompleted +=   
          new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
  
        // Load the grammar object to the recognizer.  
        recognizer.LoadGrammarAsync(mediaMenuGrammar);  
  
        // Set the input to the recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start asynchronous, continuous recognition.  
        recognizer.RecognizeAsync();  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void recognizer_RecognizeCompleted(object sender, RecognizeCompletedEventArgs e)  
    {  
      if (e.Error != null)  
      {  
        Console.WriteLine(  
          "RecognizeCompleted, error occurred during recognition: {0}", e.Error);  
        return;  
      }  
  
      if (e.InitialSilenceTimeout || e.BabbleTimeout)  
      {  
        Console.WriteLine(  
          "RecognizeCompleted: BabbleTimeout({0}), InitialSilenceTimeout({1}).",  
          e.BabbleTimeout, e.InitialSilenceTimeout);  
        return;  
      }  
  
      if (e.InputStreamEnded)  
      {  
        Console.WriteLine(  
          "RecognizeCompleted: AudioPosition({0}), InputStreamEnded({1}).",  
          e.AudioPosition, e.InputStreamEnded);  
      }  
  
      if (e.Result != null)  
      {  
        Console.WriteLine("RecognizeCompleted:");  
        Console.WriteLine("  Grammar: " + e.Result.Grammar.Name);  
        Console.WriteLine("  Recognized text: " + e.Result.Text);  
        Console.WriteLine("  Confidence score: " + e.Result.Confidence);  
        Console.WriteLine("  Audio position: " + e.AudioPosition);  
      }  
  
      else  
      {  
        Console.WriteLine("RecognizeCompleted: No result.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the LoadGrammarCompleted event.  
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      Console.WriteLine("Grammar loaded:  " + e.Grammar.Name);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.RecognizeCompletedEventArgs" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
      </Docs>
    </Member>
    <Member MemberName="RecognizerAudioPosition">
      <MemberSignature Language="C#" Value="public TimeSpan RecognizerAudioPosition { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.TimeSpan RecognizerAudioPosition" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property RecognizerAudioPosition As TimeSpan" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property TimeSpan RecognizerAudioPosition { TimeSpan get(); };" />
      <MemberSignature Language="F#" Value="member this.RecognizerAudioPosition : TimeSpan" Usage="System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.TimeSpan</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>获取在处理的音频输入中的 <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> 当前位置。</summary>
        <value>识别器的位置在处理音频的输入。</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 音频位置特定于每个语音识别器。 输入流的零值在启用后建立。  
  
 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A>属性引用 <xref:System.Speech.Recognition.SpeechRecognitionEngine> 对象在其音频输入中的位置。 与此相反，该 <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A> 属性将引用输入设备在其生成的音频流中的位置。 这些位置可能不同。 例如，如果识别器收到了尚未生成识别结果的输入，则该属性的值 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A> 小于属性的值 <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A> 。  
  
 ]]></format>
        </remarks>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition" />
      </Docs>
    </Member>
    <Member MemberName="RecognizerInfo">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.RecognizerInfo RecognizerInfo { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance class System.Speech.Recognition.RecognizerInfo RecognizerInfo" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerInfo" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property RecognizerInfo As RecognizerInfo" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property System::Speech::Recognition::RecognizerInfo ^ RecognizerInfo { System::Speech::Recognition::RecognizerInfo ^ get(); };" />
      <MemberSignature Language="F#" Value="member this.RecognizerInfo : System.Speech.Recognition.RecognizerInfo" Usage="System.Speech.Recognition.SpeechRecognitionEngine.RecognizerInfo" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.RecognizerInfo</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>获取有关 <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> 的当前实例的信息。</summary>
        <value>有关当前语音识别器的相关信息。</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 若要获取有关当前系统所有已安装的语音识别器的信息，请使用 <xref:System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers%2A> 方法。  
  
   
  
## Examples  
 下面的示例获取当前进程内语音识别引擎的部分数据列表。 有关详细信息，请参阅 <xref:System.Speech.Recognition.RecognizerInfo>。  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace RecognitionEngine  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
      using (SpeechRecognitionEngine recognizer = new SpeechRecognitionEngine())  
      {  
        Console.WriteLine("Information for the current speech recognition engine:");  
        Console.WriteLine("  Name: {0}", recognizer.RecognizerInfo.Name);  
        Console.WriteLine("  Culture: {0}", recognizer.RecognizerInfo.Culture.ToString());  
        Console.WriteLine("  Description: {0}", recognizer.RecognizerInfo.Description);  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.#ctor(System.Speech.Recognition.RecognizerInfo)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers" />
      </Docs>
    </Member>
    <Member MemberName="RecognizerUpdateReached">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.RecognizerUpdateReachedEventArgs&gt; RecognizerUpdateReached;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.RecognizerUpdateReachedEventArgs&gt; RecognizerUpdateReached" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached" />
      <MemberSignature Language="VB.NET" Value="Public Custom Event RecognizerUpdateReached As EventHandler(Of RecognizerUpdateReachedEventArgs) " FrameworkAlternate="netframework-3.0;netframework-3.5;netframework-4.0;netframework-4.5;netframework-4.5.1;netframework-4.5.2" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::RecognizerUpdateReachedEventArgs ^&gt; ^ RecognizerUpdateReached;" />
      <MemberSignature Language="F#" Value="member this.RecognizerUpdateReached : EventHandler&lt;System.Speech.Recognition.RecognizerUpdateReachedEventArgs&gt; " Usage="member this.RecognizerUpdateReached : System.EventHandler&lt;System.Speech.Recognition.RecognizerUpdateReachedEventArgs&gt; " />
      <MemberSignature Language="VB.NET" Value="Public Event RecognizerUpdateReached As EventHandler(Of RecognizerUpdateReachedEventArgs) " FrameworkAlternate="netframework-4.6;netframework-4.6.1;netframework-4.6.2;netframework-4.7;netframework-4.7.1;netframework-4.7.2;netframework-4.8" />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.RecognizerUpdateReachedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>当运行的 <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> 暂停接受修改时引发。</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> <xref:System.Speech.Recognition.SpeechRecognitionEngine> 在修改其设置或其对象之前，应用程序必须使用暂停正在运行的实例 <xref:System.Speech.Recognition.Grammar> 。 <xref:System.Speech.Recognition.SpeechRecognitionEngine>当已准备好接受修改时，将引发此事件。  
  
 例如，当暂停时 <xref:System.Speech.Recognition.SpeechRecognitionEngine> ，可以加载、卸载、启用和禁用 <xref:System.Speech.Recognition.Grammar> 对象，以及修改、和属性的值 <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A> <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A> 。 有关更多信息，请参见 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> 方法。  
  
 创建 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> 委托时，需要标识将处理该事件的方法。 若要将事件与事件处理程序关联，请将该委托的一个实例添加到事件中。 除非移除了该委托，否则每当发生该事件时就会调用事件处理程序。 有关事件处理程序委托的详细信息，请参阅 [事件和委托](https://go.microsoft.com/fwlink/?LinkId=162418)。  
  
   
  
## Examples  
 下面的示例演示了一个加载和卸载对象的控制台应用程序 <xref:System.Speech.Recognition.Grammar> 。 应用程序使用 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> 方法请求语音识别引擎暂停，使其可以接收更新。 然后，应用程序将加载或卸载 <xref:System.Speech.Recognition.Grammar> 对象。  
  
 在每次更新时，事件处理程序会将 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> 当前加载的对象的名称和状态写入 <xref:System.Speech.Recognition.Grammar> 控制台。 加载和卸载语法时，应用程序首先识别场动物的名称，然后识别场动物的名称和水果的名称，然后仅识别水果的名称。  
  
```  
using System;  
using System.Speech.Recognition;  
using System.Collections.Generic;  
using System.Threading;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    private static SpeechRecognitionEngine recognizer;  
    public static void Main(string[] args)  
    {  
  
      // Initialize an in-process speech recognition engine and configure its input.  
      using (recognizer = new SpeechRecognitionEngine(new System.Globalization.CultureInfo("en-US")))  
      {  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Create the first grammar - Farm.  
        Choices animals = new Choices(new string[] { "cow", "pig", "goat" });  
        GrammarBuilder farm = new GrammarBuilder(animals);  
        Grammar farmAnimals = new Grammar(farm);  
        farmAnimals.Name = "Farm";  
  
        // Create the second grammar - Fruit.  
        Choices fruit = new Choices(new string[] { "apples", "peaches", "oranges" });  
        GrammarBuilder favorite = new GrammarBuilder(fruit);  
        Grammar favoriteFruit = new Grammar(favorite);  
        favoriteFruit.Name = "Fruit";  
  
        // Attach event handlers.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
        recognizer.RecognizerUpdateReached +=  
          new EventHandler<RecognizerUpdateReachedEventArgs>(recognizer_RecognizerUpdateReached);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(recognizer_SpeechRecognitionRejected);  
  
        // Load the Farm grammar.  
        recognizer.LoadGrammar(farmAnimals);  
  
        // Start asynchronous, continuous recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
        Console.WriteLine("Starting asynchronous, continuous recognition");  
        Console.WriteLine("  Farm grammar is loaded and enabled.");  
  
        // Pause to recognize farm animals.  
        Thread.Sleep(7000);  
        Console.WriteLine();  
  
        // Request an update and load the Fruit grammar.  
        recognizer.RequestRecognizerUpdate();  
        recognizer.LoadGrammarAsync(favoriteFruit);  
        Thread.Sleep(7000);  
  
        // Request an update and unload the Farm grammar.  
        recognizer.RequestRecognizerUpdate();  
        recognizer.UnloadGrammar(farmAnimals);  
        Thread.Sleep(7000);  
      }  
  
      // Keep the console window open.  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // At the update, get the names and enabled status of the currently loaded grammars.  
    public static void recognizer_RecognizerUpdateReached(  
      object sender, RecognizerUpdateReachedEventArgs e)  
    {  
      Console.WriteLine();  
      Console.WriteLine("Update reached:");  
      Thread.Sleep(1000);  
  
      string qualifier;  
      List<Grammar> grammars = new List<Grammar>(recognizer.Grammars);  
      foreach (Grammar g in grammars)  
      {  
        qualifier = (g.Enabled) ? "enabled" : "disabled";  
        Console.WriteLine("  {0} grammar is loaded and {1}.",  
        g.Name, qualifier);  
      }  
    }  
  
    // Write the text of the recognized phrase to the console.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("    Speech recognized: " + e.Result.Text);  
    }  
  
    // Write a message to the console when recognition fails.  
    static void recognizer_SpeechRecognitionRejected(object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine("    Recognition attempt failed");  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.RecognizerUpdateReachedEventArgs" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate" />
        <altmember cref="T:System.Speech.Recognition.SpeechDetectedEventArgs" />
      </Docs>
    </Member>
    <MemberGroup MemberName="RequestRecognizerUpdate">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>请求识别器暂停更新其状态。</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 使用此方法将更改同步到识别器。 例如，如果在识别器正在处理输入时加载或卸载语音识别语法，请使用此方法和事件，将 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> 应用程序行为与识别器的状态进行同步。  
  
 调用此方法时，识别器将暂停或完成异步操作并生成一个 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> 事件。 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached>然后，事件处理程序可以在识别操作之间修改识别器的状态。 处理 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> 事件时，识别器将暂停，直到事件处理程序返回。  
  
> [!NOTE]
>  如果识别器的输入在识别器引发事件之前发生了更改 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> ，则放弃该请求。  
  
 调用此方法时：  
  
-   如果识别器未处理输入，则识别器会立即生成 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> 事件。  
  
-   如果识别器正在处理由无声或背景噪音组成的输入，识别器将暂停识别操作并生成 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> 事件。  
  
-   如果识别器正在处理的输入不包括无声或背景干扰，则识别器完成识别操作，然后生成 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> 事件。  
  
 识别器处理 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> 事件时：  
  
-   识别器不处理输入，属性的值 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A> 保持不变。  
  
-   识别器继续收集输入，属性的值 <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A> 可以更改。  
  
 ]]></format>
        </remarks>
      </Docs>
    </MemberGroup>
    <Member MemberName="RequestRecognizerUpdate">
      <MemberSignature Language="C#" Value="public void RequestRecognizerUpdate ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void RequestRecognizerUpdate() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate" />
      <MemberSignature Language="VB.NET" Value="Public Sub RequestRecognizerUpdate ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void RequestRecognizerUpdate();" />
      <MemberSignature Language="F#" Value="member this.RequestRecognizerUpdate : unit -&gt; unit" Usage="speechRecognitionEngine.RequestRecognizerUpdate " />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>请求识别器暂停更新其状态。</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 当识别器生成 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> 事件时， <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs.UserToken%2A> 的属性 <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs> 为 `null` 。  
  
 若要提供用户令牌，请使用 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> 或 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> 方法。 若要指定音频位置偏移量，请使用 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> 方法。  
  
   
  
## Examples  
 下面的示例演示了一个加载和卸载对象的控制台应用程序 <xref:System.Speech.Recognition.Grammar> 。 应用程序使用 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> 方法请求语音识别引擎暂停，使其可以接收更新。 然后，应用程序将加载或卸载 <xref:System.Speech.Recognition.Grammar> 对象。  
  
 在每次更新时，事件处理程序会将 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> 当前加载的对象的名称和状态写入 <xref:System.Speech.Recognition.Grammar> 控制台。 加载和卸载语法时，应用程序首先识别场动物的名称，然后识别场动物的名称和水果的名称，然后仅识别水果的名称。  
  
```  
using System;  
using System.Speech.Recognition;  
using System.Collections.Generic;  
using System.Threading;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    private static SpeechRecognitionEngine recognizer;  
    public static void Main(string[] args)  
    {  
  
      // Initialize an in-process speech recognition engine and configure its input.  
      using (recognizer = new SpeechRecognitionEngine(new System.Globalization.CultureInfo("en-US")))  
      {  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Create the first grammar - Farm.  
        Choices animals = new Choices(new string[] { "cow", "pig", "goat" });  
        GrammarBuilder farm = new GrammarBuilder(animals);  
        Grammar farmAnimals = new Grammar(farm);  
        farmAnimals.Name = "Farm";  
  
        // Create the second grammar - Fruit.  
        Choices fruit = new Choices(new string[] { "apples", "peaches", "oranges" });  
        GrammarBuilder favorite = new GrammarBuilder(fruit);  
        Grammar favoriteFruit = new Grammar(favorite);  
        favoriteFruit.Name = "Fruit";  
  
        // Attach event handlers.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
        recognizer.RecognizerUpdateReached +=  
          new EventHandler<RecognizerUpdateReachedEventArgs>(recognizer_RecognizerUpdateReached);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(recognizer_SpeechRecognitionRejected);  
  
        // Load the Farm grammar.  
        recognizer.LoadGrammar(farmAnimals);  
  
        // Start asynchronous, continuous recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
        Console.WriteLine("Starting asynchronous, continuous recognition");  
        Console.WriteLine("  Farm grammar is loaded and enabled.");  
  
        // Pause to recognize farm animals.  
        Thread.Sleep(7000);  
        Console.WriteLine();  
  
        // Request an update and load the Fruit grammar.  
        recognizer.RequestRecognizerUpdate();  
        recognizer.LoadGrammarAsync(favoriteFruit);  
        Thread.Sleep(7000);  
  
        // Request an update and unload the Farm grammar.  
        recognizer.RequestRecognizerUpdate();  
        recognizer.UnloadGrammar(farmAnimals);  
        Thread.Sleep(7000);  
      }  
  
      // Keep the console window open.  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // At the update, get the names and enabled status of the currently loaded grammars.  
    public static void recognizer_RecognizerUpdateReached(  
      object sender, RecognizerUpdateReachedEventArgs e)  
    {  
      Console.WriteLine();  
      Console.WriteLine("Update reached:");  
      Thread.Sleep(1000);  
  
      string qualifier;  
      List<Grammar> grammars = new List<Grammar>(recognizer.Grammars);  
      foreach (Grammar g in grammars)  
      {  
        qualifier = (g.Enabled) ? "enabled" : "disabled";  
        Console.WriteLine("  {0} grammar is loaded and {1}.",  
        g.Name, qualifier);  
      }  
    }  
  
    // Write the text of the recognized phrase to the console.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("    Speech recognized: " + e.Result.Text);  
    }  
  
    // Write a message to the console when recognition fails.  
    static void recognizer_SpeechRecognitionRejected(object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine("    Recognition attempt failed");  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition" />
      </Docs>
    </Member>
    <Member MemberName="RequestRecognizerUpdate">
      <MemberSignature Language="C#" Value="public void RequestRecognizerUpdate (object userToken);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void RequestRecognizerUpdate(object userToken) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate(System.Object)" />
      <MemberSignature Language="VB.NET" Value="Public Sub RequestRecognizerUpdate (userToken As Object)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void RequestRecognizerUpdate(System::Object ^ userToken);" />
      <MemberSignature Language="F#" Value="member this.RequestRecognizerUpdate : obj -&gt; unit" Usage="speechRecognitionEngine.RequestRecognizerUpdate userToken" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="userToken" Type="System.Object" />
      </Parameters>
      <Docs>
        <param name="userToken">用户定义信息包含该操作的信息。</param>
        <summary>请求识别器暂停更新其状态并为关联的事件提供用户标记。</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 当识别器生成 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> 事件时， <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs.UserToken%2A> 的属性将 <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs> 包含参数的值 `userToken` 。  
  
 若要指定音频位置偏移量，请使用 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> 方法。  
  
 ]]></format>
        </remarks>
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition" />
      </Docs>
    </Member>
    <Member MemberName="RequestRecognizerUpdate">
      <MemberSignature Language="C#" Value="public void RequestRecognizerUpdate (object userToken, TimeSpan audioPositionAheadToRaiseUpdate);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void RequestRecognizerUpdate(object userToken, valuetype System.TimeSpan audioPositionAheadToRaiseUpdate) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate(System.Object,System.TimeSpan)" />
      <MemberSignature Language="VB.NET" Value="Public Sub RequestRecognizerUpdate (userToken As Object, audioPositionAheadToRaiseUpdate As TimeSpan)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void RequestRecognizerUpdate(System::Object ^ userToken, TimeSpan audioPositionAheadToRaiseUpdate);" />
      <MemberSignature Language="F#" Value="member this.RequestRecognizerUpdate : obj * TimeSpan -&gt; unit" Usage="speechRecognitionEngine.RequestRecognizerUpdate (userToken, audioPositionAheadToRaiseUpdate)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="userToken" Type="System.Object" />
        <Parameter Name="audioPositionAheadToRaiseUpdate" Type="System.TimeSpan" />
      </Parameters>
      <Docs>
        <param name="userToken">用户定义信息包含该操作的信息。</param>
        <param name="audioPositionAheadToRaiseUpdate">从当前到延迟请求的 <see cref="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition" /> 偏移量。</param>
        <summary>请求识别器暂停更新其状态并为关联的事件提供偏移量和用户标记。</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 在识别器等于当前的加号之前，识别器不会启动识别器更新请求 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A> <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A> `audioPositionAheadToRaiseUpdate` 。  
  
 当识别器生成 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> 事件时， <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs.UserToken%2A> 的属性将 <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs> 包含参数的值 `userToken` 。  
  
 ]]></format>
        </remarks>
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition" />
      </Docs>
    </Member>
    <Member MemberName="SetInputToAudioStream">
      <MemberSignature Language="C#" Value="public void SetInputToAudioStream (System.IO.Stream audioSource, System.Speech.AudioFormat.SpeechAudioFormatInfo audioFormat);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void SetInputToAudioStream(class System.IO.Stream audioSource, class System.Speech.AudioFormat.SpeechAudioFormatInfo audioFormat) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream(System.IO.Stream,System.Speech.AudioFormat.SpeechAudioFormatInfo)" />
      <MemberSignature Language="VB.NET" Value="Public Sub SetInputToAudioStream (audioSource As Stream, audioFormat As SpeechAudioFormatInfo)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void SetInputToAudioStream(System::IO::Stream ^ audioSource, System::Speech::AudioFormat::SpeechAudioFormatInfo ^ audioFormat);" />
      <MemberSignature Language="F#" Value="member this.SetInputToAudioStream : System.IO.Stream * System.Speech.AudioFormat.SpeechAudioFormatInfo -&gt; unit" Usage="speechRecognitionEngine.SetInputToAudioStream (audioSource, audioFormat)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="audioSource" Type="System.IO.Stream" />
        <Parameter Name="audioFormat" Type="System.Speech.AudioFormat.SpeechAudioFormatInfo" />
      </Parameters>
      <Docs>
        <param name="audioSource">音频输入流。</param>
        <param name="audioFormat">这种音频输入的格式。</param>
        <summary>配置 <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> 对象以接收来自音频流的输入。</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 如果识别器在识别操作期间到达输入流的末尾，则识别操作将使用可用的输入完成。 任何后续的识别操作都可能会生成异常，除非你将输入更新为识别器。  
  
   
  
## Examples  
 下面的示例演示了演示基本语音识别的控制台应用程序的一部分。 该示例使用包含短语 "测试测试 1 2 3" 和 "mister cooper" 的音频文件的输入，并以 pause 分隔。 该示例生成以下输出。  
  
```  
  
Starting asynchronous recognition...  
  Recognized text =  Testing testing 123  
  Recognized text =  Mr. Cooper  
  End of stream encountered.  
Done.  
  
Press any key to exit...  
```  
  
```csharp  
  
using System;  
using System.Globalization;  
using System.IO;  
using System.Speech.AudioFormat;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace InputExamples  
{  
  class Program  
  {  
    // Indicate whether asynchronous recognition is complete.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
  
        // Create and load a grammar.  
        Grammar dictation = new DictationGrammar();  
        dictation.Name = "Dictation Grammar";  
  
        recognizer.LoadGrammar(dictation);  
  
        // Configure the input to the recognizer.  
        recognizer.SetInputToAudioStream(  
          File.OpenRead(@"c:\temp\audioinput\example.wav"),  
          new SpeechAudioFormatInfo(  
            44100, AudioBitsPerSample.Sixteen, AudioChannel.Mono));  
  
        // Attach event handlers.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(  
            RecognizeCompletedHandler);  
  
        // Perform recognition of the whole file.  
        Console.WriteLine("Starting asynchronous recognition...");  
        completed = false;  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
        Console.WriteLine("Done.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechRecognized event.  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      if (e.Result != null && e.Result.Text != null)  
      {  
        Console.WriteLine("  Recognized text =  {0}", e.Result.Text);  
      }  
      else  
      {  
        Console.WriteLine("  Recognized text not available.");  
      }  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void RecognizeCompletedHandler(  
      object sender, RecognizeCompletedEventArgs e)  
    {  
      if (e.Error != null)  
      {  
        Console.WriteLine("  Error encountered, {0}: {1}",  
          e.Error.GetType().Name, e.Error.Message);  
      }  
      if (e.Cancelled)  
      {  
        Console.WriteLine("  Operation cancelled.");  
      }  
      if (e.InputStreamEnded)  
      {  
        Console.WriteLine("  End of stream encountered.");  
      }  
  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream(System.IO.Stream)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="SetInputToDefaultAudioDevice">
      <MemberSignature Language="C#" Value="public void SetInputToDefaultAudioDevice ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void SetInputToDefaultAudioDevice() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice" />
      <MemberSignature Language="VB.NET" Value="Public Sub SetInputToDefaultAudioDevice ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void SetInputToDefaultAudioDevice();" />
      <MemberSignature Language="F#" Value="member this.SetInputToDefaultAudioDevice : unit -&gt; unit" Usage="speechRecognitionEngine.SetInputToDefaultAudioDevice " />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>配置 <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> 对象以接收来自默认音频设备的输入。</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Examples  
 下面的示例演示了演示基本语音识别的控制台应用程序的一部分。 该示例使用默认音频设备的输出，执行多个异步识别操作，并在用户 utters "exit" 短语时退出。  
  
```csharp  
  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace DefaultInput  
{  
  class Program  
  {  
    // Indicate whether asynchronous recognition has finished.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
  
        // Create and load the exit grammar.  
        Grammar exitGrammar = new Grammar(new GrammarBuilder("exit"));  
        exitGrammar.Name = "Exit Grammar";  
        recognizer.LoadGrammar(exitGrammar);  
  
        // Create and load the dictation grammar.  
        Grammar dictation = new DictationGrammar();  
        dictation.Name = "Dictation Grammar";  
        recognizer.LoadGrammar(dictation);  
  
        // Attach event handlers to the recognizer.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(  
            RecognizeCompletedHandler);  
  
        // Assign input to the recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Begin asynchronous recognition.  
        Console.WriteLine("Starting recognition...");  
        completed = false;  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Wait for recognition to finish.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
        Console.WriteLine("Done.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("  Speech recognized:");  
      string grammarName = "<not available>";  
      if (e.Result.Grammar.Name != null &&  
        !e.Result.Grammar.Name.Equals(string.Empty))  
      {  
        grammarName = e.Result.Grammar.Name;  
      }  
      Console.WriteLine("    {0,-17} - {1}",  
        grammarName, e.Result.Text);  
  
      if (grammarName.Equals("Exit Grammar"))  
      {  
        ((SpeechRecognitionEngine)sender).RecognizeAsyncCancel();  
      }  
    }  
  
    static void RecognizeCompletedHandler(  
      object sender, RecognizeCompletedEventArgs e)  
    {  
      Console.WriteLine("  Recognition completed.");  
      completed = true;  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream(System.IO.Stream,System.Speech.AudioFormat.SpeechAudioFormatInfo)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream(System.IO.Stream)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="SetInputToNull">
      <MemberSignature Language="C#" Value="public void SetInputToNull ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void SetInputToNull() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull" />
      <MemberSignature Language="VB.NET" Value="Public Sub SetInputToNull ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void SetInputToNull();" />
      <MemberSignature Language="F#" Value="member this.SetInputToNull : unit -&gt; unit" Usage="speechRecognitionEngine.SetInputToNull " />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>禁用到语音识别器的输入。</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 <xref:System.Speech.Recognition.SpeechRecognitionEngine>当使用 <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize%2A> 和 <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> 方法时，或者当暂时使识别引擎脱机时，将对象配置为不输入。  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream(System.IO.Stream,System.Speech.AudioFormat.SpeechAudioFormatInfo)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream(System.IO.Stream)" />
      </Docs>
    </Member>
    <Member MemberName="SetInputToWaveFile">
      <MemberSignature Language="C#" Value="public void SetInputToWaveFile (string path);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void SetInputToWaveFile(string path) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile(System.String)" />
      <MemberSignature Language="VB.NET" Value="Public Sub SetInputToWaveFile (path As String)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void SetInputToWaveFile(System::String ^ path);" />
      <MemberSignature Language="F#" Value="member this.SetInputToWaveFile : string -&gt; unit" Usage="speechRecognitionEngine.SetInputToWaveFile path" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="path" Type="System.String" />
      </Parameters>
      <Docs>
        <param name="path">要作为输入的文件的路径。</param>
        <summary>配置 <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> 对象以接收从波形音频格式 (.wav) 文件的输入。</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 如果识别器在识别操作期间到达输入文件的末尾，则识别操作将使用可用的输入完成。 任何后续的识别操作都可能会生成异常，除非你将输入更新为识别器。  
  
   
  
## Examples  
 下面的示例对 .wav 文件中的音频执行识别，并将识别的文本写入控制台。  
  
```  
using System;  
using System.IO;  
using System.Speech.Recognition;  
using System.Speech.AudioFormat;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    static bool completed;  
  
    static void Main(string[] args)  
  
    // Initialize an in-process speech recognition engine.  
    {  
      using (SpeechRecognitionEngine recognizer =  
         new SpeechRecognitionEngine())  
      {  
  
        // Create and load a grammar.  
        Grammar dictation = new DictationGrammar();  
        dictation.Name = "Dictation Grammar";  
  
        recognizer.LoadGrammar(dictation);  
  
        // Configure the input to the recognizer.  
recognizer.SetInputToWaveFile(@"c:\temp\SampleWAVInput.wav");  
  
        // Attach event handlers for the results of recognition.  
        recognizer.SpeechRecognized +=   
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
        recognizer.RecognizeCompleted +=   
          new EventHandler<RecognizeCompletedEventArgs>(recognizer_RecognizeCompleted);  
  
        // Perform recognition on the entire file.  
        Console.WriteLine("Starting asynchronous recognition...");  
        completed = false;  
        recognizer.RecognizeAsync();  
  
        // Keep the console window open.  
        while (!completed)  
        {  
          Console.ReadLine();  
        }  
        Console.WriteLine("Done.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      if (e.Result != null && e.Result.Text != null)  
      {  
        Console.WriteLine("  Recognized text =  {0}", e.Result.Text);  
      }  
      else  
      {  
        Console.WriteLine("  Recognized text not available.");  
      }  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void recognizer_RecognizeCompleted(object sender, RecognizeCompletedEventArgs e)  
    {  
      if (e.Error != null)  
      {  
        Console.WriteLine("  Error encountered, {0}: {1}",  
        e.Error.GetType().Name, e.Error.Message);  
      }  
      if (e.Cancelled)  
      {  
        Console.WriteLine("  Operation cancelled.");  
      }  
      if (e.InputStreamEnded)  
      {  
        Console.WriteLine("  End of stream encountered.");  
      }  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream(System.IO.Stream,System.Speech.AudioFormat.SpeechAudioFormatInfo)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream(System.IO.Stream)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="SetInputToWaveStream">
      <MemberSignature Language="C#" Value="public void SetInputToWaveStream (System.IO.Stream audioSource);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void SetInputToWaveStream(class System.IO.Stream audioSource) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream(System.IO.Stream)" />
      <MemberSignature Language="VB.NET" Value="Public Sub SetInputToWaveStream (audioSource As Stream)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void SetInputToWaveStream(System::IO::Stream ^ audioSource);" />
      <MemberSignature Language="F#" Value="member this.SetInputToWaveStream : System.IO.Stream -&gt; unit" Usage="speechRecognitionEngine.SetInputToWaveStream audioSource" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="audioSource" Type="System.IO.Stream" />
      </Parameters>
      <Docs>
        <param name="audioSource">包含音频数据的流。</param>
        <summary>配置 <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> 对象以接收来自包含波形音频格式 (.wav) 数据的流的输入。</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 如果识别器在识别操作期间到达输入流的末尾，则识别操作将使用可用的输入完成。 任何后续的识别操作都可能会生成异常，除非你将输入更新为识别器。  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream(System.IO.Stream,System.Speech.AudioFormat.SpeechAudioFormatInfo)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile(System.String)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="SpeechDetected">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.SpeechDetectedEventArgs&gt; SpeechDetected;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.SpeechDetectedEventArgs&gt; SpeechDetected" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
      <MemberSignature Language="VB.NET" Value="Public Custom Event SpeechDetected As EventHandler(Of SpeechDetectedEventArgs) " FrameworkAlternate="netframework-3.0;netframework-3.5;netframework-4.0;netframework-4.5;netframework-4.5.1;netframework-4.5.2" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::SpeechDetectedEventArgs ^&gt; ^ SpeechDetected;" />
      <MemberSignature Language="F#" Value="member this.SpeechDetected : EventHandler&lt;System.Speech.Recognition.SpeechDetectedEventArgs&gt; " Usage="member this.SpeechDetected : System.EventHandler&lt;System.Speech.Recognition.SpeechDetectedEventArgs&gt; " />
      <MemberSignature Language="VB.NET" Value="Public Event SpeechDetected As EventHandler(Of SpeechDetectedEventArgs) " FrameworkAlternate="netframework-4.6;netframework-4.6.1;netframework-4.6.2;netframework-4.7;netframework-4.7.1;netframework-4.7.2;netframework-4.8" />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.SpeechDetectedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>当 <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> 检测它可以标识为语音的输入的时候引发。</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 每个语音识别器都有一种算法来区分无声和语音。 <xref:System.Speech.Recognition.SpeechRecognitionEngine>执行语音识别操作时，它会 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected> 在其算法将输入标识为语音时引发事件。 <xref:System.Speech.Recognition.SpeechDetectedEventArgs.AudioPosition%2A>关联的对象的属性 <xref:System.Speech.Recognition.SpeechDetectedEventArgs> 指示识别器在输入流中检测到的语音位置。 引发 <xref:System.Speech.Recognition.SpeechRecognitionEngine> <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected> 事件后引发 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized> 、 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> 或 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> 事件。  
  
 有关详细信息，请参阅 <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A> 、 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> 、 <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize%2A> 和 <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> 方法。  
  
 创建 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected> 委托时，需要标识将处理该事件的方法。 若要将事件与事件处理程序关联，请将该委托的一个实例添加到事件中。 除非移除了该委托，否则每当发生该事件时就会调用事件处理程序。 有关事件处理程序委托的详细信息，请参阅 [事件和委托](https://go.microsoft.com/fwlink/?LinkId=162418)。  
  
   
  
## Examples  
 下面的示例是一个控制台应用程序的一部分，用于为航班选择源和目标城市。 此应用程序识别诸如 "我想要从迈阿密飞到芝加哥" 之类的短语。  该示例使用 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected> 事件在 <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A> 每次检测到语音时报告。  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
  
    // Initialize an in-process speech recognition engine.  
    {  
      using (SpeechRecognitionEngine recognizer =  
         new SpeechRecognitionEngine())  
      {  
  
        // Create a grammar.  
        Choices cities = new Choices(new string[] {   
          "Los Angeles", "New York", "Chicago", "San Francisco", "Miami", "Dallas" });  
  
        GrammarBuilder gb = new GrammarBuilder();  
        gb.Append("I would like to fly from");  
        gb.Append(cities);  
        gb.Append("to");  
        gb.Append(cities);  
  
        // Create a Grammar object and load it to the recognizer.  
        Grammar g = new Grammar(gb);  
        g.Name = ("City Chooser");  
        recognizer.LoadGrammarAsync(g);  
  
        // Attach event handlers.  
        recognizer.LoadGrammarCompleted +=  
          new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
        recognizer.SpeechDetected +=  
          new EventHandler<SpeechDetectedEventArgs>(recognizer_SpeechDetected);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
        // Set the input to the recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start recognition.  
        recognizer.RecognizeAsync();  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
    }  
  
    // Handle the SpeechDetected event.  
    static void recognizer_SpeechDetected(object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine("  Speech detected at AudioPosition = {0}", e.AudioPosition);  
    }  
  
    // Handle the LoadGrammarCompleted event.  
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      Console.WriteLine("Grammar loaded: " + e.Grammar.Name);  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("  Speech recognized: " + e.Result.Text);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.SpeechDetectedEventArgs" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
      </Docs>
    </Member>
    <Member MemberName="SpeechHypothesized">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.SpeechHypothesizedEventArgs&gt; SpeechHypothesized;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.SpeechHypothesizedEventArgs&gt; SpeechHypothesized" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
      <MemberSignature Language="VB.NET" Value="Public Custom Event SpeechHypothesized As EventHandler(Of SpeechHypothesizedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::SpeechHypothesizedEventArgs ^&gt; ^ SpeechHypothesized;" />
      <MemberSignature Language="F#" Value="member this.SpeechHypothesized : EventHandler&lt;System.Speech.Recognition.SpeechHypothesizedEventArgs&gt; " Usage="member this.SpeechHypothesized : System.EventHandler&lt;System.Speech.Recognition.SpeechHypothesizedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.SpeechHypothesizedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>当 <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> 识别了可能是一个语法的多个完整的短语的组件的一个或多个单词的时候引发。</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 在 <xref:System.Speech.Recognition.SpeechRecognitionEngine> <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized> 尝试标识输入短语时，会生成大量事件。 您可以在 <xref:System.Speech.Recognition.RecognitionEventArgs.Result%2A> <xref:System.Speech.Recognition.SpeechHypothesizedEventArgs> 事件处理程序中的对象的属性中访问部分识别的短语的文本 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized> 。 通常，处理这些事件仅适用于调试。  
  
 <xref:System.Speech.Recognition.SpeechHypothesizedEventArgs> 派生自 <xref:System.Speech.Recognition.RecognitionEventArgs>。  
  
 有关详细信息，请参阅 <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> 属性和 <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A> 、 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> 、 <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize%2A> 和 <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> 方法。  
  
 创建 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized> 委托时，需要标识将处理该事件的方法。 若要将事件与事件处理程序关联，请将该委托的一个实例添加到事件中。 除非移除了该委托，否则每当发生该事件时就会调用事件处理程序。 有关事件处理程序委托的详细信息，请参阅 [事件和委托](https://go.microsoft.com/fwlink/?LinkId=162418)。  
  
   
  
## Examples  
 下面的示例将识别一些短语，如 "在爵士乐类别中显示音乐家列表"。 该示例使用 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized> 事件在控制台中显示不完整的短语片段，因为它们被识别。  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
  
    // Initialize an in-process speech recognition engine.  
    {  
      using (SpeechRecognitionEngine recognizer =  
         new SpeechRecognitionEngine())  
      {  
  
        // Create a grammar.  
        //  Create lists of alternative choices.  
        Choices listTypes = new Choices(new string[] { "albums", "artists" });  
        Choices genres = new Choices(new string[] {   
          "blues", "classical", "gospel", "jazz", "rock" });  
  
        //  Create a GrammarBuilder object and assemble the grammar components.  
        GrammarBuilder mediaMenu = new GrammarBuilder("Display the list of");  
        mediaMenu.Append(listTypes);  
        mediaMenu.Append("in the");  
        mediaMenu.Append(genres);  
        mediaMenu.Append("category.");  
  
        //  Build a Grammar object from the GrammarBuilder.  
        Grammar mediaMenuGrammar = new Grammar(mediaMenu);  
        mediaMenuGrammar.Name = "Media Chooser";  
  
        // Attach event handlers.  
        recognizer.LoadGrammarCompleted +=  
          new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
        recognizer.SpeechHypothesized +=  
          new EventHandler<SpeechHypothesizedEventArgs>(recognizer_SpeechHypothesized);  
  
        // Load the grammar object to the recognizer.  
        recognizer.LoadGrammarAsync(mediaMenuGrammar);  
  
        // Set the input to the recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start asynchronous recognition.  
        recognizer.RecognizeAsync();  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
    }  
  
    // Handle the SpeechHypothesized event.  
    static void recognizer_SpeechHypothesized(object sender, SpeechHypothesizedEventArgs e)  
    {  
      Console.WriteLine("Speech hypothesized: " + e.Result.Text);  
    }  
  
    // Handle the LoadGrammarCompleted event.  
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      Console.WriteLine("Grammar loaded: " + e.Grammar.Name);  
      Console.WriteLine();  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine();   
      Console.WriteLine("Speech recognized: " + e.Result.Text);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.SpeechDetectedEventArgs" />
        <altmember cref="T:System.Speech.Recognition.Grammar" />
        <altmember cref="T:System.Speech.Recognition.RecognitionResult" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
        <altmember cref="T:System.Speech.Recognition.RecognitionEventArgs" />
      </Docs>
    </Member>
    <Member MemberName="SpeechRecognitionRejected">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.SpeechRecognitionRejectedEventArgs&gt; SpeechRecognitionRejected;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.SpeechRecognitionRejectedEventArgs&gt; SpeechRecognitionRejected" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
      <MemberSignature Language="VB.NET" Value="Public Custom Event SpeechRecognitionRejected As EventHandler(Of SpeechRecognitionRejectedEventArgs) " FrameworkAlternate="netframework-3.0;netframework-3.5;netframework-4.0;netframework-4.5;netframework-4.5.1;netframework-4.5.2" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::SpeechRecognitionRejectedEventArgs ^&gt; ^ SpeechRecognitionRejected;" />
      <MemberSignature Language="F#" Value="member this.SpeechRecognitionRejected : EventHandler&lt;System.Speech.Recognition.SpeechRecognitionRejectedEventArgs&gt; " Usage="member this.SpeechRecognitionRejected : System.EventHandler&lt;System.Speech.Recognition.SpeechRecognitionRejectedEventArgs&gt; " />
      <MemberSignature Language="VB.NET" Value="Public Event SpeechRecognitionRejected As EventHandler(Of SpeechRecognitionRejectedEventArgs) " FrameworkAlternate="netframework-4.6;netframework-4.6.1;netframework-4.6.2;netframework-4.7;netframework-4.7.1;netframework-4.7.2;netframework-4.8" />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.SpeechRecognitionRejectedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>当 <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> 采用不与其加载启用的 <see cref="T:System.Speech.Recognition.Grammar" /> 对象匹配的输入的时候引发。</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 识别器会引发此事件，前提是它确定输入与已加载和启用的任何对象的完全置信度不符 <xref:System.Speech.Recognition.Grammar> 。 <xref:System.Speech.Recognition.RecognitionEventArgs.Result%2A>的属性 <xref:System.Speech.Recognition.SpeechRecognitionRejectedEventArgs> 包含被拒绝的 <xref:System.Speech.Recognition.RecognitionResult> 对象。 您可以使用事件的处理程序 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> 来检索 <xref:System.Speech.Recognition.RecognitionResult.Alternates%2A> 已拒绝的识别及其 <xref:System.Speech.Recognition.RecognizedPhrase.Confidence%2A> 分数。  
  
 如果你的应用程序使用的是 <xref:System.Speech.Recognition.SpeechRecognitionEngine> 实例，则可以修改使用其中一种方法接受或拒绝语音输入的置信度级别 <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> 。 您可以使用 <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A> 、 <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> 、 <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A> 和属性修改语音识别对非语音输入的响应方式 <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> 。  
  
 创建 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> 委托时，需要标识将处理该事件的方法。 若要将事件与事件处理程序关联，请将该委托的一个实例添加到事件中。 除非移除了该委托，否则每当发生该事件时就会调用事件处理程序。 有关事件处理程序委托的详细信息，请参阅 [事件和委托](https://go.microsoft.com/fwlink/?LinkId=162418)。  
  
   
  
## Examples  
 下面的示例将识别一些短语，如 "在爵士乐类别中显示音乐家列表" 或 "显示唱片集 gospel"。 该示例使用事件的处理程序 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> ，以便在语音输入无法与语法的内容匹配且足以 <xref:System.Speech.Recognition.RecognizedPhrase.Confidence%2A> 产生成功的识别时在控制台中显示通知。 处理程序还显示 <xref:System.Speech.Recognition.RecognitionResult.Alternates%2A> 由于置信度较低而被拒绝的识别结果。  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
  
    // Initialize an in-process speech recognition engine.  
    {  
      using (SpeechRecognitionEngine recognizer =  
         new SpeechRecognitionEngine(new System.Globalization.CultureInfo("en-US")))  
      {  
  
        // Create a grammar.  
        //  Create lists of alternative choices.  
        Choices listTypes = new Choices(new string[] { "albums", "artists" });  
        Choices genres = new Choices(new string[] {   
          "blues", "classical", "gospel", "jazz", "rock" });  
  
        //  Create a GrammarBuilder object and assemble the grammar components.  
        GrammarBuilder mediaMenu = new GrammarBuilder("Display");  
        mediaMenu.Append("the list of", 0, 1);  
        mediaMenu.Append(listTypes);  
        mediaMenu.Append("in the", 0, 1);  
        mediaMenu.Append(genres);  
        mediaMenu.Append("category", 0, 1);  
  
        //  Build a Grammar object from the GrammarBuilder.  
        Grammar mediaMenuGrammar = new Grammar(mediaMenu);  
        mediaMenuGrammar.Name = "Media Chooser";  
  
        // Attach event handlers.  
        recognizer.LoadGrammarCompleted +=  
          new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(recognizer_SpeechRecognitionRejected);  
  
        // Load the grammar object to the recognizer.  
        recognizer.LoadGrammarAsync(mediaMenuGrammar);  
  
        // Set the input to the recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
    }  
  
    // Handle the SpeechRecognitionRejected event.  
    static void recognizer_SpeechRecognitionRejected(object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine("Speech input was rejected.");  
      foreach (RecognizedPhrase phrase in e.Result.Alternates)  
      {  
      Console.WriteLine("  Rejected phrase: " + phrase.Text);  
      Console.WriteLine("  Confidence score: " + phrase.Confidence);  
      }  
    }  
  
    // Handle the LoadGrammarCompleted event.  
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      Console.WriteLine("Grammar loaded: " + e.Grammar.Name);  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Speech recognized: " + e.Result.Text);  
      Console.WriteLine("  Confidence score: " + e.Result.Confidence);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.SpeechDetectedEventArgs" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
      </Docs>
    </Member>
    <Member MemberName="SpeechRecognized">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.SpeechRecognizedEventArgs&gt; SpeechRecognized;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.SpeechRecognizedEventArgs&gt; SpeechRecognized" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
      <MemberSignature Language="VB.NET" Value="Public Custom Event SpeechRecognized As EventHandler(Of SpeechRecognizedEventArgs) " FrameworkAlternate="netframework-3.0;netframework-3.5;netframework-4.0;netframework-4.5;netframework-4.5.1;netframework-4.5.2" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::SpeechRecognizedEventArgs ^&gt; ^ SpeechRecognized;" />
      <MemberSignature Language="F#" Value="member this.SpeechRecognized : EventHandler&lt;System.Speech.Recognition.SpeechRecognizedEventArgs&gt; " Usage="member this.SpeechRecognized : System.EventHandler&lt;System.Speech.Recognition.SpeechRecognizedEventArgs&gt; " />
      <MemberSignature Language="VB.NET" Value="Public Event SpeechRecognized As EventHandler(Of SpeechRecognizedEventArgs) " FrameworkAlternate="netframework-4.6;netframework-4.6.1;netframework-4.6.2;netframework-4.7;netframework-4.7.1;netframework-4.7.2;netframework-4.8" />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.SpeechRecognizedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>当 <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> 采用与其加载启用的 <see cref="T:System.Speech.Recognition.Grammar" /> 对象匹配的输入的时候引发。</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 您可以使用或方法之一启动识别操作 <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A> <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> 。 识别器会引发 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> 事件，前提是它确定输入项与它的加载对象之一匹配 <xref:System.Speech.Recognition.Grammar> ，并具有足够的置信度来构成识别。 <xref:System.Speech.Recognition.RecognitionEventArgs.Result%2A>的属性 <xref:System.Speech.Recognition.SpeechRecognitionRejectedEventArgs> 包含接受的 <xref:System.Speech.Recognition.RecognitionResult> 对象。 事件处理程序 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> 可获取识别的短语，以及 <xref:System.Speech.Recognition.RecognitionResult.Alternates%2A> 置信度分数较低的识别的列表。  
  
 如果你的应用程序使用的是 <xref:System.Speech.Recognition.SpeechRecognitionEngine> 实例，则可以修改使用其中一种方法接受或拒绝语音输入的置信度级别 <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> 。  您可以使用 <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A> 、 <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> 、 <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A> 和属性修改语音识别对非语音输入的响应方式 <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> 。  
  
 当识别器接收与某个语法匹配的输入时，该 <xref:System.Speech.Recognition.Grammar> 对象可以引发其 <xref:System.Speech.Recognition.Grammar.SpeechRecognized> 事件。 <xref:System.Speech.Recognition.Grammar>对象的 <xref:System.Speech.Recognition.Grammar.SpeechRecognized> 事件在语音识别器的事件发生前引发 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> 。 特定于特定语法的任何任务都应始终由事件的处理程序执行 <xref:System.Speech.Recognition.Grammar.SpeechRecognized> 。  
  
 创建 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> 委托时，需要标识将处理该事件的方法。 若要将事件与事件处理程序关联，请将该委托的一个实例添加到事件中。 除非移除了该委托，否则每当发生该事件时就会调用事件处理程序。 有关事件处理程序委托的详细信息，请参阅 [事件和委托](https://go.microsoft.com/fwlink/?LinkId=162418)。  
  
   
  
## Examples  
 下面的示例是一个控制台应用程序的一部分，该应用程序可创建语音识别语法、构造 <xref:System.Speech.Recognition.Grammar> 对象并将其加载到中 <xref:System.Speech.Recognition.SpeechRecognitionEngine> 以执行识别。 该示例演示了对的语音输入 <xref:System.Speech.Recognition.SpeechRecognitionEngine> 、关联的识别结果以及由语音识别器引发的相关事件。  
  
 "我想要飞入到迈阿密" 之类的朗读输入将触发 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> 事件。 说到 "从休斯顿到芝加哥飞入" 这一短语不会触发 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> 事件。  
  
 该示例使用事件的处理程序 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> 显示已成功识别的短语以及它们在控制台中包含的语义。  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
  
    // Initialize an in-process speech recognition engine.  
    {  
      using (SpeechRecognitionEngine recognizer = new SpeechRecognitionEngine())  
      {  
  
        // Create SemanticResultValue objects that contain cities and airport codes.  
        SemanticResultValue chicago = new SemanticResultValue("Chicago", "ORD");  
        SemanticResultValue boston = new SemanticResultValue("Boston", "BOS");  
        SemanticResultValue miami = new SemanticResultValue("Miami", "MIA");  
        SemanticResultValue dallas = new SemanticResultValue("Dallas", "DFW");  
  
        // Create a Choices object and add the SemanticResultValue objects, using  
        // implicit conversion from SemanticResultValue to GrammarBuilder  
        Choices cities = new Choices();  
        cities.Add(new Choices(new GrammarBuilder[] { chicago, boston, miami, dallas }));  
  
        // Build the phrase and add SemanticResultKeys.  
        GrammarBuilder chooseCities = new GrammarBuilder();  
        chooseCities.Append("I want to fly from");  
        chooseCities.Append(new SemanticResultKey("origin", cities));  
        chooseCities.Append("to");  
        chooseCities.Append(new SemanticResultKey("destination", cities));  
  
        // Build a Grammar object from the GrammarBuilder.  
        Grammar bookFlight = new Grammar(chooseCities);  
        bookFlight.Name = "Book Flight";  
  
        // Add a handler for the LoadGrammarCompleted event.  
        recognizer.LoadGrammarCompleted +=  
          new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
  
        // Add a handler for the SpeechRecognized event.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
        // Load the grammar object to the recognizer.  
        recognizer.LoadGrammarAsync(bookFlight);  
  
        // Set the input to the recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start recognition.  
        recognizer.RecognizeAsync();  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
    }  
  
    // Handle the LoadGrammarCompleted event.  
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      Console.WriteLine("Grammar loaded: " + e.Grammar.Name);  
      Console.WriteLine();  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Speech recognized:  " + e.Result.Text);  
      Console.WriteLine();  
      Console.WriteLine("Semantic results:");  
      Console.WriteLine("  The flight origin is " + e.Result.Semantics["origin"].Value);  
      Console.WriteLine("  The flight destination is " + e.Result.Semantics["destination"].Value);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.SpeechDetectedEventArgs" />
        <altmember cref="T:System.Speech.Recognition.Grammar" />
        <altmember cref="T:System.Speech.Recognition.RecognitionResult" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="T:System.Speech.Recognition.RecognitionEventArgs" />
      </Docs>
    </Member>
    <Member MemberName="UnloadAllGrammars">
      <MemberSignature Language="C#" Value="public void UnloadAllGrammars ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void UnloadAllGrammars() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.UnloadAllGrammars" />
      <MemberSignature Language="VB.NET" Value="Public Sub UnloadAllGrammars ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void UnloadAllGrammars();" />
      <MemberSignature Language="F#" Value="member this.UnloadAllGrammars : unit -&gt; unit" Usage="speechRecognitionEngine.UnloadAllGrammars " />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>从卸载识别器的所有 <see cref="T:System.Speech.Recognition.Grammar" /> 对象。</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 如果识别器当前正在异步加载 <xref:System.Speech.Recognition.Grammar> ，则此方法将等待，直到加载了该 <xref:System.Speech.Recognition.Grammar> <xref:System.Speech.Recognition.Grammar> 实例中的所有对象 <xref:System.Speech.Recognition.SpeechRecognitionEngine> 。  
  
 若要卸载特定语法，请使用 <xref:System.Speech.Recognition.SpeechRecognitionEngine.UnloadGrammar%2A> 方法。  
  
   
  
## Examples  
 下面的示例演示了控制台应用程序的一部分，该应用程序演示了语音识别语法的同步加载和卸载。  
  
```  
Loading grammars...  
Loaded grammars:  
 - Grammar1  
 - Grammar2  
 - Grammar3  
  
Unloading Grammar1...  
Loaded grammars:  
 - Grammar2  
 - Grammar3  
  
Unloading all grammars...  
No grammars loaded.  
  
Press any key to exit...  
```  
  
```csharp  
  
using System;  
using System.Collections.Generic;  
using System.Globalization;  
using System.Speech.Recognition;  
  
namespace UnloadGrammars  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
        Console.WriteLine("Loading grammars...");  
  
        // Create and load a number of grammars.  
        Grammar grammar1 = new Grammar(new GrammarBuilder("first grammar"));  
        grammar1.Name = "Grammar1";  
        recognizer.LoadGrammar(grammar1);  
  
        Grammar grammar2 = new Grammar(new GrammarBuilder("second grammar"));  
        grammar2.Name = "Grammar2";  
        recognizer.LoadGrammar(grammar2);  
  
        Grammar grammar3 = new Grammar(new GrammarBuilder("third grammar"));  
        grammar3.Name = "Grammar3";  
        recognizer.LoadGrammar(grammar3);  
  
        // List the recognizer's loaded grammars.  
        ListGrammars(recognizer);  
  
        // Unload one grammar and list the loaded grammars.  
        Console.WriteLine("Unloading Grammar1...");  
        recognizer.UnloadGrammar(grammar1);  
        ListGrammars(recognizer);  
  
        // Unload all grammars and list the loaded grammars.  
        Console.WriteLine("Unloading all grammars...");  
        recognizer.UnloadAllGrammars();  
        ListGrammars(recognizer);  
      }  
  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    private static void ListGrammars(SpeechRecognitionEngine recognizer)  
    {  
      // Make a copy of the recognizer's grammar collection.  
      List<Grammar> loadedGrammars = new List<Grammar>(recognizer.Grammars);  
  
      if (loadedGrammars.Count > 0)  
      {  
        Console.WriteLine("Loaded grammars:");  
        foreach (Grammar g in recognizer.Grammars)  
        {  
          Console.WriteLine(" - {0}", g.Name);  
        }  
      }  
      else  
      {  
        Console.WriteLine("No grammars loaded.");  
      }  
      Console.WriteLine();  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar(System.Speech.Recognition.Grammar)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync(System.Speech.Recognition.Grammar)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.UnloadGrammar(System.Speech.Recognition.Grammar)" />
      </Docs>
    </Member>
    <Member MemberName="UnloadGrammar">
      <MemberSignature Language="C#" Value="public void UnloadGrammar (System.Speech.Recognition.Grammar grammar);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void UnloadGrammar(class System.Speech.Recognition.Grammar grammar) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.UnloadGrammar(System.Speech.Recognition.Grammar)" />
      <MemberSignature Language="VB.NET" Value="Public Sub UnloadGrammar (grammar As Grammar)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void UnloadGrammar(System::Speech::Recognition::Grammar ^ grammar);" />
      <MemberSignature Language="F#" Value="member this.UnloadGrammar : System.Speech.Recognition.Grammar -&gt; unit" Usage="speechRecognitionEngine.UnloadGrammar grammar" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="grammar" Type="System.Speech.Recognition.Grammar" />
      </Parameters>
      <Docs>
        <param name="grammar">未加载的语法对象。</param>
        <summary>从指定的 <see cref="T:System.Speech.Recognition.Grammar" /> 实例卸载一个指定的 <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> 对象。</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 如果识别器正在运行，则应用程序必须 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> 先使用暂停 <xref:System.Speech.Recognition.SpeechRecognitionEngine> 实例，然后再加载、卸载、启用或禁用 <xref:System.Speech.Recognition.Grammar> 对象。 若要卸载所有 <xref:System.Speech.Recognition.Grammar> 对象，请使用 <xref:System.Speech.Recognition.SpeechRecognitionEngine.UnloadAllGrammars%2A> 方法。  
  
   
  
## Examples  
 下面的示例演示了控制台应用程序的一部分，该应用程序演示了语音识别语法的同步加载和卸载。  
  
```  
Loading grammars...  
Loaded grammars:  
 - Grammar1  
 - Grammar2  
 - Grammar3  
  
Unloading Grammar1...  
Loaded grammars:  
 - Grammar2  
 - Grammar3  
  
Unloading all grammars...  
No grammars loaded.  
  
Press any key to exit...  
```  
  
```csharp  
  
using System;  
using System.Collections.Generic;  
using System.Globalization;  
using System.Speech.Recognition;  
  
namespace UnloadGrammars  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
        Console.WriteLine("Loading grammars...");  
  
        // Create and load a number of grammars.  
        Grammar grammar1 = new Grammar(new GrammarBuilder("first grammar"));  
        grammar1.Name = "Grammar1";  
        recognizer.LoadGrammar(grammar1);  
  
        Grammar grammar2 = new Grammar(new GrammarBuilder("second grammar"));  
        grammar2.Name = "Grammar2";  
        recognizer.LoadGrammar(grammar2);  
  
        Grammar grammar3 = new Grammar(new GrammarBuilder("third grammar"));  
        grammar3.Name = "Grammar3";  
        recognizer.LoadGrammar(grammar3);  
  
        // List the recognizer's loaded grammars.  
        ListGrammars(recognizer);  
  
        // Unload one grammar and list the loaded grammars.  
        Console.WriteLine("Unloading Grammar1...");  
        recognizer.UnloadGrammar(grammar1);  
        ListGrammars(recognizer);  
  
        // Unload all grammars and list the loaded grammars.  
        Console.WriteLine("Unloading all grammars...");  
        recognizer.UnloadAllGrammars();  
        ListGrammars(recognizer);  
      }  
  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    private static void ListGrammars(SpeechRecognitionEngine recognizer)  
    {  
      // Make a copy of the recognizer's grammar collection.  
      List<Grammar> loadedGrammars = new List<Grammar>(recognizer.Grammars);  
  
      if (loadedGrammars.Count > 0)  
      {  
        Console.WriteLine("Loaded grammars:");  
        foreach (Grammar g in recognizer.Grammars)  
        {  
          Console.WriteLine(" - {0}", g.Name);  
        }  
      }  
      else  
      {  
        Console.WriteLine("No grammars loaded.");  
      }  
      Console.WriteLine();  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentNullException"><paramref name="Grammar" /> 为 <see langword="null" />。</exception>
        <exception cref="T:System.InvalidOperationException">此语法识别器未加载，或者此识别器当前加载的语法异步。</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar(System.Speech.Recognition.Grammar)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync(System.Speech.Recognition.Grammar)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.UnloadAllGrammars" />
      </Docs>
    </Member>
    <MemberGroup MemberName="UpdateRecognizerSetting">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>更新为识别器设置的值。</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 识别器设置可以包含字符串、64位整数或内存地址数据。 下表描述了为 Microsoft Speech API (SAPI) 兼容的识别器定义的设置。 对于支持设置的每个识别器，以下设置必须具有相同的范围。 支持这些设置并可支持其他设置时，不需要使用 SAPI 兼容的识别器。  
  
|名称|说明|  
|----------|-----------------|  
|`ResourceUsage`|指定识别器的 CPU 消耗。 范围为0到100。 默认值为 50。|  
|`ResponseSpeed`|指示语音识别器完成识别操作之前明确输入结束时的静默时间长度。 范围为0到10000毫秒 (ms) 。 此设置对应于识别器的 <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A> 属性。 默认值为150ms。|  
|`ComplexResponseSpeed`|指示语音识别器完成识别操作之前，以毫秒为单位的静默时间长度（以毫秒为单位） (毫秒) 。 范围为0到10、000ms。 此设置对应于识别器的 <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> 属性。 默认值为500毫秒。|  
|`AdaptationOn`|指示是否在 (值 = `1`) 或关闭 (值 =) 时声音模型的适应 `0` 。 )  (默认值 `1` 。|  
|`PersistedBackgroundAdaptation`|指示是否 (值 = `1`) 或关闭 (值 = `0`) ，并保留注册表中的设置。 )  (默认值 `1` 。|  
  
 若要返回某个识别器的设置，请使用 <xref:System.Speech.Recognition.SpeechRecognitionEngine.QueryRecognizerSetting%2A> 方法。  
  
 除了之外 `PersistedBackgroundAdaptation` ，使用方法设置的属性值 <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> 仅对的当前实例保持有效 <xref:System.Speech.Recognition.SpeechRecognitionEngine> ，之后它们恢复为其默认设置。  
  
 您可以使用 <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A> 、 <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> 、 <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A> 和属性修改语音识别对非语音输入的响应方式 <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> 。  
  
 ]]></format>
        </remarks>
      </Docs>
    </MemberGroup>
    <Member MemberName="UpdateRecognizerSetting">
      <MemberSignature Language="C#" Value="public void UpdateRecognizerSetting (string settingName, int updatedValue);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void UpdateRecognizerSetting(string settingName, int32 updatedValue) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting(System.String,System.Int32)" />
      <MemberSignature Language="VB.NET" Value="Public Sub UpdateRecognizerSetting (settingName As String, updatedValue As Integer)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void UpdateRecognizerSetting(System::String ^ settingName, int updatedValue);" />
      <MemberSignature Language="F#" Value="member this.UpdateRecognizerSetting : string * int -&gt; unit" Usage="speechRecognitionEngine.UpdateRecognizerSetting (settingName, updatedValue)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="settingName" Type="System.String" />
        <Parameter Name="updatedValue" Type="System.Int32" />
      </Parameters>
      <Docs>
        <param name="settingName">设置为更新的名称。</param>
        <param name="updatedValue">设置的新建值。</param>
        <summary>指定的整数值的 <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> 指定设置。</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 除了之外 `PersistedBackgroundAdaptation` ，使用方法设置的属性值 <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> 仅对的当前实例保持有效 <xref:System.Speech.Recognition.SpeechRecognitionEngine> ，之后它们恢复为其默认设置。 <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A>有关支持的设置的说明，请参阅。  
  
   
  
## Examples  
 下面的示例是一个控制台应用程序的一部分，该控制台应用程序为支持 en-us 区域设置的识别器定义的多个设置的值。 该示例更新置信度设置，然后查询识别器来检查更新的值。 该示例生成以下输出。  
  
```  
Settings for recognizer MS-1033-80-DESK:  
  
  ResourceUsage                  is not supported by this recognizer.  
  ResponseSpeed                  = 150  
  ComplexResponseSpeed           = 500  
  AdaptationOn                   = 1  
  PersistedBackgroundAdaptation  = 1  
  
Updated settings:  
  
  ResourceUsage                  is not supported by this recognizer.  
  ResponseSpeed                  = 200  
  ComplexResponseSpeed           = 300  
  AdaptationOn                   = 0  
  PersistedBackgroundAdaptation  = 0  
  
Press any key to exit...  
```  
  
```csharp  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
  
namespace RecognizerSettings  
{  
  class Program  
  {  
    static readonly string[] settings = new string[] {  
      "ResourceUsage",  
      "ResponseSpeed",  
      "ComplexResponseSpeed",  
      "AdaptationOn",  
      "PersistedBackgroundAdaptation",  
    };  
  
    static void Main(string[] args)  
    {  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new System.Globalization.CultureInfo("en-US")))  
      {  
        Console.WriteLine("Settings for recognizer {0}:",  
          recognizer.RecognizerInfo.Name);  
        Console.WriteLine();  
  
        // List the current settings.  
        ListSettings(recognizer);  
  
        // Change some of the settings.  
        recognizer.UpdateRecognizerSetting("ResponseSpeed", 200);  
        recognizer.UpdateRecognizerSetting("ComplexResponseSpeed", 300);  
        recognizer.UpdateRecognizerSetting("AdaptationOn", 1);  
        recognizer.UpdateRecognizerSetting("PersistedBackgroundAdaptation", 0);  
  
        Console.WriteLine("Updated settings:");  
        Console.WriteLine();  
  
        // List the updated settings.  
        ListSettings(recognizer);  
      }  
  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    private static void ListSettings(SpeechRecognitionEngine recognizer)  
    {  
      foreach (string setting in settings)  
      {  
        try  
        {  
          object value = recognizer.QueryRecognizerSetting(setting);  
          Console.WriteLine("  {0,-30} = {1}", setting, value);  
        }  
        catch  
        {  
          Console.WriteLine("  {0,-30} is not supported by this recognizer.",  
            setting);  
        }  
      }  
      Console.WriteLine();  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentNullException"><paramref name="settingName" /> 为 <see langword="null" />。</exception>
        <exception cref="T:System.ArgumentException"><paramref name="settingName" /> 为空字符串 ("")。</exception>
        <exception cref="T:System.Collections.Generic.KeyNotFoundException">不用该名称设置的识别器。</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.QueryRecognizerSetting(System.String)" />
      </Docs>
    </Member>
    <Member MemberName="UpdateRecognizerSetting">
      <MemberSignature Language="C#" Value="public void UpdateRecognizerSetting (string settingName, string updatedValue);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void UpdateRecognizerSetting(string settingName, string updatedValue) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting(System.String,System.String)" />
      <MemberSignature Language="VB.NET" Value="Public Sub UpdateRecognizerSetting (settingName As String, updatedValue As String)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void UpdateRecognizerSetting(System::String ^ settingName, System::String ^ updatedValue);" />
      <MemberSignature Language="F#" Value="member this.UpdateRecognizerSetting : string * string -&gt; unit" Usage="speechRecognitionEngine.UpdateRecognizerSetting (settingName, updatedValue)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="settingName" Type="System.String" />
        <Parameter Name="updatedValue" Type="System.String" />
      </Parameters>
      <Docs>
        <param name="settingName">设置为更新的名称。</param>
        <param name="updatedValue">设置的新建值。</param>
        <summary>更新指定字符串值设定的指定语音识别引擎。</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 除了之外 `PersistedBackgroundAdaptation` ，使用方法设置的属性值 <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> 仅对的当前实例保持有效 <xref:System.Speech.Recognition.SpeechRecognitionEngine> ，之后它们恢复为其默认设置。 <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A>有关支持的设置的说明，请参阅。  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentNullException"><paramref name="settingName" /> 为 <see langword="null" />。</exception>
        <exception cref="T:System.ArgumentException"><paramref name="settingName" /> 为空字符串 ("")。</exception>
        <exception cref="T:System.Collections.Generic.KeyNotFoundException">不用该名称设置的识别器。</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.QueryRecognizerSetting(System.String)" />
      </Docs>
    </Member>
  </Members>
</Type>
